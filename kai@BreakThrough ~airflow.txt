Welcome to Ubuntu 22.04.4 LTS (GNU/Linux 5.15.133.1-microsoft-standard-WSL2 x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/pro

 * Strictly confined Kubernetes makes edge and IoT secure. Learn how MicroK8s
   just raised the bar for easy, resilient and secure K8s cluster deployment.

   https://ubuntu.com/engage/secure-kubernetes-at-the-edge

This message is shown once a day. To disable it please create the
/home/kai/.hushlogin file.
kai@BreakThrough:~$ airflow scheduler
  ____________       _____________
 ____    |__( )_________  __/__  /________      __
____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
 _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
[2024-03-11T20:42:14.657+0530] {task_context_logger.py:63} INFO - Task context logging is enabled
[2024-03-11T20:42:14.658+0530] {executor_loader.py:115} INFO - Loaded executor: SequentialExecutor
[2024-03-11 20:42:14 +0530] [253581] [INFO] Starting gunicorn 21.2.0
[2024-03-11T20:42:14.732+0530] {scheduler_job_runner.py:808} INFO - Starting the scheduler
[2024-03-11T20:42:14.734+0530] {scheduler_job_runner.py:815} INFO - Processing each file at most -1 times
[2024-03-11 20:42:14 +0530] [253581] [INFO] Listening at: http://[::]:8793 (253581)
[2024-03-11 20:42:14 +0530] [253581] [INFO] Using worker: sync
[2024-03-11T20:42:14.742+0530] {manager.py:169} INFO - Launched DagFileProcessorManager with pid: 253583
[2024-03-11 20:42:14 +0530] [253582] [INFO] Booting worker with pid: 253582
[2024-03-11T20:42:14.746+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-11T20:42:14.764+0530] {settings.py:60} INFO - Configured default timezone UTC
[2024-03-11 20:42:14 +0530] [253584] [INFO] Booting worker with pid: 253584
[2024-03-11T20:42:14.916+0530] {manager.py:392} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[2024-03-11T20:42:15.399+0530] {dag.py:3834} INFO - Setting next_dagrun for catchup_dag to 2024-03-10 12:00:00+00:00, run_after=2024-03-11 00:00:00+00:00
[2024-03-11T20:42:15.532+0530] {scheduler_job_runner.py:424} INFO - 1 tasks up for execution:
        <TaskInstance: catchup_dag.over_18 scheduled__2024-03-10T00:00:00+00:00 [scheduled]>
[2024-03-11T20:42:15.532+0530] {scheduler_job_runner.py:487} INFO - DAG catchup_dag has 0/16 running and queued tasks
[2024-03-11T20:42:15.532+0530] {scheduler_job_runner.py:603} INFO - Setting the following tasks to queued state:
        <TaskInstance: catchup_dag.over_18 scheduled__2024-03-10T00:00:00+00:00 [scheduled]>
[2024-03-11T20:42:15.535+0530] {taskinstance.py:2283} WARNING - cannot record scheduled_duration for task over_18 because previous state change time has not been saved
[2024-03-11T20:42:15.536+0530] {scheduler_job_runner.py:646} INFO - Sending TaskInstanceKey(dag_id='catchup_dag', task_id='over_18', run_id='scheduled__2024-03-10T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default
[2024-03-11T20:42:15.536+0530] {base_executor.py:146} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'catchup_dag', 'over_18', 'scheduled__2024-03-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_catchup.py']
[2024-03-11T20:42:15.554+0530] {sequential_executor.py:73} INFO - Executing command: ['airflow', 'tasks', 'run', 'catchup_dag', 'over_18', 'scheduled__2024-03-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_catchup.py']
[2024-03-11T20:42:16.759+0530] {dagbag.py:540} INFO - Filling up the DagBag from /home/kai/airflow/dags/execute_catchup.py
Changing /home/kai/airflow/logs/dag_id=catchup_dag/run_id=scheduled__2024-03-10T00:00:00+00:00/task_id=over_18 permission to 509
Changing /home/kai/airflow/logs/dag_id=catchup_dag/run_id=scheduled__2024-03-10T00:00:00+00:00 permission to 509
[2024-03-11T20:42:16.994+0530] {task_command.py:423} INFO - Running <TaskInstance: catchup_dag.over_18 scheduled__2024-03-10T00:00:00+00:00 [queued]> on host BreakThrough.
[2024-03-11T20:42:18.110+0530] {scheduler_job_runner.py:696} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='catchup_dag', task_id='over_18', run_id='scheduled__2024-03-10T00:00:00+00:00', try_number=1, map_index=-1)
[2024-03-11T20:42:18.123+0530] {scheduler_job_runner.py:733} INFO - TaskInstance Finished: dag_id=catchup_dag, task_id=over_18, run_id=scheduled__2024-03-10T00:00:00+00:00, map_index=-1, run_start_date=2024-03-11 15:12:17.077801+00:00, run_end_date=2024-03-11 15:12:17.599137+00:00, run_duration=0.521336, state=success, executor_state=success, try_number=1, max_tries=0,job_id=173, pool=default_pool, queue=default, priority_weight=4, operator=PythonOperator, queued_dttm=2024-03-11 15:12:15.533398+00:00, queued_by_job_id=172, pid=253587
[2024-03-11T20:42:19.863+0530] {dag.py:3834} INFO - Setting next_dagrun for catchup_dag to 2024-03-11 00:00:00+00:00, run_after=2024-03-11 12:00:00+00:00
[2024-03-11T20:42:19.965+0530] {scheduler_job_runner.py:424} INFO - 2 tasks up for execution:
        <TaskInstance: catchup_dag.over_18 scheduled__2024-03-10T12:00:00+00:00 [scheduled]>
        <TaskInstance: catchup_dag.branch scheduled__2024-03-10T00:00:00+00:00 [scheduled]>
[2024-03-11T20:42:19.966+0530] {scheduler_job_runner.py:487} INFO - DAG catchup_dag has 0/16 running and queued tasks
[2024-03-11T20:42:19.966+0530] {scheduler_job_runner.py:487} INFO - DAG catchup_dag has 1/16 running and queued tasks
[2024-03-11T20:42:19.966+0530] {scheduler_job_runner.py:603} INFO - Setting the following tasks to queued state:
        <TaskInstance: catchup_dag.over_18 scheduled__2024-03-10T12:00:00+00:00 [scheduled]>
        <TaskInstance: catchup_dag.branch scheduled__2024-03-10T00:00:00+00:00 [scheduled]>
[2024-03-11T20:42:19.970+0530] {taskinstance.py:2283} WARNING - cannot record scheduled_duration for task over_18 because previous state change time has not been saved
[2024-03-11T20:42:19.970+0530] {taskinstance.py:2283} WARNING - cannot record scheduled_duration for task branch because previous state change time has not been saved
[2024-03-11T20:42:19.971+0530] {scheduler_job_runner.py:646} INFO - Sending TaskInstanceKey(dag_id='catchup_dag', task_id='over_18', run_id='scheduled__2024-03-10T12:00:00+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default
[2024-03-11T20:42:19.971+0530] {base_executor.py:146} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'catchup_dag', 'over_18', 'scheduled__2024-03-10T12:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_catchup.py']
[2024-03-11T20:42:19.971+0530] {scheduler_job_runner.py:646} INFO - Sending TaskInstanceKey(dag_id='catchup_dag', task_id='branch', run_id='scheduled__2024-03-10T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default
[2024-03-11T20:42:19.971+0530] {base_executor.py:146} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'catchup_dag', 'branch', 'scheduled__2024-03-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_catchup.py']
[2024-03-11T20:42:19.990+0530] {sequential_executor.py:73} INFO - Executing command: ['airflow', 'tasks', 'run', 'catchup_dag', 'over_18', 'scheduled__2024-03-10T12:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_catchup.py']
[2024-03-11T20:42:21.125+0530] {dagbag.py:540} INFO - Filling up the DagBag from /home/kai/airflow/dags/execute_catchup.py
Changing /home/kai/airflow/logs/dag_id=catchup_dag/run_id=scheduled__2024-03-10T12:00:00+00:00/task_id=over_18 permission to 509
Changing /home/kai/airflow/logs/dag_id=catchup_dag/run_id=scheduled__2024-03-10T12:00:00+00:00 permission to 509
[2024-03-11T20:42:21.316+0530] {task_command.py:423} INFO - Running <TaskInstance: catchup_dag.over_18 scheduled__2024-03-10T12:00:00+00:00 [queued]> on host BreakThrough.
[2024-03-11T20:42:22.148+0530] {sequential_executor.py:73} INFO - Executing command: ['airflow', 'tasks', 'run', 'catchup_dag', 'branch', 'scheduled__2024-03-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_catchup.py']
[2024-03-11T20:42:23.214+0530] {dagbag.py:540} INFO - Filling up the DagBag from /home/kai/airflow/dags/execute_catchup.py
Changing /home/kai/airflow/logs/dag_id=catchup_dag/run_id=scheduled__2024-03-10T00:00:00+00:00/task_id=branch permission to509
[2024-03-11T20:42:23.401+0530] {task_command.py:423} INFO - Running <TaskInstance: catchup_dag.branch scheduled__2024-03-10T00:00:00+00:00 [queued]> on host BreakThrough.
[2024-03-11T20:42:24.229+0530] {scheduler_job_runner.py:696} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='catchup_dag', task_id='over_18', run_id='scheduled__2024-03-10T12:00:00+00:00', try_number=1, map_index=-1)
[2024-03-11T20:42:24.229+0530] {scheduler_job_runner.py:696} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='catchup_dag', task_id='branch', run_id='scheduled__2024-03-10T00:00:00+00:00', try_number=1, map_index=-1)
[2024-03-11T20:42:24.237+0530] {scheduler_job_runner.py:733} INFO - TaskInstance Finished: dag_id=catchup_dag, task_id=branch, run_id=scheduled__2024-03-10T00:00:00+00:00, map_index=-1, run_start_date=2024-03-11 15:12:23.469703+00:00, run_end_date=2024-03-11 15:12:23.844317+00:00, run_duration=0.374614, state=success, executor_state=success, try_number=1, max_tries=0, job_id=175, pool=default_pool, queue=default, priority_weight=3, operator=BranchPythonOperator, queued_dttm=2024-03-11 15:12:19.967935+00:00, queued_by_job_id=172, pid=253619
[2024-03-11T20:42:24.237+0530] {scheduler_job_runner.py:733} INFO - TaskInstance Finished: dag_id=catchup_dag, task_id=over_18, run_id=scheduled__2024-03-10T12:00:00+00:00, map_index=-1, run_start_date=2024-03-11 15:12:21.383401+00:00, run_end_date=2024-03-11 15:12:21.735064+00:00, run_duration=0.351663, state=success, executor_state=success, try_number=1, max_tries=0,job_id=174, pool=default_pool, queue=default, priority_weight=4, operator=PythonOperator, queued_dttm=2024-03-11 15:12:19.967935+00:00, queued_by_job_id=172, pid=253617
[2024-03-11T20:42:25.002+0530] {dag.py:3834} INFO - Setting next_dagrun for catchup_dag to 2024-03-11 12:00:00+00:00, run_after=2024-03-12 00:00:00+00:00
[2024-03-11T20:42:25.117+0530] {scheduler_job_runner.py:424} INFO - 3 tasks up for execution:
        <TaskInstance: catchup_dag.over_18 scheduled__2024-03-11T00:00:00+00:00 [scheduled]>
        <TaskInstance: catchup_dag.branch scheduled__2024-03-10T12:00:00+00:00 [scheduled]>
        <TaskInstance: catchup_dag.not_eligible_to_vote scheduled__2024-03-10T00:00:00+00:00 [scheduled]>
[2024-03-11T20:42:25.117+0530] {scheduler_job_runner.py:487} INFO - DAG catchup_dag has 0/16 running and queued tasks
[2024-03-11T20:42:25.117+0530] {scheduler_job_runner.py:487} INFO - DAG catchup_dag has 1/16 running and queued tasks
[2024-03-11T20:42:25.118+0530] {scheduler_job_runner.py:487} INFO - DAG catchup_dag has 2/16 running and queued tasks
[2024-03-11T20:42:25.118+0530] {scheduler_job_runner.py:603} INFO - Setting the following tasks to queued state:
        <TaskInstance: catchup_dag.over_18 scheduled__2024-03-11T00:00:00+00:00 [scheduled]>
        <TaskInstance: catchup_dag.branch scheduled__2024-03-10T12:00:00+00:00 [scheduled]>
        <TaskInstance: catchup_dag.not_eligible_to_vote scheduled__2024-03-10T00:00:00+00:00 [scheduled]>
[2024-03-11T20:42:25.120+0530] {taskinstance.py:2283} WARNING - cannot record scheduled_duration for task over_18 because previous state change time has not been saved
[2024-03-11T20:42:25.121+0530] {taskinstance.py:2283} WARNING - cannot record scheduled_duration for task branch because previous state change time has not been saved
[2024-03-11T20:42:25.121+0530] {taskinstance.py:2283} WARNING - cannot record scheduled_duration for task not_eligible_to_vote because previous state change time has not been saved
[2024-03-11T20:42:25.122+0530] {scheduler_job_runner.py:646} INFO - Sending TaskInstanceKey(dag_id='catchup_dag', task_id='over_18', run_id='scheduled__2024-03-11T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default
[2024-03-11T20:42:25.122+0530] {base_executor.py:146} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'catchup_dag', 'over_18', 'scheduled__2024-03-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_catchup.py']
[2024-03-11T20:42:25.122+0530] {scheduler_job_runner.py:646} INFO - Sending TaskInstanceKey(dag_id='catchup_dag', task_id='branch', run_id='scheduled__2024-03-10T12:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default
[2024-03-11T20:42:25.122+0530] {base_executor.py:146} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'catchup_dag', 'branch', 'scheduled__2024-03-10T12:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_catchup.py']
[2024-03-11T20:42:25.123+0530] {scheduler_job_runner.py:646} INFO - Sending TaskInstanceKey(dag_id='catchup_dag', task_id='not_eligible_to_vote', run_id='scheduled__2024-03-10T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default
[2024-03-11T20:42:25.123+0530] {base_executor.py:146} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'catchup_dag', 'not_eligible_to_vote', 'scheduled__2024-03-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_catchup.py']
[2024-03-11T20:42:25.142+0530] {sequential_executor.py:73} INFO - Executing command: ['airflow', 'tasks', 'run', 'catchup_dag', 'over_18', 'scheduled__2024-03-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_catchup.py']
[2024-03-11T20:42:26.264+0530] {dagbag.py:540} INFO - Filling up the DagBag from /home/kai/airflow/dags/execute_catchup.py
Changing /home/kai/airflow/logs/dag_id=catchup_dag/run_id=scheduled__2024-03-11T00:00:00+00:00/task_id=over_18 permission to 509
Changing /home/kai/airflow/logs/dag_id=catchup_dag/run_id=scheduled__2024-03-11T00:00:00+00:00 permission to 509
[2024-03-11T20:42:26.503+0530] {task_command.py:423} INFO - Running <TaskInstance: catchup_dag.over_18 scheduled__2024-03-11T00:00:00+00:00 [queued]> on host BreakThrough.
[2024-03-11T20:42:27.749+0530] {sequential_executor.py:73} INFO - Executing command: ['airflow', 'tasks', 'run', 'catchup_dag', 'branch', 'scheduled__2024-03-10T12:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_catchup.py']
[2024-03-11T20:42:29.485+0530] {dagbag.py:540} INFO - Filling up the DagBag from /home/kai/airflow/dags/execute_catchup.py
Changing /home/kai/airflow/logs/dag_id=catchup_dag/run_id=scheduled__2024-03-10T12:00:00+00:00/task_id=branch permission to509
[2024-03-11T20:42:29.744+0530] {task_command.py:423} INFO - Running <TaskInstance: catchup_dag.branch scheduled__2024-03-10T12:00:00+00:00 [queued]> on host BreakThrough.
[2024-03-11T20:42:30.743+0530] {sequential_executor.py:73} INFO - Executing command: ['airflow', 'tasks', 'run', 'catchup_dag', 'not_eligible_to_vote', 'scheduled__2024-03-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_catchup.py']
[2024-03-11T20:42:32.218+0530] {dagbag.py:540} INFO - Filling up the DagBag from /home/kai/airflow/dags/execute_catchup.py
Changing /home/kai/airflow/logs/dag_id=catchup_dag/run_id=scheduled__2024-03-10T00:00:00+00:00/task_id=not_eligible_to_votepermission to 509
[2024-03-11T20:42:32.478+0530] {task_command.py:423} INFO - Running <TaskInstance: catchup_dag.not_eligible_to_vote scheduled__2024-03-10T00:00:00+00:00 [queued]> on host BreakThrough.
[2024-03-11T20:42:33.264+0530] {scheduler_job_runner.py:696} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='catchup_dag', task_id='over_18', run_id='scheduled__2024-03-11T00:00:00+00:00', try_number=1, map_index=-1)
[2024-03-11T20:42:33.264+0530] {scheduler_job_runner.py:696} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='catchup_dag', task_id='branch', run_id='scheduled__2024-03-10T12:00:00+00:00', try_number=1, map_index=-1)
[2024-03-11T20:42:33.265+0530] {scheduler_job_runner.py:696} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='catchup_dag', task_id='not_eligible_to_vote', run_id='scheduled__2024-03-10T00:00:00+00:00', try_number=1, map_index=-1)
[2024-03-11T20:42:33.276+0530] {scheduler_job_runner.py:733} INFO - TaskInstance Finished: dag_id=catchup_dag, task_id=not_eligible_to_vote, run_id=scheduled__2024-03-10T00:00:00+00:00, map_index=-1, run_start_date=2024-03-11 15:12:32.586436+00:00, run_end_date=2024-03-11 15:12:32.887749+00:00, run_duration=0.301313, state=success, executor_state=success, try_number=1,max_tries=0, job_id=178, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-03-11 15:12:25.119207+00:00, queued_by_job_id=172, pid=253640
[2024-03-11T20:42:33.276+0530] {scheduler_job_runner.py:733} INFO - TaskInstance Finished: dag_id=catchup_dag, task_id=branch, run_id=scheduled__2024-03-10T12:00:00+00:00, map_index=-1, run_start_date=2024-03-11 15:12:29.838271+00:00, run_end_date=2024-03-11 15:12:30.206442+00:00, run_duration=0.368171, state=success, executor_state=success, try_number=1, max_tries=0, job_id=177, pool=default_pool, queue=default, priority_weight=3, operator=BranchPythonOperator, queued_dttm=2024-03-11 15:12:25.119207+00:00, queued_by_job_id=172, pid=253633
[2024-03-11T20:42:33.276+0530] {scheduler_job_runner.py:733} INFO - TaskInstance Finished: dag_id=catchup_dag, task_id=over_18, run_id=scheduled__2024-03-11T00:00:00+00:00, map_index=-1, run_start_date=2024-03-11 15:12:26.624469+00:00, run_end_date=2024-03-11 15:12:27.252415+00:00, run_duration=0.627946, state=success, executor_state=success, try_number=1, max_tries=0,job_id=176, pool=default_pool, queue=default, priority_weight=4, operator=PythonOperator, queued_dttm=2024-03-11 15:12:25.119207+00:00, queued_by_job_id=172, pid=253630
[2024-03-11T20:42:33.560+0530] {dagrun.py:795} INFO - Marking run <DagRun catchup_dag @ 2024-03-10 00:00:00+00:00: scheduled__2024-03-10T00:00:00+00:00, state:running, queued_at: 2024-03-11 15:12:15.377713+00:00. externally triggered: False> successful
[2024-03-11T20:42:33.560+0530] {dagrun.py:846} INFO - DagRun Finished: dag_id=catchup_dag, execution_date=2024-03-10 00:00:00+00:00, run_id=scheduled__2024-03-10T00:00:00+00:00, run_start_date=2024-03-11 15:12:15.434288+00:00, run_end_date=2024-03-11 15:12:33.560810+00:00, run_duration=18.126522, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-03-10 00:00:00+00:00, data_interval_end=2024-03-10 12:00:00+00:00, dag_hash=4f7883b017859a72a3cf82330f5591ef
[2024-03-11T20:42:33.566+0530] {dag.py:3834} INFO - Setting next_dagrun for catchup_dag to 2024-03-10 12:00:00+00:00, run_after=2024-03-11 00:00:00+00:00
[2024-03-11T20:42:33.591+0530] {scheduler_job_runner.py:424} INFO - 2 tasks up for execution:
        <TaskInstance: catchup_dag.branch scheduled__2024-03-11T00:00:00+00:00 [scheduled]>
        <TaskInstance: catchup_dag.not_eligible_to_vote scheduled__2024-03-10T12:00:00+00:00 [scheduled]>
[2024-03-11T20:42:33.592+0530] {scheduler_job_runner.py:487} INFO - DAG catchup_dag has 0/16 running and queued tasks
[2024-03-11T20:42:33.592+0530] {scheduler_job_runner.py:487} INFO - DAG catchup_dag has 1/16 running and queued tasks
[2024-03-11T20:42:33.592+0530] {scheduler_job_runner.py:603} INFO - Setting the following tasks to queued state:
        <TaskInstance: catchup_dag.branch scheduled__2024-03-11T00:00:00+00:00 [scheduled]>
        <TaskInstance: catchup_dag.not_eligible_to_vote scheduled__2024-03-10T12:00:00+00:00 [scheduled]>
[2024-03-11T20:42:33.594+0530] {taskinstance.py:2283} WARNING - cannot record scheduled_duration for task branch because previous state change time has not been saved
[2024-03-11T20:42:33.594+0530] {taskinstance.py:2283} WARNING - cannot record scheduled_duration for task not_eligible_to_vote because previous state change time has not been saved
[2024-03-11T20:42:33.594+0530] {scheduler_job_runner.py:646} INFO - Sending TaskInstanceKey(dag_id='catchup_dag', task_id='branch', run_id='scheduled__2024-03-11T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default
[2024-03-11T20:42:33.594+0530] {base_executor.py:146} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'catchup_dag', 'branch', 'scheduled__2024-03-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_catchup.py']
[2024-03-11T20:42:33.595+0530] {scheduler_job_runner.py:646} INFO - Sending TaskInstanceKey(dag_id='catchup_dag', task_id='not_eligible_to_vote', run_id='scheduled__2024-03-10T12:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default
[2024-03-11T20:42:33.595+0530] {base_executor.py:146} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'catchup_dag', 'not_eligible_to_vote', 'scheduled__2024-03-10T12:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_catchup.py']
[2024-03-11T20:42:33.613+0530] {sequential_executor.py:73} INFO - Executing command: ['airflow', 'tasks', 'run', 'catchup_dag', 'branch', 'scheduled__2024-03-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_catchup.py']
[2024-03-11T20:42:34.760+0530] {dagbag.py:540} INFO - Filling up the DagBag from /home/kai/airflow/dags/execute_catchup.py
Changing /home/kai/airflow/logs/dag_id=catchup_dag/run_id=scheduled__2024-03-11T00:00:00+00:00/task_id=branch permission to509
[2024-03-11T20:42:34.951+0530] {task_command.py:423} INFO - Running <TaskInstance: catchup_dag.branch scheduled__2024-03-11T00:00:00+00:00 [queued]> on host BreakThrough.
[2024-03-11T20:42:35.744+0530] {sequential_executor.py:73} INFO - Executing command: ['airflow', 'tasks', 'run', 'catchup_dag', 'not_eligible_to_vote', 'scheduled__2024-03-10T12:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_catchup.py']
[2024-03-11T20:42:37.241+0530] {dagbag.py:540} INFO - Filling up the DagBag from /home/kai/airflow/dags/execute_catchup.py
Changing /home/kai/airflow/logs/dag_id=catchup_dag/run_id=scheduled__2024-03-10T12:00:00+00:00/task_id=not_eligible_to_votepermission to 509
[2024-03-11T20:42:37.457+0530] {task_command.py:423} INFO - Running <TaskInstance: catchup_dag.not_eligible_to_vote scheduled__2024-03-10T12:00:00+00:00 [queued]> on host BreakThrough.
[2024-03-11T20:42:38.210+0530] {scheduler_job_runner.py:696} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='catchup_dag', task_id='branch', run_id='scheduled__2024-03-11T00:00:00+00:00', try_number=1, map_index=-1)
[2024-03-11T20:42:38.210+0530] {scheduler_job_runner.py:696} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='catchup_dag', task_id='not_eligible_to_vote', run_id='scheduled__2024-03-10T12:00:00+00:00', try_number=1, map_index=-1)
[2024-03-11T20:42:38.217+0530] {scheduler_job_runner.py:733} INFO - TaskInstance Finished: dag_id=catchup_dag, task_id=not_eligible_to_vote, run_id=scheduled__2024-03-10T12:00:00+00:00, map_index=-1, run_start_date=2024-03-11 15:12:37.538433+00:00, run_end_date=2024-03-11 15:12:37.748233+00:00, run_duration=0.2098, state=success, executor_state=success, try_number=1, max_tries=0, job_id=180, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-03-1115:12:33.593254+00:00, queued_by_job_id=172, pid=253645
[2024-03-11T20:42:38.218+0530] {scheduler_job_runner.py:733} INFO - TaskInstance Finished: dag_id=catchup_dag, task_id=branch, run_id=scheduled__2024-03-11T00:00:00+00:00, map_index=-1, run_start_date=2024-03-11 15:12:35.024841+00:00, run_end_date=2024-03-11 15:12:35.304899+00:00, run_duration=0.280058, state=success, executor_state=success, try_number=1, max_tries=0, job_id=179, pool=default_pool, queue=default, priority_weight=3, operator=BranchPythonOperator, queued_dttm=2024-03-11 15:12:33.593254+00:00, queued_by_job_id=172, pid=253643
[2024-03-11T20:42:38.447+0530] {dag.py:3834} INFO - Setting next_dagrun for catchup_dag to 2024-03-11 00:00:00+00:00, run_after=2024-03-11 12:00:00+00:00
[2024-03-11T20:42:38.480+0530] {dagrun.py:795} INFO - Marking run <DagRun catchup_dag @ 2024-03-10 12:00:00+00:00: scheduled__2024-03-10T12:00:00+00:00, state:running, queued_at: 2024-03-11 15:12:19.858596+00:00. externally triggered: False> successful
[2024-03-11T20:42:38.480+0530] {dagrun.py:846} INFO - DagRun Finished: dag_id=catchup_dag, execution_date=2024-03-10 12:00:00+00:00, run_id=scheduled__2024-03-10T12:00:00+00:00, run_start_date=2024-03-11 15:12:19.903666+00:00, run_end_date=2024-03-11 15:12:38.480542+00:00, run_duration=18.576876, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-03-10 12:00:00+00:00, data_interval_end=2024-03-11 00:00:00+00:00, dag_hash=4f7883b017859a72a3cf82330f5591ef
[2024-03-11T20:42:38.484+0530] {dag.py:3834} INFO - Setting next_dagrun for catchup_dag to 2024-03-11 00:00:00+00:00, run_after=2024-03-11 12:00:00+00:00
[2024-03-11T20:42:38.508+0530] {scheduler_job_runner.py:424} INFO - 1 tasks up for execution:
        <TaskInstance: catchup_dag.eligible_to_vote scheduled__2024-03-11T00:00:00+00:00 [scheduled]>
[2024-03-11T20:42:38.509+0530] {scheduler_job_runner.py:487} INFO - DAG catchup_dag has 0/16 running and queued tasks
[2024-03-11T20:42:38.509+0530] {scheduler_job_runner.py:603} INFO - Setting the following tasks to queued state:
        <TaskInstance: catchup_dag.eligible_to_vote scheduled__2024-03-11T00:00:00+00:00 [scheduled]>
[2024-03-11T20:42:38.510+0530] {taskinstance.py:2283} WARNING - cannot record scheduled_duration for task eligible_to_vote because previous state change time has not been saved
[2024-03-11T20:42:38.511+0530] {scheduler_job_runner.py:646} INFO - Sending TaskInstanceKey(dag_id='catchup_dag', task_id='eligible_to_vote', run_id='scheduled__2024-03-11T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default
[2024-03-11T20:42:38.512+0530] {base_executor.py:146} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'catchup_dag', 'eligible_to_vote', 'scheduled__2024-03-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_catchup.py']
[2024-03-11T20:42:38.535+0530] {sequential_executor.py:73} INFO - Executing command: ['airflow', 'tasks', 'run', 'catchup_dag', 'eligible_to_vote', 'scheduled__2024-03-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_catchup.py']
[2024-03-11T20:42:39.948+0530] {dagbag.py:540} INFO - Filling up the DagBag from /home/kai/airflow/dags/execute_catchup.py
Changing /home/kai/airflow/logs/dag_id=catchup_dag/run_id=scheduled__2024-03-11T00:00:00+00:00/task_id=eligible_to_vote permission to 509
[2024-03-11T20:42:40.340+0530] {task_command.py:423} INFO - Running <TaskInstance: catchup_dag.eligible_to_vote scheduled__2024-03-11T00:00:00+00:00 [queued]> on host BreakThrough.
[2024-03-11T20:42:41.139+0530] {scheduler_job_runner.py:696} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='catchup_dag', task_id='eligible_to_vote', run_id='scheduled__2024-03-11T00:00:00+00:00', try_number=1, map_index=-1)
[2024-03-11T20:42:41.151+0530] {scheduler_job_runner.py:733} INFO - TaskInstance Finished: dag_id=catchup_dag, task_id=eligible_to_vote, run_id=scheduled__2024-03-11T00:00:00+00:00, map_index=-1, run_start_date=2024-03-11 15:12:40.457025+00:00, run_end_date=2024-03-11 15:12:40.693508+00:00, run_duration=0.236483, state=success, executor_state=success, try_number=1, max_tries=0, job_id=181, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-03-11 15:12:38.509970+00:00, queued_by_job_id=172, pid=253648
[2024-03-11T20:42:42.119+0530] {dag.py:3834} INFO - Setting next_dagrun for catchup_dag to 2024-03-11 12:00:00+00:00, run_after=2024-03-12 00:00:00+00:00
[2024-03-11T20:42:42.150+0530] {dagrun.py:795} INFO - Marking run <DagRun catchup_dag @ 2024-03-11 00:00:00+00:00: scheduled__2024-03-11T00:00:00+00:00, state:running, queued_at: 2024-03-11 15:12:24.997233+00:00. externally triggered: False> successful
[2024-03-11T20:42:42.151+0530] {dagrun.py:846} INFO - DagRun Finished: dag_id=catchup_dag, execution_date=2024-03-11 00:00:00+00:00, run_id=scheduled__2024-03-11T00:00:00+00:00, run_start_date=2024-03-11 15:12:25.032131+00:00, run_end_date=2024-03-11 15:12:42.151423+00:00, run_duration=17.119292, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-03-11 00:00:00+00:00, data_interval_end=2024-03-11 12:00:00+00:00, dag_hash=4f7883b017859a72a3cf82330f5591ef
[2024-03-11T20:42:42.155+0530] {dag.py:3834} INFO - Setting next_dagrun for catchup_dag to 2024-03-11 12:00:00+00:00, run_after=2024-03-12 00:00:00+00:00
[2024-03-11T20:47:15.033+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-11T20:52:15.072+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-11T20:57:15.123+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-11T21:02:15.160+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-11T21:07:15.193+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-11T21:12:15.245+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-11T21:17:15.285+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-11T21:22:15.338+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-11T21:27:15.366+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-11T21:32:15.417+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-11T21:35:26.412+0530] {dag.py:3834} INFO - Setting next_dagrun for python_operator_dag to 2024-03-11 00:00:00+00:00, run_after=2024-03-12 00:00:00+00:00
[2024-03-11T21:35:26.501+0530] {scheduler_job_runner.py:424} INFO - 1 tasks up for execution:
        <TaskInstance: python_operator_dag.taskA scheduled__2024-03-10T00:00:00+00:00 [scheduled]>
[2024-03-11T21:35:26.501+0530] {scheduler_job_runner.py:487} INFO - DAG python_operator_dag has 0/16 running and queued tasks
[2024-03-11T21:35:26.502+0530] {scheduler_job_runner.py:603} INFO - Setting the following tasks to queued state:
        <TaskInstance: python_operator_dag.taskA scheduled__2024-03-10T00:00:00+00:00 [scheduled]>
[2024-03-11T21:35:26.503+0530] {taskinstance.py:2283} WARNING - cannot record scheduled_duration for task taskA because previous state change time has not been saved
[2024-03-11T21:35:26.504+0530] {scheduler_job_runner.py:646} INFO - Sending TaskInstanceKey(dag_id='python_operator_dag', task_id='taskA', run_id='scheduled__2024-03-10T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default
[2024-03-11T21:35:26.505+0530] {base_executor.py:146} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'python_operator_dag', 'taskA', 'scheduled__2024-03-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_python_operators.py']
[2024-03-11T21:35:26.525+0530] {sequential_executor.py:73} INFO - Executing command: ['airflow', 'tasks', 'run', 'python_operator_dag', 'taskA', 'scheduled__2024-03-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_python_operators.py']
[2024-03-11T21:35:27.777+0530] {dagbag.py:540} INFO - Filling up the DagBag from /home/kai/airflow/dags/execute_python_operators.py
Changing /home/kai/airflow/logs/dag_id=python_operator_dag/run_id=scheduled__2024-03-10T00:00:00+00:00/task_id=taskA permission to 509
Changing /home/kai/airflow/logs/dag_id=python_operator_dag/run_id=scheduled__2024-03-10T00:00:00+00:00 permission to 509
[2024-03-11T21:35:27.994+0530] {task_command.py:423} INFO - Running <TaskInstance: python_operator_dag.taskA scheduled__2024-03-10T00:00:00+00:00 [queued]> on host BreakThrough.
[2024-03-11T21:35:28.874+0530] {scheduler_job_runner.py:696} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operator_dag', task_id='taskA', run_id='scheduled__2024-03-10T00:00:00+00:00', try_number=1, map_index=-1)
[2024-03-11T21:35:28.878+0530] {scheduler_job_runner.py:733} INFO - TaskInstance Finished: dag_id=python_operator_dag, task_id=taskA, run_id=scheduled__2024-03-10T00:00:00+00:00, map_index=-1, run_start_date=2024-03-11 16:05:28.067163+00:00, run_end_date=2024-03-11 16:05:28.408631+00:00, run_duration=0.341468, state=success, executor_state=success, try_number=1, max_tries=0, job_id=182, pool=default_pool, queue=default, priority_weight=4, operator=PythonOperator, queued_dttm=2024-03-11 16:05:26.502568+00:00, queued_by_job_id=172, pid=256924
[2024-03-11T21:35:28.935+0530] {scheduler_job_runner.py:424} INFO - 2 tasks up for execution:
        <TaskInstance: python_operator_dag.taskB scheduled__2024-03-10T00:00:00+00:00 [scheduled]>
        <TaskInstance: python_operator_dag.taskC scheduled__2024-03-10T00:00:00+00:00 [scheduled]>
[2024-03-11T21:35:28.936+0530] {scheduler_job_runner.py:487} INFO - DAG python_operator_dag has 0/16 running and queued tasks
[2024-03-11T21:35:28.936+0530] {scheduler_job_runner.py:487} INFO - DAG python_operator_dag has 1/16 running and queued tasks
[2024-03-11T21:35:28.936+0530] {scheduler_job_runner.py:603} INFO - Setting the following tasks to queued state:
        <TaskInstance: python_operator_dag.taskB scheduled__2024-03-10T00:00:00+00:00 [scheduled]>
        <TaskInstance: python_operator_dag.taskC scheduled__2024-03-10T00:00:00+00:00 [scheduled]>
[2024-03-11T21:35:28.940+0530] {taskinstance.py:2283} WARNING - cannot record scheduled_duration for task taskB because previous state change time has not been saved
[2024-03-11T21:35:28.940+0530] {taskinstance.py:2283} WARNING - cannot record scheduled_duration for task taskC because previous state change time has not been saved
[2024-03-11T21:35:28.941+0530] {scheduler_job_runner.py:646} INFO - Sending TaskInstanceKey(dag_id='python_operator_dag', task_id='taskB', run_id='scheduled__2024-03-10T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default
[2024-03-11T21:35:28.942+0530] {base_executor.py:146} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'python_operator_dag', 'taskB', 'scheduled__2024-03-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_python_operators.py']
[2024-03-11T21:35:28.942+0530] {scheduler_job_runner.py:646} INFO - Sending TaskInstanceKey(dag_id='python_operator_dag', task_id='taskC', run_id='scheduled__2024-03-10T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default
[2024-03-11T21:35:28.943+0530] {base_executor.py:146} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'python_operator_dag', 'taskC', 'scheduled__2024-03-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_python_operators.py']
[2024-03-11T21:35:28.960+0530] {sequential_executor.py:73} INFO - Executing command: ['airflow', 'tasks', 'run', 'python_operator_dag', 'taskB', 'scheduled__2024-03-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_python_operators.py']
[2024-03-11T21:35:30.402+0530] {dagbag.py:540} INFO - Filling up the DagBag from /home/kai/airflow/dags/execute_python_operators.py
Changing /home/kai/airflow/logs/dag_id=python_operator_dag/run_id=scheduled__2024-03-10T00:00:00+00:00/task_id=taskB permission to 509
[2024-03-11T21:35:30.631+0530] {task_command.py:423} INFO - Running <TaskInstance: python_operator_dag.taskB scheduled__2024-03-10T00:00:00+00:00 [queued]> on host BreakThrough.
[2024-03-11T21:35:31.339+0530] {sequential_executor.py:73} INFO - Executing command: ['airflow', 'tasks', 'run', 'python_operator_dag', 'taskC', 'scheduled__2024-03-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_python_operators.py']
[2024-03-11T21:35:32.631+0530] {dagbag.py:540} INFO - Filling up the DagBag from /home/kai/airflow/dags/execute_python_operators.py
Changing /home/kai/airflow/logs/dag_id=python_operator_dag/run_id=scheduled__2024-03-10T00:00:00+00:00/task_id=taskC permission to 509
[2024-03-11T21:35:32.931+0530] {task_command.py:423} INFO - Running <TaskInstance: python_operator_dag.taskC scheduled__2024-03-10T00:00:00+00:00 [queued]> on host BreakThrough.
[2024-03-11T21:35:33.890+0530] {scheduler_job_runner.py:696} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operator_dag', task_id='taskB', run_id='scheduled__2024-03-10T00:00:00+00:00', try_number=1, map_index=-1)
[2024-03-11T21:35:33.890+0530] {scheduler_job_runner.py:696} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operator_dag', task_id='taskC', run_id='scheduled__2024-03-10T00:00:00+00:00', try_number=1, map_index=-1)
[2024-03-11T21:35:33.895+0530] {scheduler_job_runner.py:733} INFO - TaskInstance Finished: dag_id=python_operator_dag, task_id=taskB, run_id=scheduled__2024-03-10T00:00:00+00:00, map_index=-1, run_start_date=2024-03-11 16:05:30.700130+00:00, run_end_date=2024-03-11 16:05:30.912247+00:00, run_duration=0.212117, state=success, executor_state=success, try_number=1, max_tries=0, job_id=183, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-03-11 16:05:28.938090+00:00, queued_by_job_id=172, pid=256928
[2024-03-11T21:35:33.895+0530] {scheduler_job_runner.py:733} INFO - TaskInstance Finished: dag_id=python_operator_dag, task_id=taskC, run_id=scheduled__2024-03-10T00:00:00+00:00, map_index=-1, run_start_date=2024-03-11 16:05:33.015999+00:00, run_end_date=2024-03-11 16:05:33.318742+00:00, run_duration=0.302743, state=success, executor_state=success, try_number=1, max_tries=0, job_id=184, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-03-11 16:05:28.938090+00:00, queued_by_job_id=172, pid=256930
[2024-03-11T21:35:34.177+0530] {scheduler_job_runner.py:424} INFO - 1 tasks up for execution:
        <TaskInstance: python_operator_dag.taskD scheduled__2024-03-10T00:00:00+00:00 [scheduled]>
[2024-03-11T21:35:34.177+0530] {scheduler_job_runner.py:487} INFO - DAG python_operator_dag has 0/16 running and queued tasks
[2024-03-11T21:35:34.177+0530] {scheduler_job_runner.py:603} INFO - Setting the following tasks to queued state:
        <TaskInstance: python_operator_dag.taskD scheduled__2024-03-10T00:00:00+00:00 [scheduled]>
[2024-03-11T21:35:34.178+0530] {taskinstance.py:2283} WARNING - cannot record scheduled_duration for task taskD because previous state change time has not been saved
[2024-03-11T21:35:34.179+0530] {scheduler_job_runner.py:646} INFO - Sending TaskInstanceKey(dag_id='python_operator_dag', task_id='taskD', run_id='scheduled__2024-03-10T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default
[2024-03-11T21:35:34.179+0530] {base_executor.py:146} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'python_operator_dag', 'taskD', 'scheduled__2024-03-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_python_operators.py']
[2024-03-11T21:35:34.197+0530] {sequential_executor.py:73} INFO - Executing command: ['airflow', 'tasks', 'run', 'python_operator_dag', 'taskD', 'scheduled__2024-03-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_python_operators.py']
[2024-03-11T21:35:35.290+0530] {dagbag.py:540} INFO - Filling up the DagBag from /home/kai/airflow/dags/execute_python_operators.py
Changing /home/kai/airflow/logs/dag_id=python_operator_dag/run_id=scheduled__2024-03-10T00:00:00+00:00/task_id=taskD permission to 509
[2024-03-11T21:35:35.482+0530] {task_command.py:423} INFO - Running <TaskInstance: python_operator_dag.taskD scheduled__2024-03-10T00:00:00+00:00 [queued]> on host BreakThrough.
[2024-03-11T21:35:36.179+0530] {scheduler_job_runner.py:696} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operator_dag', task_id='taskD', run_id='scheduled__2024-03-10T00:00:00+00:00', try_number=1, map_index=-1)
[2024-03-11T21:35:36.183+0530] {scheduler_job_runner.py:733} INFO - TaskInstance Finished: dag_id=python_operator_dag, task_id=taskD, run_id=scheduled__2024-03-10T00:00:00+00:00, map_index=-1, run_start_date=2024-03-11 16:05:35.548366+00:00, run_end_date=2024-03-11 16:05:35.744816+00:00, run_duration=0.19645, state=success, executor_state=success, try_number=1, max_tries=0, job_id=185, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-03-11 16:05:34.178225+00:00, queued_by_job_id=172, pid=256933
[2024-03-11T21:35:36.215+0530] {dagrun.py:795} INFO - Marking run <DagRun python_operator_dag @ 2024-03-10 00:00:00+00:00: scheduled__2024-03-10T00:00:00+00:00, state:running, queued_at: 2024-03-11 16:05:26.403914+00:00. externally triggered: False> successful
[2024-03-11T21:35:36.215+0530] {dagrun.py:846} INFO - DagRun Finished: dag_id=python_operator_dag, execution_date=2024-03-10 00:00:00+00:00, run_id=scheduled__2024-03-10T00:00:00+00:00, run_start_date=2024-03-11 16:05:26.448047+00:00, run_end_date=2024-03-11 16:05:36.215695+00:00, run_duration=9.767648, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-03-10 00:00:00+00:00, data_interval_end=2024-03-11 00:00:00+00:00, dag_hash=a20110bb3fc3f3e578df332951629bcd
[2024-03-11T21:35:36.217+0530] {dag.py:3834} INFO - Setting next_dagrun for python_operator_dag to 2024-03-11 00:00:00+00:00, run_after=2024-03-12 00:00:00+00:00
[2024-03-11T21:37:15.452+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-11T21:42:15.487+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-11T21:42:22.271+0530] {processor.py:376} WARNING - Error when trying to pre-import module 'airflow.utils.days' found in /home/kai/airflow/dags/dag_with_taskflow.py: No module named 'airflow.utils.days'
[2024-03-11T21:42:53.211+0530] {processor.py:376} WARNING - Error when trying to pre-import module 'airflow.utils.days' found in /home/kai/airflow/dags/dag_with_taskflow.py: No module named 'airflow.utils.days'
[2024-03-11T21:43:00.332+0530] {processor.py:376} WARNING - Error when trying to pre-import module 'airflow.utils.days' found in /home/kai/airflow/dags/dag_with_taskflow.py: No module named 'airflow.utils.days'
[2024-03-11T21:43:31.367+0530] {processor.py:376} WARNING - Error when trying to pre-import module 'airflow.utils.days' found in /home/kai/airflow/dags/dag_with_taskflow.py: No module named 'airflow.utils.days'
[2024-03-11T21:43:59.958+0530] {dag.py:3834} INFO - Setting next_dagrun for simple_dag_with_taskflow to 2024-03-11 00:00:00+00:00, run_after=2024-03-12 00:00:00+00:00
[2024-03-11T21:44:00.061+0530] {scheduler_job_runner.py:424} INFO - 1 tasks up for execution:
        <TaskInstance: simple_dag_with_taskflow.task_a scheduled__2024-03-10T00:00:00+00:00 [scheduled]>
[2024-03-11T21:44:00.062+0530] {scheduler_job_runner.py:487} INFO - DAG simple_dag_with_taskflow has 0/16 running and queued tasks
[2024-03-11T21:44:00.062+0530] {scheduler_job_runner.py:603} INFO - Setting the following tasks to queued state:
        <TaskInstance: simple_dag_with_taskflow.task_a scheduled__2024-03-10T00:00:00+00:00 [scheduled]>
[2024-03-11T21:44:00.063+0530] {taskinstance.py:2283} WARNING - cannot record scheduled_duration for task task_a because previous state change time has not been saved
[2024-03-11T21:44:00.064+0530] {scheduler_job_runner.py:646} INFO - Sending TaskInstanceKey(dag_id='simple_dag_with_taskflow', task_id='task_a', run_id='scheduled__2024-03-10T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 4and queue default
[2024-03-11T21:44:00.065+0530] {base_executor.py:146} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'simple_dag_with_taskflow', 'task_a', 'scheduled__2024-03-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_with_taskflow.py']
[2024-03-11T21:44:00.084+0530] {sequential_executor.py:73} INFO - Executing command: ['airflow', 'tasks', 'run', 'simple_dag_with_taskflow', 'task_a', 'scheduled__2024-03-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_with_taskflow.py']
[2024-03-11T21:44:01.267+0530] {dagbag.py:540} INFO - Filling up the DagBag from /home/kai/airflow/dags/dag_with_taskflow.py
Changing /home/kai/airflow/logs/dag_id=simple_dag_with_taskflow/run_id=scheduled__2024-03-10T00:00:00+00:00/task_id=task_a permission to 509
Changing /home/kai/airflow/logs/dag_id=simple_dag_with_taskflow/run_id=scheduled__2024-03-10T00:00:00+00:00 permission to 509
Changing /home/kai/airflow/logs/dag_id=simple_dag_with_taskflow permission to 509
[2024-03-11T21:44:01.486+0530] {task_command.py:423} INFO - Running <TaskInstance: simple_dag_with_taskflow.task_a scheduled__2024-03-10T00:00:00+00:00 [queued]> on host BreakThrough.
[2024-03-11T21:44:02.202+0530] {scheduler_job_runner.py:696} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='simple_dag_with_taskflow', task_id='task_a', run_id='scheduled__2024-03-10T00:00:00+00:00', try_number=1, map_index=-1)
[2024-03-11T21:44:02.207+0530] {scheduler_job_runner.py:733} INFO - TaskInstance Finished: dag_id=simple_dag_with_taskflow,task_id=task_a, run_id=scheduled__2024-03-10T00:00:00+00:00, map_index=-1, run_start_date=2024-03-11 16:14:01.557973+00:00,run_end_date=2024-03-11 16:14:01.759222+00:00, run_duration=0.201249, state=success, executor_state=success, try_number=1, max_tries=0, job_id=186, pool=default_pool, queue=default, priority_weight=4, operator=_PythonDecoratedOperator, queued_dttm=2024-03-11 16:14:00.063046+00:00, queued_by_job_id=172, pid=257488
[2024-03-11T21:44:02.264+0530] {scheduler_job_runner.py:424} INFO - 2 tasks up for execution:
        <TaskInstance: simple_dag_with_taskflow.task_b scheduled__2024-03-10T00:00:00+00:00 [scheduled]>
        <TaskInstance: simple_dag_with_taskflow.task_c scheduled__2024-03-10T00:00:00+00:00 [scheduled]>
[2024-03-11T21:44:02.264+0530] {scheduler_job_runner.py:487} INFO - DAG simple_dag_with_taskflow has 0/16 running and queued tasks
[2024-03-11T21:44:02.265+0530] {scheduler_job_runner.py:487} INFO - DAG simple_dag_with_taskflow has 1/16 running and queued tasks
[2024-03-11T21:44:02.265+0530] {scheduler_job_runner.py:603} INFO - Setting the following tasks to queued state:
        <TaskInstance: simple_dag_with_taskflow.task_b scheduled__2024-03-10T00:00:00+00:00 [scheduled]>
        <TaskInstance: simple_dag_with_taskflow.task_c scheduled__2024-03-10T00:00:00+00:00 [scheduled]>
[2024-03-11T21:44:02.266+0530] {taskinstance.py:2283} WARNING - cannot record scheduled_duration for task task_b because previous state change time has not been saved
[2024-03-11T21:44:02.266+0530] {taskinstance.py:2283} WARNING - cannot record scheduled_duration for task task_c because previous state change time has not been saved
[2024-03-11T21:44:02.267+0530] {scheduler_job_runner.py:646} INFO - Sending TaskInstanceKey(dag_id='simple_dag_with_taskflow', task_id='task_b', run_id='scheduled__2024-03-10T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2and queue default
[2024-03-11T21:44:02.267+0530] {base_executor.py:146} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'simple_dag_with_taskflow', 'task_b', 'scheduled__2024-03-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_with_taskflow.py']
[2024-03-11T21:44:02.268+0530] {scheduler_job_runner.py:646} INFO - Sending TaskInstanceKey(dag_id='simple_dag_with_taskflow', task_id='task_c', run_id='scheduled__2024-03-10T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2and queue default
[2024-03-11T21:44:02.269+0530] {base_executor.py:146} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'simple_dag_with_taskflow', 'task_c', 'scheduled__2024-03-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_with_taskflow.py']
[2024-03-11T21:44:02.293+0530] {sequential_executor.py:73} INFO - Executing command: ['airflow', 'tasks', 'run', 'simple_dag_with_taskflow', 'task_b', 'scheduled__2024-03-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_with_taskflow.py']
[2024-03-11T21:44:03.416+0530] {dagbag.py:540} INFO - Filling up the DagBag from /home/kai/airflow/dags/dag_with_taskflow.py
Changing /home/kai/airflow/logs/dag_id=simple_dag_with_taskflow/run_id=scheduled__2024-03-10T00:00:00+00:00/task_id=task_b permission to 509
[2024-03-11T21:44:03.650+0530] {task_command.py:423} INFO - Running <TaskInstance: simple_dag_with_taskflow.task_b scheduled__2024-03-10T00:00:00+00:00 [queued]> on host BreakThrough.
[2024-03-11T21:44:04.416+0530] {sequential_executor.py:73} INFO - Executing command: ['airflow', 'tasks', 'run', 'simple_dag_with_taskflow', 'task_c', 'scheduled__2024-03-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_with_taskflow.py']
[2024-03-11T21:44:05.695+0530] {dagbag.py:540} INFO - Filling up the DagBag from /home/kai/airflow/dags/dag_with_taskflow.py
Changing /home/kai/airflow/logs/dag_id=simple_dag_with_taskflow/run_id=scheduled__2024-03-10T00:00:00+00:00/task_id=task_c permission to 509
[2024-03-11T21:44:05.925+0530] {task_command.py:423} INFO - Running <TaskInstance: simple_dag_with_taskflow.task_c scheduled__2024-03-10T00:00:00+00:00 [queued]> on host BreakThrough.
[2024-03-11T21:44:06.685+0530] {scheduler_job_runner.py:696} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='simple_dag_with_taskflow', task_id='task_b', run_id='scheduled__2024-03-10T00:00:00+00:00', try_number=1, map_index=-1)
[2024-03-11T21:44:06.686+0530] {scheduler_job_runner.py:696} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='simple_dag_with_taskflow', task_id='task_c', run_id='scheduled__2024-03-10T00:00:00+00:00', try_number=1, map_index=-1)
[2024-03-11T21:44:06.693+0530] {scheduler_job_runner.py:733} INFO - TaskInstance Finished: dag_id=simple_dag_with_taskflow,task_id=task_b, run_id=scheduled__2024-03-10T00:00:00+00:00, map_index=-1, run_start_date=2024-03-11 16:14:03.712630+00:00,run_end_date=2024-03-11 16:14:03.907066+00:00, run_duration=0.194436, state=success, executor_state=success, try_number=1, max_tries=0, job_id=187, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2024-03-11 16:14:02.265736+00:00, queued_by_job_id=172, pid=257492
[2024-03-11T21:44:06.693+0530] {scheduler_job_runner.py:733} INFO - TaskInstance Finished: dag_id=simple_dag_with_taskflow,task_id=task_c, run_id=scheduled__2024-03-10T00:00:00+00:00, map_index=-1, run_start_date=2024-03-11 16:14:05.993779+00:00,run_end_date=2024-03-11 16:14:06.196129+00:00, run_duration=0.20235, state=success, executor_state=success, try_number=1, max_tries=0, job_id=188, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2024-03-11 16:14:02.265736+00:00, queued_by_job_id=172, pid=257494
[2024-03-11T21:44:06.983+0530] {scheduler_job_runner.py:424} INFO - 1 tasks up for execution:
        <TaskInstance: simple_dag_with_taskflow.task_d scheduled__2024-03-10T00:00:00+00:00 [scheduled]>
[2024-03-11T21:44:06.983+0530] {scheduler_job_runner.py:487} INFO - DAG simple_dag_with_taskflow has 0/16 running and queued tasks
[2024-03-11T21:44:06.983+0530] {scheduler_job_runner.py:603} INFO - Setting the following tasks to queued state:
        <TaskInstance: simple_dag_with_taskflow.task_d scheduled__2024-03-10T00:00:00+00:00 [scheduled]>
[2024-03-11T21:44:06.985+0530] {taskinstance.py:2283} WARNING - cannot record scheduled_duration for task task_d because previous state change time has not been saved
[2024-03-11T21:44:06.986+0530] {scheduler_job_runner.py:646} INFO - Sending TaskInstanceKey(dag_id='simple_dag_with_taskflow', task_id='task_d', run_id='scheduled__2024-03-10T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1and queue default
[2024-03-11T21:44:06.987+0530] {base_executor.py:146} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'simple_dag_with_taskflow', 'task_d', 'scheduled__2024-03-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_with_taskflow.py']
[2024-03-11T21:44:07.005+0530] {sequential_executor.py:73} INFO - Executing command: ['airflow', 'tasks', 'run', 'simple_dag_with_taskflow', 'task_d', 'scheduled__2024-03-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_with_taskflow.py']
[2024-03-11T21:44:08.117+0530] {dagbag.py:540} INFO - Filling up the DagBag from /home/kai/airflow/dags/dag_with_taskflow.py
Changing /home/kai/airflow/logs/dag_id=simple_dag_with_taskflow/run_id=scheduled__2024-03-10T00:00:00+00:00/task_id=task_d permission to 509
[2024-03-11T21:44:08.348+0530] {task_command.py:423} INFO - Running <TaskInstance: simple_dag_with_taskflow.task_d scheduled__2024-03-10T00:00:00+00:00 [queued]> on host BreakThrough.
[2024-03-11T21:44:09.050+0530] {scheduler_job_runner.py:696} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='simple_dag_with_taskflow', task_id='task_d', run_id='scheduled__2024-03-10T00:00:00+00:00', try_number=1, map_index=-1)
[2024-03-11T21:44:09.055+0530] {scheduler_job_runner.py:733} INFO - TaskInstance Finished: dag_id=simple_dag_with_taskflow,task_id=task_d, run_id=scheduled__2024-03-10T00:00:00+00:00, map_index=-1, run_start_date=2024-03-11 16:14:08.417045+00:00,run_end_date=2024-03-11 16:14:08.617732+00:00, run_duration=0.200687, state=success, executor_state=success, try_number=1, max_tries=0, job_id=189, pool=default_pool, queue=default, priority_weight=1, operator=_PythonDecoratedOperator, queued_dttm=2024-03-11 16:14:06.984421+00:00, queued_by_job_id=172, pid=257497
[2024-03-11T21:44:09.718+0530] {dagrun.py:795} INFO - Marking run <DagRun simple_dag_with_taskflow @ 2024-03-10 00:00:00+00:00: scheduled__2024-03-10T00:00:00+00:00, state:running, queued_at: 2024-03-11 16:13:59.953865+00:00. externally triggered:False> successful
[2024-03-11T21:44:09.718+0530] {dagrun.py:846} INFO - DagRun Finished: dag_id=simple_dag_with_taskflow, execution_date=2024-03-10 00:00:00+00:00, run_id=scheduled__2024-03-10T00:00:00+00:00, run_start_date=2024-03-11 16:13:59.992398+00:00, run_end_date=2024-03-11 16:14:09.718464+00:00, run_duration=9.726066, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-03-10 00:00:00+00:00, data_interval_end=2024-03-11 00:00:00+00:00, dag_hash=8536b7bd67f02f5866bb4aaaf3c64f1d
[2024-03-11T21:44:09.724+0530] {dag.py:3834} INFO - Setting next_dagrun for simple_dag_with_taskflow to 2024-03-11 00:00:00+00:00, run_after=2024-03-12 00:00:00+00:00
[2024-03-11T21:47:16.133+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-11T21:52:16.173+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-11T21:57:16.209+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-11T21:57:46.680+0530] {dag.py:3834} INFO - Setting next_dagrun for xcom_taskflow_dag to 2024-03-10 00:00:00+00:00,run_after=2024-03-11 00:00:00+00:00
[2024-03-11T21:57:46.765+0530] {scheduler_job_runner.py:424} INFO - 1 tasks up for execution:
        <TaskInstance: xcom_taskflow_dag.push_function scheduled__2024-03-09T00:00:00+00:00 [scheduled]>
[2024-03-11T21:57:46.765+0530] {scheduler_job_runner.py:487} INFO - DAG xcom_taskflow_dag has 0/16 running and queued tasks
[2024-03-11T21:57:46.766+0530] {scheduler_job_runner.py:603} INFO - Setting the following tasks to queued state:
        <TaskInstance: xcom_taskflow_dag.push_function scheduled__2024-03-09T00:00:00+00:00 [scheduled]>
[2024-03-11T21:57:46.768+0530] {taskinstance.py:2283} WARNING - cannot record scheduled_duration for task push_function because previous state change time has not been saved
[2024-03-11T21:57:46.769+0530] {scheduler_job_runner.py:646} INFO - Sending TaskInstanceKey(dag_id='xcom_taskflow_dag', task_id='push_function', run_id='scheduled__2024-03-09T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3and queue default
[2024-03-11T21:57:46.769+0530] {base_executor.py:146} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'xcom_taskflow_dag', 'push_function', 'scheduled__2024-03-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_xcom_dag.py']
[2024-03-11T21:57:46.788+0530] {sequential_executor.py:73} INFO - Executing command: ['airflow', 'tasks', 'run', 'xcom_taskflow_dag', 'push_function', 'scheduled__2024-03-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_xcom_dag.py']
[2024-03-11T21:57:47.920+0530] {dagbag.py:540} INFO - Filling up the DagBag from /home/kai/airflow/dags/taskflow_xcom_dag.py
Changing /home/kai/airflow/logs/dag_id=xcom_taskflow_dag/run_id=scheduled__2024-03-09T00:00:00+00:00/task_id=push_function permission to 509
Changing /home/kai/airflow/logs/dag_id=xcom_taskflow_dag/run_id=scheduled__2024-03-09T00:00:00+00:00 permission to 509
Changing /home/kai/airflow/logs/dag_id=xcom_taskflow_dag permission to 509
[2024-03-11T21:57:48.194+0530] {task_command.py:423} INFO - Running <TaskInstance: xcom_taskflow_dag.push_function scheduled__2024-03-09T00:00:00+00:00 [queued]> on host BreakThrough.
[2024-03-11T21:57:49.176+0530] {scheduler_job_runner.py:696} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='xcom_taskflow_dag', task_id='push_function', run_id='scheduled__2024-03-09T00:00:00+00:00', try_number=1, map_index=-1)
[2024-03-11T21:57:49.180+0530] {scheduler_job_runner.py:733} INFO - TaskInstance Finished: dag_id=xcom_taskflow_dag, task_id=push_function, run_id=scheduled__2024-03-09T00:00:00+00:00, map_index=-1, run_start_date=2024-03-11 16:27:48.269007+00:00,run_end_date=2024-03-11 16:27:48.699321+00:00, run_duration=0.430314, state=success, executor_state=success, try_number=1, max_tries=0, job_id=190, pool=default_pool, queue=default, priority_weight=3, operator=_PythonDecoratedOperator, queued_dttm=2024-03-11 16:27:46.767182+00:00, queued_by_job_id=172, pid=258396
[2024-03-11T21:57:50.143+0530] {dag.py:3834} INFO - Setting next_dagrun for xcom_taskflow_dag to 2024-03-11 00:00:00+00:00,run_after=2024-03-12 00:00:00+00:00
[2024-03-11T21:57:50.238+0530] {scheduler_job_runner.py:424} INFO - 2 tasks up for execution:
        <TaskInstance: xcom_taskflow_dag.push_function scheduled__2024-03-10T00:00:00+00:00 [scheduled]>
        <TaskInstance: xcom_taskflow_dag.compute_purchase scheduled__2024-03-09T00:00:00+00:00 [scheduled]>
[2024-03-11T21:57:50.238+0530] {scheduler_job_runner.py:487} INFO - DAG xcom_taskflow_dag has 0/16 running and queued tasks
[2024-03-11T21:57:50.238+0530] {scheduler_job_runner.py:487} INFO - DAG xcom_taskflow_dag has 1/16 running and queued tasks
[2024-03-11T21:57:50.239+0530] {scheduler_job_runner.py:603} INFO - Setting the following tasks to queued state:
        <TaskInstance: xcom_taskflow_dag.push_function scheduled__2024-03-10T00:00:00+00:00 [scheduled]>
        <TaskInstance: xcom_taskflow_dag.compute_purchase scheduled__2024-03-09T00:00:00+00:00 [scheduled]>
[2024-03-11T21:57:50.241+0530] {taskinstance.py:2283} WARNING - cannot record scheduled_duration for task push_function because previous state change time has not been saved
[2024-03-11T21:57:50.241+0530] {taskinstance.py:2283} WARNING - cannot record scheduled_duration for task compute_purchase because previous state change time has not been saved
[2024-03-11T21:57:50.242+0530] {scheduler_job_runner.py:646} INFO - Sending TaskInstanceKey(dag_id='xcom_taskflow_dag', task_id='push_function', run_id='scheduled__2024-03-10T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3and queue default
[2024-03-11T21:57:50.242+0530] {base_executor.py:146} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'xcom_taskflow_dag', 'push_function', 'scheduled__2024-03-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_xcom_dag.py']
[2024-03-11T21:57:50.242+0530] {scheduler_job_runner.py:646} INFO - Sending TaskInstanceKey(dag_id='xcom_taskflow_dag', task_id='compute_purchase', run_id='scheduled__2024-03-09T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default
[2024-03-11T21:57:50.243+0530] {base_executor.py:146} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'xcom_taskflow_dag', 'compute_purchase', 'scheduled__2024-03-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_xcom_dag.py']
[2024-03-11T21:57:50.267+0530] {sequential_executor.py:73} INFO - Executing command: ['airflow', 'tasks', 'run', 'xcom_taskflow_dag', 'push_function', 'scheduled__2024-03-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_xcom_dag.py']
[2024-03-11T21:57:51.468+0530] {dagbag.py:540} INFO - Filling up the DagBag from /home/kai/airflow/dags/taskflow_xcom_dag.py
Changing /home/kai/airflow/logs/dag_id=xcom_taskflow_dag/run_id=scheduled__2024-03-10T00:00:00+00:00/task_id=push_function permission to 509
Changing /home/kai/airflow/logs/dag_id=xcom_taskflow_dag/run_id=scheduled__2024-03-10T00:00:00+00:00 permission to 509
[2024-03-11T21:57:51.714+0530] {task_command.py:423} INFO - Running <TaskInstance: xcom_taskflow_dag.push_function scheduled__2024-03-10T00:00:00+00:00 [queued]> on host BreakThrough.
[2024-03-11T21:57:52.521+0530] {sequential_executor.py:73} INFO - Executing command: ['airflow', 'tasks', 'run', 'xcom_taskflow_dag', 'compute_purchase', 'scheduled__2024-03-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_xcom_dag.py']
[2024-03-11T21:57:53.687+0530] {dagbag.py:540} INFO - Filling up the DagBag from /home/kai/airflow/dags/taskflow_xcom_dag.py
Changing /home/kai/airflow/logs/dag_id=xcom_taskflow_dag/run_id=scheduled__2024-03-09T00:00:00+00:00/task_id=compute_purchase permission to 509
[2024-03-11T21:57:53.920+0530] {task_command.py:423} INFO - Running <TaskInstance: xcom_taskflow_dag.compute_purchase scheduled__2024-03-09T00:00:00+00:00 [queued]> on host BreakThrough.
[2024-03-11T21:57:54.740+0530] {scheduler_job_runner.py:696} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='xcom_taskflow_dag', task_id='push_function', run_id='scheduled__2024-03-10T00:00:00+00:00', try_number=1, map_index=-1)
[2024-03-11T21:57:54.740+0530] {scheduler_job_runner.py:696} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='xcom_taskflow_dag', task_id='compute_purchase', run_id='scheduled__2024-03-09T00:00:00+00:00',try_number=1, map_index=-1)
[2024-03-11T21:57:54.745+0530] {scheduler_job_runner.py:733} INFO - TaskInstance Finished: dag_id=xcom_taskflow_dag, task_id=compute_purchase, run_id=scheduled__2024-03-09T00:00:00+00:00, map_index=-1, run_start_date=2024-03-11 16:27:54.056647+00:00, run_end_date=2024-03-11 16:27:54.288839+00:00, run_duration=0.232192, state=success, executor_state=success, try_number=1, max_tries=0, job_id=192, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2024-03-11 16:27:50.239991+00:00, queued_by_job_id=172, pid=258410
[2024-03-11T21:57:54.746+0530] {scheduler_job_runner.py:733} INFO - TaskInstance Finished: dag_id=xcom_taskflow_dag, task_id=push_function, run_id=scheduled__2024-03-10T00:00:00+00:00, map_index=-1, run_start_date=2024-03-11 16:27:51.796167+00:00,run_end_date=2024-03-11 16:27:52.056254+00:00, run_duration=0.260087, state=success, executor_state=success, try_number=1, max_tries=0, job_id=191, pool=default_pool, queue=default, priority_weight=3, operator=_PythonDecoratedOperator, queued_dttm=2024-03-11 16:27:50.239991+00:00, queued_by_job_id=172, pid=258408
[2024-03-11T21:57:55.047+0530] {scheduler_job_runner.py:424} INFO - 2 tasks up for execution:
        <TaskInstance: xcom_taskflow_dag.compute_purchase scheduled__2024-03-10T00:00:00+00:00 [scheduled]>
        <TaskInstance: xcom_taskflow_dag.notify scheduled__2024-03-09T00:00:00+00:00 [scheduled]>
[2024-03-11T21:57:55.047+0530] {scheduler_job_runner.py:487} INFO - DAG xcom_taskflow_dag has 0/16 running and queued tasks
[2024-03-11T21:57:55.047+0530] {scheduler_job_runner.py:487} INFO - DAG xcom_taskflow_dag has 1/16 running and queued tasks
[2024-03-11T21:57:55.048+0530] {scheduler_job_runner.py:603} INFO - Setting the following tasks to queued state:
        <TaskInstance: xcom_taskflow_dag.compute_purchase scheduled__2024-03-10T00:00:00+00:00 [scheduled]>
        <TaskInstance: xcom_taskflow_dag.notify scheduled__2024-03-09T00:00:00+00:00 [scheduled]>
[2024-03-11T21:57:55.051+0530] {taskinstance.py:2283} WARNING - cannot record scheduled_duration for task compute_purchase because previous state change time has not been saved
[2024-03-11T21:57:55.051+0530] {taskinstance.py:2283} WARNING - cannot record scheduled_duration for task notify because previous state change time has not been saved
[2024-03-11T21:57:55.052+0530] {scheduler_job_runner.py:646} INFO - Sending TaskInstanceKey(dag_id='xcom_taskflow_dag', task_id='compute_purchase', run_id='scheduled__2024-03-10T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default
[2024-03-11T21:57:55.052+0530] {base_executor.py:146} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'xcom_taskflow_dag', 'compute_purchase', 'scheduled__2024-03-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_xcom_dag.py']
[2024-03-11T21:57:55.052+0530] {scheduler_job_runner.py:646} INFO - Sending TaskInstanceKey(dag_id='xcom_taskflow_dag', task_id='notify', run_id='scheduled__2024-03-09T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default
[2024-03-11T21:57:55.053+0530] {base_executor.py:146} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'xcom_taskflow_dag', 'notify', 'scheduled__2024-03-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_xcom_dag.py']
[2024-03-11T21:57:55.072+0530] {sequential_executor.py:73} INFO - Executing command: ['airflow', 'tasks', 'run', 'xcom_taskflow_dag', 'compute_purchase', 'scheduled__2024-03-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_xcom_dag.py']
[2024-03-11T21:57:56.196+0530] {dagbag.py:540} INFO - Filling up the DagBag from /home/kai/airflow/dags/taskflow_xcom_dag.py
Changing /home/kai/airflow/logs/dag_id=xcom_taskflow_dag/run_id=scheduled__2024-03-10T00:00:00+00:00/task_id=compute_purchase permission to 509
[2024-03-11T21:57:56.428+0530] {task_command.py:423} INFO - Running <TaskInstance: xcom_taskflow_dag.compute_purchase scheduled__2024-03-10T00:00:00+00:00 [queued]> on host BreakThrough.
[2024-03-11T21:57:57.229+0530] {sequential_executor.py:73} INFO - Executing command: ['airflow', 'tasks', 'run', 'xcom_taskflow_dag', 'notify', 'scheduled__2024-03-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_xcom_dag.py']
[2024-03-11T21:57:58.313+0530] {dagbag.py:540} INFO - Filling up the DagBag from /home/kai/airflow/dags/taskflow_xcom_dag.py
Changing /home/kai/airflow/logs/dag_id=xcom_taskflow_dag/run_id=scheduled__2024-03-09T00:00:00+00:00/task_id=notify permission to 509
[2024-03-11T21:57:58.538+0530] {task_command.py:423} INFO - Running <TaskInstance: xcom_taskflow_dag.notify scheduled__2024-03-09T00:00:00+00:00 [queued]> on host BreakThrough.
[2024-03-11T21:57:59.222+0530] {scheduler_job_runner.py:696} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='xcom_taskflow_dag', task_id='compute_purchase', run_id='scheduled__2024-03-10T00:00:00+00:00',try_number=1, map_index=-1)
[2024-03-11T21:57:59.222+0530] {scheduler_job_runner.py:696} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='xcom_taskflow_dag', task_id='notify', run_id='scheduled__2024-03-09T00:00:00+00:00', try_number=1, map_index=-1)
[2024-03-11T21:57:59.226+0530] {scheduler_job_runner.py:733} INFO - TaskInstance Finished: dag_id=xcom_taskflow_dag, task_id=notify, run_id=scheduled__2024-03-09T00:00:00+00:00, map_index=-1, run_start_date=2024-03-11 16:27:58.600268+00:00, run_end_date=2024-03-11 16:27:58.800939+00:00, run_duration=0.200671, state=success, executor_state=success, try_number=1, max_tries=0, job_id=194, pool=default_pool, queue=default, priority_weight=1, operator=_PythonDecoratedOperator, queued_dttm=2024-03-11 16:27:55.049631+00:00, queued_by_job_id=172, pid=258415
[2024-03-11T21:57:59.227+0530] {scheduler_job_runner.py:733} INFO - TaskInstance Finished: dag_id=xcom_taskflow_dag, task_id=compute_purchase, run_id=scheduled__2024-03-10T00:00:00+00:00, map_index=-1, run_start_date=2024-03-11 16:27:56.506014+00:00, run_end_date=2024-03-11 16:27:56.740095+00:00, run_duration=0.234081, state=success, executor_state=success, try_number=1, max_tries=0, job_id=193, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2024-03-11 16:27:55.049631+00:00, queued_by_job_id=172, pid=258413
[2024-03-11T21:57:59.469+0530] {dagrun.py:795} INFO - Marking run <DagRun xcom_taskflow_dag @ 2024-03-09 00:00:00+00:00: scheduled__2024-03-09T00:00:00+00:00, state:running, queued_at: 2024-03-11 16:27:46.675867+00:00. externally triggered: False>successful
[2024-03-11T21:57:59.469+0530] {dagrun.py:846} INFO - DagRun Finished: dag_id=xcom_taskflow_dag, execution_date=2024-03-09 00:00:00+00:00, run_id=scheduled__2024-03-09T00:00:00+00:00, run_start_date=2024-03-11 16:27:46.708226+00:00, run_end_date=2024-03-11 16:27:59.469853+00:00, run_duration=12.761627, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-03-09 00:00:00+00:00, data_interval_end=2024-03-10 00:00:00+00:00, dag_hash=bd61cc8c37b1a69f321a4effa9b44c9c
[2024-03-11T21:57:59.472+0530] {dag.py:3834} INFO - Setting next_dagrun for xcom_taskflow_dag to 2024-03-10 00:00:00+00:00,run_after=2024-03-11 00:00:00+00:00
[2024-03-11T21:57:59.501+0530] {scheduler_job_runner.py:424} INFO - 1 tasks up for execution:
        <TaskInstance: xcom_taskflow_dag.notify scheduled__2024-03-10T00:00:00+00:00 [scheduled]>
[2024-03-11T21:57:59.501+0530] {scheduler_job_runner.py:487} INFO - DAG xcom_taskflow_dag has 0/16 running and queued tasks
[2024-03-11T21:57:59.501+0530] {scheduler_job_runner.py:603} INFO - Setting the following tasks to queued state:
        <TaskInstance: xcom_taskflow_dag.notify scheduled__2024-03-10T00:00:00+00:00 [scheduled]>
[2024-03-11T21:57:59.503+0530] {taskinstance.py:2283} WARNING - cannot record scheduled_duration for task notify because previous state change time has not been saved
[2024-03-11T21:57:59.503+0530] {scheduler_job_runner.py:646} INFO - Sending TaskInstanceKey(dag_id='xcom_taskflow_dag', task_id='notify', run_id='scheduled__2024-03-10T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default
[2024-03-11T21:57:59.504+0530] {base_executor.py:146} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'xcom_taskflow_dag', 'notify', 'scheduled__2024-03-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_xcom_dag.py']
[2024-03-11T21:57:59.522+0530] {sequential_executor.py:73} INFO - Executing command: ['airflow', 'tasks', 'run', 'xcom_taskflow_dag', 'notify', 'scheduled__2024-03-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_xcom_dag.py']
[2024-03-11T21:58:00.807+0530] {dagbag.py:540} INFO - Filling up the DagBag from /home/kai/airflow/dags/taskflow_xcom_dag.py
Changing /home/kai/airflow/logs/dag_id=xcom_taskflow_dag/run_id=scheduled__2024-03-10T00:00:00+00:00/task_id=notify permission to 509
[2024-03-11T21:58:01.048+0530] {task_command.py:423} INFO - Running <TaskInstance: xcom_taskflow_dag.notify scheduled__2024-03-10T00:00:00+00:00 [queued]> on host BreakThrough.
[2024-03-11T21:58:01.788+0530] {scheduler_job_runner.py:696} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='xcom_taskflow_dag', task_id='notify', run_id='scheduled__2024-03-10T00:00:00+00:00', try_number=1, map_index=-1)
[2024-03-11T21:58:01.796+0530] {scheduler_job_runner.py:733} INFO - TaskInstance Finished: dag_id=xcom_taskflow_dag, task_id=notify, run_id=scheduled__2024-03-10T00:00:00+00:00, map_index=-1, run_start_date=2024-03-11 16:28:01.123471+00:00, run_end_date=2024-03-11 16:28:01.343491+00:00, run_duration=0.22002, state=success, executor_state=success, try_number=1, max_tries=0, job_id=195, pool=default_pool, queue=default, priority_weight=1, operator=_PythonDecoratedOperator, queued_dttm=2024-03-11 16:27:59.502341+00:00, queued_by_job_id=172, pid=258418
[2024-03-11T21:58:02.061+0530] {dag.py:3834} INFO - Setting next_dagrun for xcom_taskflow_dag to 2024-03-11 00:00:00+00:00,run_after=2024-03-12 00:00:00+00:00
[2024-03-11T21:58:02.090+0530] {dagrun.py:795} INFO - Marking run <DagRun xcom_taskflow_dag @ 2024-03-10 00:00:00+00:00: scheduled__2024-03-10T00:00:00+00:00, state:running, queued_at: 2024-03-11 16:27:50.140587+00:00. externally triggered: False>successful
[2024-03-11T21:58:02.090+0530] {dagrun.py:846} INFO - DagRun Finished: dag_id=xcom_taskflow_dag, execution_date=2024-03-10 00:00:00+00:00, run_id=scheduled__2024-03-10T00:00:00+00:00, run_start_date=2024-03-11 16:27:50.173148+00:00, run_end_date=2024-03-11 16:28:02.090514+00:00, run_duration=11.917366, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-03-10 00:00:00+00:00, data_interval_end=2024-03-11 00:00:00+00:00, dag_hash=bd61cc8c37b1a69f321a4effa9b44c9c
[2024-03-11T21:58:02.093+0530] {dag.py:3834} INFO - Setting next_dagrun for xcom_taskflow_dag to 2024-03-11 00:00:00+00:00,run_after=2024-03-12 00:00:00+00:00
[2024-03-11T22:02:16.255+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-11T22:07:16.289+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-11T22:09:38.722+0530] {scheduler_job_runner.py:424} INFO - 1 tasks up for execution:
        <TaskInstance: xcom_taskflow_dag.push_function manual__2024-03-11T16:39:36.054949+00:00 [scheduled]>
[2024-03-11T22:09:38.723+0530] {scheduler_job_runner.py:487} INFO - DAG xcom_taskflow_dag has 0/16 running and queued tasks
[2024-03-11T22:09:38.723+0530] {scheduler_job_runner.py:603} INFO - Setting the following tasks to queued state:
        <TaskInstance: xcom_taskflow_dag.push_function manual__2024-03-11T16:39:36.054949+00:00 [scheduled]>
[2024-03-11T22:09:38.725+0530] {taskinstance.py:2283} WARNING - cannot record scheduled_duration for task push_function because previous state change time has not been saved
[2024-03-11T22:09:38.726+0530] {scheduler_job_runner.py:646} INFO - Sending TaskInstanceKey(dag_id='xcom_taskflow_dag', task_id='push_function', run_id='manual__2024-03-11T16:39:36.054949+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default
[2024-03-11T22:09:38.727+0530] {base_executor.py:146} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'xcom_taskflow_dag', 'push_function', 'manual__2024-03-11T16:39:36.054949+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_xcom_dag.py']
[2024-03-11T22:09:38.747+0530] {sequential_executor.py:73} INFO - Executing command: ['airflow', 'tasks', 'run', 'xcom_taskflow_dag', 'push_function', 'manual__2024-03-11T16:39:36.054949+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_xcom_dag.py']
[2024-03-11T22:09:40.131+0530] {dagbag.py:540} INFO - Filling up the DagBag from /home/kai/airflow/dags/taskflow_xcom_dag.py
Changing /home/kai/airflow/logs/dag_id=xcom_taskflow_dag/run_id=manual__2024-03-11T16:39:36.054949+00:00/task_id=push_function permission to 509
Changing /home/kai/airflow/logs/dag_id=xcom_taskflow_dag/run_id=manual__2024-03-11T16:39:36.054949+00:00 permission to 509
[2024-03-11T22:09:40.409+0530] {task_command.py:423} INFO - Running <TaskInstance: xcom_taskflow_dag.push_function manual__2024-03-11T16:39:36.054949+00:00 [queued]> on host BreakThrough.
[2024-03-11T22:09:41.556+0530] {scheduler_job_runner.py:696} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='xcom_taskflow_dag', task_id='push_function', run_id='manual__2024-03-11T16:39:36.054949+00:00', try_number=1, map_index=-1)
[2024-03-11T22:09:41.561+0530] {scheduler_job_runner.py:733} INFO - TaskInstance Finished: dag_id=xcom_taskflow_dag, task_id=push_function, run_id=manual__2024-03-11T16:39:36.054949+00:00, map_index=-1, run_start_date=2024-03-11 16:39:40.478319+00:00, run_end_date=2024-03-11 16:39:41.041055+00:00, run_duration=0.562736, state=success, executor_state=success, try_number=1, max_tries=0, job_id=196, pool=default_pool, queue=default, priority_weight=3, operator=_PythonDecoratedOperator, queued_dttm=2024-03-11 16:39:38.724068+00:00, queued_by_job_id=172, pid=259242
[2024-03-11T22:09:41.853+0530] {scheduler_job_runner.py:424} INFO - 1 tasks up for execution:
        <TaskInstance: xcom_taskflow_dag.compute_purchase manual__2024-03-11T16:39:36.054949+00:00 [scheduled]>
[2024-03-11T22:09:41.853+0530] {scheduler_job_runner.py:487} INFO - DAG xcom_taskflow_dag has 0/16 running and queued tasks
[2024-03-11T22:09:41.853+0530] {scheduler_job_runner.py:603} INFO - Setting the following tasks to queued state:
        <TaskInstance: xcom_taskflow_dag.compute_purchase manual__2024-03-11T16:39:36.054949+00:00 [scheduled]>
[2024-03-11T22:09:41.855+0530] {taskinstance.py:2283} WARNING - cannot record scheduled_duration for task compute_purchase because previous state change time has not been saved
[2024-03-11T22:09:41.855+0530] {scheduler_job_runner.py:646} INFO - Sending TaskInstanceKey(dag_id='xcom_taskflow_dag', task_id='compute_purchase', run_id='manual__2024-03-11T16:39:36.054949+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default
[2024-03-11T22:09:41.856+0530] {base_executor.py:146} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'xcom_taskflow_dag', 'compute_purchase', 'manual__2024-03-11T16:39:36.054949+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_xcom_dag.py']
[2024-03-11T22:09:41.881+0530] {sequential_executor.py:73} INFO - Executing command: ['airflow', 'tasks', 'run', 'xcom_taskflow_dag', 'compute_purchase', 'manual__2024-03-11T16:39:36.054949+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_xcom_dag.py']
[2024-03-11T22:09:43.245+0530] {dagbag.py:540} INFO - Filling up the DagBag from /home/kai/airflow/dags/taskflow_xcom_dag.py
Changing /home/kai/airflow/logs/dag_id=xcom_taskflow_dag/run_id=manual__2024-03-11T16:39:36.054949+00:00/task_id=compute_purchase permission to 509
[2024-03-11T22:09:43.463+0530] {task_command.py:423} INFO - Running <TaskInstance: xcom_taskflow_dag.compute_purchase manual__2024-03-11T16:39:36.054949+00:00 [queued]> on host BreakThrough.
[2024-03-11T22:09:44.373+0530] {scheduler_job_runner.py:696} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='xcom_taskflow_dag', task_id='compute_purchase', run_id='manual__2024-03-11T16:39:36.054949+00:00', try_number=1, map_index=-1)
[2024-03-11T22:09:44.378+0530] {scheduler_job_runner.py:733} INFO - TaskInstance Finished: dag_id=xcom_taskflow_dag, task_id=compute_purchase, run_id=manual__2024-03-11T16:39:36.054949+00:00, map_index=-1, run_start_date=2024-03-11 16:39:43.548032+00:00, run_end_date=2024-03-11 16:39:43.904818+00:00, run_duration=0.356786, state=success, executor_state=success, try_number=1, max_tries=0, job_id=197, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2024-03-11 16:39:41.854370+00:00, queued_by_job_id=172, pid=259245
[2024-03-11T22:09:44.549+0530] {scheduler_job_runner.py:424} INFO - 1 tasks up for execution:
        <TaskInstance: xcom_taskflow_dag.notify manual__2024-03-11T16:39:36.054949+00:00 [scheduled]>
[2024-03-11T22:09:44.549+0530] {scheduler_job_runner.py:487} INFO - DAG xcom_taskflow_dag has 0/16 running and queued tasks
[2024-03-11T22:09:44.549+0530] {scheduler_job_runner.py:603} INFO - Setting the following tasks to queued state:
        <TaskInstance: xcom_taskflow_dag.notify manual__2024-03-11T16:39:36.054949+00:00 [scheduled]>
[2024-03-11T22:09:44.551+0530] {taskinstance.py:2283} WARNING - cannot record scheduled_duration for task notify because previous state change time has not been saved
[2024-03-11T22:09:44.551+0530] {scheduler_job_runner.py:646} INFO - Sending TaskInstanceKey(dag_id='xcom_taskflow_dag', task_id='notify', run_id='manual__2024-03-11T16:39:36.054949+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default
[2024-03-11T22:09:44.551+0530] {base_executor.py:146} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'xcom_taskflow_dag', 'notify', 'manual__2024-03-11T16:39:36.054949+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_xcom_dag.py']
[2024-03-11T22:09:44.576+0530] {sequential_executor.py:73} INFO - Executing command: ['airflow', 'tasks', 'run', 'xcom_taskflow_dag', 'notify', 'manual__2024-03-11T16:39:36.054949+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_xcom_dag.py']
[2024-03-11T22:09:45.651+0530] {dagbag.py:540} INFO - Filling up the DagBag from /home/kai/airflow/dags/taskflow_xcom_dag.py
Changing /home/kai/airflow/logs/dag_id=xcom_taskflow_dag/run_id=manual__2024-03-11T16:39:36.054949+00:00/task_id=notify permission to 509
[2024-03-11T22:09:45.883+0530] {task_command.py:423} INFO - Running <TaskInstance: xcom_taskflow_dag.notify manual__2024-03-11T16:39:36.054949+00:00 [queued]> on host BreakThrough.
[2024-03-11T22:09:46.580+0530] {scheduler_job_runner.py:696} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='xcom_taskflow_dag', task_id='notify', run_id='manual__2024-03-11T16:39:36.054949+00:00', try_number=1, map_index=-1)
[2024-03-11T22:09:46.584+0530] {scheduler_job_runner.py:733} INFO - TaskInstance Finished: dag_id=xcom_taskflow_dag, task_id=notify, run_id=manual__2024-03-11T16:39:36.054949+00:00, map_index=-1, run_start_date=2024-03-11 16:39:45.956668+00:00, run_end_date=2024-03-11 16:39:46.159036+00:00, run_duration=0.202368, state=success, executor_state=success, try_number=1, max_tries=0, job_id=198, pool=default_pool, queue=default, priority_weight=1, operator=_PythonDecoratedOperator, queued_dttm=2024-03-11 16:39:44.550387+00:00, queued_by_job_id=172, pid=259248
[2024-03-11T22:09:46.849+0530] {dagrun.py:795} INFO - Marking run <DagRun xcom_taskflow_dag @ 2024-03-11 16:39:36.054949+00:00: manual__2024-03-11T16:39:36.054949+00:00, state:running, queued_at: 2024-03-11 16:39:36.077699+00:00. externally triggered: True> successful
[2024-03-11T22:09:46.850+0530] {dagrun.py:846} INFO - DagRun Finished: dag_id=xcom_taskflow_dag, execution_date=2024-03-11 16:39:36.054949+00:00, run_id=manual__2024-03-11T16:39:36.054949+00:00, run_start_date=2024-03-11 16:39:38.664923+00:00, run_end_date=2024-03-11 16:39:46.850110+00:00, run_duration=8.185187, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-03-10 00:00:00+00:00, data_interval_end=2024-03-11 00:00:00+00:00, dag_hash=0908720abf396cc59db92ef1b8addb0f
[2024-03-11T22:12:16.326+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-12 07:51:00 +0530] [253581] [CRITICAL] WORKER TIMEOUT (pid:253582)
[2024-03-12 07:51:01 +0530] [253581] [CRITICAL] WORKER TIMEOUT (pid:253584)
[2024-03-12 07:51:01 +0530] [253584] [INFO] Worker exiting (pid: 253584)
[2024-03-12 07:51:01 +0530] [253581] [ERROR] Worker (pid:253584) exited with code 1
[2024-03-12 07:51:01 +0530] [253581] [ERROR] Worker (pid:253584) exited with code 1.
[2024-03-12 07:51:01 +0530] [253581] [ERROR] Worker (pid:253582) was sent SIGKILL! Perhaps out of memory?
[2024-03-12 07:51:01 +0530] [259509] [INFO] Booting worker with pid: 259509
[2024-03-12 07:51:01 +0530] [259511] [INFO] Booting worker with pid: 259511
[2024-03-12T07:51:02.848+0530] {dag.py:3834} INFO - Setting next_dagrun for xcom_taskflow_dag to 2024-03-12 00:00:00+00:00,run_after=2024-03-13 00:00:00+00:00
[2024-03-12T07:51:03.410+0530] {scheduler_job_runner.py:424} INFO - 1 tasks up for execution:
        <TaskInstance: xcom_taskflow_dag.push_function scheduled__2024-03-11T00:00:00+00:00 [scheduled]>
[2024-03-12T07:51:03.410+0530] {scheduler_job_runner.py:487} INFO - DAG xcom_taskflow_dag has 0/16 running and queued tasks
[2024-03-12T07:51:03.411+0530] {scheduler_job_runner.py:603} INFO - Setting the following tasks to queued state:
        <TaskInstance: xcom_taskflow_dag.push_function scheduled__2024-03-11T00:00:00+00:00 [scheduled]>
[2024-03-12T07:51:03.422+0530] {taskinstance.py:2283} WARNING - cannot record scheduled_duration for task push_function because previous state change time has not been saved
[2024-03-12T07:51:03.423+0530] {scheduler_job_runner.py:646} INFO - Sending TaskInstanceKey(dag_id='xcom_taskflow_dag', task_id='push_function', run_id='scheduled__2024-03-11T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3and queue default
[2024-03-12T07:51:03.424+0530] {base_executor.py:146} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'xcom_taskflow_dag', 'push_function', 'scheduled__2024-03-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_xcom_dag.py']
[2024-03-12T07:51:03.483+0530] {sequential_executor.py:73} INFO - Executing command: ['airflow', 'tasks', 'run', 'xcom_taskflow_dag', 'push_function', 'scheduled__2024-03-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_xcom_dag.py']
[2024-03-12T07:51:07.974+0530] {dagbag.py:540} INFO - Filling up the DagBag from /home/kai/airflow/dags/taskflow_xcom_dag.py
Changing /home/kai/airflow/logs/dag_id=xcom_taskflow_dag/run_id=scheduled__2024-03-11T00:00:00+00:00/task_id=push_function permission to 509
Changing /home/kai/airflow/logs/dag_id=xcom_taskflow_dag/run_id=scheduled__2024-03-11T00:00:00+00:00 permission to 509
[2024-03-12T07:51:08.246+0530] {task_command.py:423} INFO - Running <TaskInstance: xcom_taskflow_dag.push_function scheduled__2024-03-11T00:00:00+00:00 [queued]> on host BreakThrough.
[2024-03-12T07:51:09.086+0530] {scheduler_job_runner.py:696} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='xcom_taskflow_dag', task_id='push_function', run_id='scheduled__2024-03-11T00:00:00+00:00', try_number=1, map_index=-1)
[2024-03-12T07:51:09.097+0530] {scheduler_job_runner.py:733} INFO - TaskInstance Finished: dag_id=xcom_taskflow_dag, task_id=push_function, run_id=scheduled__2024-03-11T00:00:00+00:00, map_index=-1, run_start_date=2024-03-12 02:21:08.337519+00:00,run_end_date=2024-03-12 02:21:08.571753+00:00, run_duration=0.234234, state=success, executor_state=success, try_number=1, max_tries=0, job_id=199, pool=default_pool, queue=default, priority_weight=3, operator=_PythonDecoratedOperator, queued_dttm=2024-03-12 02:21:03.412082+00:00, queued_by_job_id=172, pid=259526
[2024-03-12T07:51:09.535+0530] {scheduler_job_runner.py:424} INFO - 1 tasks up for execution:
        <TaskInstance: xcom_taskflow_dag.compute_purchase scheduled__2024-03-11T00:00:00+00:00 [scheduled]>
[2024-03-12T07:51:09.536+0530] {scheduler_job_runner.py:487} INFO - DAG xcom_taskflow_dag has 0/16 running and queued tasks
[2024-03-12T07:51:09.536+0530] {scheduler_job_runner.py:603} INFO - Setting the following tasks to queued state:
        <TaskInstance: xcom_taskflow_dag.compute_purchase scheduled__2024-03-11T00:00:00+00:00 [scheduled]>
[2024-03-12T07:51:09.537+0530] {taskinstance.py:2283} WARNING - cannot record scheduled_duration for task compute_purchase because previous state change time has not been saved
[2024-03-12T07:51:09.538+0530] {scheduler_job_runner.py:646} INFO - Sending TaskInstanceKey(dag_id='xcom_taskflow_dag', task_id='compute_purchase', run_id='scheduled__2024-03-11T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default
[2024-03-12T07:51:09.538+0530] {base_executor.py:146} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'xcom_taskflow_dag', 'compute_purchase', 'scheduled__2024-03-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_xcom_dag.py']
[2024-03-12T07:51:09.558+0530] {sequential_executor.py:73} INFO - Executing command: ['airflow', 'tasks', 'run', 'xcom_taskflow_dag', 'compute_purchase', 'scheduled__2024-03-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_xcom_dag.py']
[2024-03-12T07:51:10.919+0530] {dagbag.py:540} INFO - Filling up the DagBag from /home/kai/airflow/dags/taskflow_xcom_dag.py
Changing /home/kai/airflow/logs/dag_id=xcom_taskflow_dag/run_id=scheduled__2024-03-11T00:00:00+00:00/task_id=compute_purchase permission to 509
[2024-03-12T07:51:11.164+0530] {task_command.py:423} INFO - Running <TaskInstance: xcom_taskflow_dag.compute_purchase scheduled__2024-03-11T00:00:00+00:00 [queued]> on host BreakThrough.
[2024-03-12T07:51:12.057+0530] {scheduler_job_runner.py:696} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='xcom_taskflow_dag', task_id='compute_purchase', run_id='scheduled__2024-03-11T00:00:00+00:00',try_number=1, map_index=-1)
[2024-03-12T07:51:12.061+0530] {scheduler_job_runner.py:733} INFO - TaskInstance Finished: dag_id=xcom_taskflow_dag, task_id=compute_purchase, run_id=scheduled__2024-03-11T00:00:00+00:00, map_index=-1, run_start_date=2024-03-12 02:21:11.249014+00:00, run_end_date=2024-03-12 02:21:11.615950+00:00, run_duration=0.366936, state=success, executor_state=success, try_number=1, max_tries=0, job_id=200, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2024-03-12 02:21:09.536865+00:00, queued_by_job_id=172, pid=259529
[2024-03-12T07:51:12.355+0530] {scheduler_job_runner.py:424} INFO - 1 tasks up for execution:
        <TaskInstance: xcom_taskflow_dag.notify scheduled__2024-03-11T00:00:00+00:00 [scheduled]>
[2024-03-12T07:51:12.355+0530] {scheduler_job_runner.py:487} INFO - DAG xcom_taskflow_dag has 0/16 running and queued tasks
[2024-03-12T07:51:12.356+0530] {scheduler_job_runner.py:603} INFO - Setting the following tasks to queued state:
        <TaskInstance: xcom_taskflow_dag.notify scheduled__2024-03-11T00:00:00+00:00 [scheduled]>
[2024-03-12T07:51:12.358+0530] {taskinstance.py:2283} WARNING - cannot record scheduled_duration for task notify because previous state change time has not been saved
[2024-03-12T07:51:12.358+0530] {scheduler_job_runner.py:646} INFO - Sending TaskInstanceKey(dag_id='xcom_taskflow_dag', task_id='notify', run_id='scheduled__2024-03-11T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default
[2024-03-12T07:51:12.359+0530] {base_executor.py:146} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'xcom_taskflow_dag', 'notify', 'scheduled__2024-03-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_xcom_dag.py']
[2024-03-12T07:51:12.400+0530] {sequential_executor.py:73} INFO - Executing command: ['airflow', 'tasks', 'run', 'xcom_taskflow_dag', 'notify', 'scheduled__2024-03-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_xcom_dag.py']
[2024-03-12T07:51:13.465+0530] {dagbag.py:540} INFO - Filling up the DagBag from /home/kai/airflow/dags/taskflow_xcom_dag.py
Changing /home/kai/airflow/logs/dag_id=xcom_taskflow_dag/run_id=scheduled__2024-03-11T00:00:00+00:00/task_id=notify permission to 509
[2024-03-12T07:51:13.689+0530] {task_command.py:423} INFO - Running <TaskInstance: xcom_taskflow_dag.notify scheduled__2024-03-11T00:00:00+00:00 [queued]> on host BreakThrough.
[2024-03-12T07:51:14.638+0530] {scheduler_job_runner.py:696} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='xcom_taskflow_dag', task_id='notify', run_id='scheduled__2024-03-11T00:00:00+00:00', try_number=1, map_index=-1)
[2024-03-12T07:51:14.646+0530] {scheduler_job_runner.py:733} INFO - TaskInstance Finished: dag_id=xcom_taskflow_dag, task_id=notify, run_id=scheduled__2024-03-11T00:00:00+00:00, map_index=-1, run_start_date=2024-03-12 02:21:13.777053+00:00, run_end_date=2024-03-12 02:21:14.099048+00:00, run_duration=0.321995, state=success, executor_state=success, try_number=1, max_tries=0, job_id=201, pool=default_pool, queue=default, priority_weight=1, operator=_PythonDecoratedOperator, queued_dttm=2024-03-12 02:21:12.356838+00:00, queued_by_job_id=172, pid=259532
[2024-03-12T07:51:15.012+0530] {dagrun.py:795} INFO - Marking run <DagRun xcom_taskflow_dag @ 2024-03-11 00:00:00+00:00: scheduled__2024-03-11T00:00:00+00:00, state:running, queued_at: 2024-03-12 02:21:02.840640+00:00. externally triggered: False>successful
[2024-03-12T07:51:15.013+0530] {dagrun.py:846} INFO - DagRun Finished: dag_id=xcom_taskflow_dag, execution_date=2024-03-11 00:00:00+00:00, run_id=scheduled__2024-03-11T00:00:00+00:00, run_start_date=2024-03-12 02:21:03.172962+00:00, run_end_date=2024-03-12 02:21:15.013337+00:00, run_duration=11.840375, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-03-11 00:00:00+00:00, data_interval_end=2024-03-12 00:00:00+00:00, dag_hash=f716084fe4b811bf9b95b1915a9c5e64
[2024-03-12T07:51:15.017+0530] {dag.py:3834} INFO - Setting next_dagrun for xcom_taskflow_dag to 2024-03-12 00:00:00+00:00,run_after=2024-03-13 00:00:00+00:00
[2024-03-12T07:54:34.801+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-12T07:59:34.965+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-12T08:04:35.003+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-12T08:09:35.386+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-12T08:14:35.641+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-12T08:19:35.677+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-12T08:24:35.921+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-12T08:29:35.987+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-12T08:34:36.288+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-12 18:21:00 +0530] [253581] [CRITICAL] WORKER TIMEOUT (pid:259509)
[2024-03-12 18:21:00 +0530] [253581] [CRITICAL] WORKER TIMEOUT (pid:259511)
[2024-03-12 18:21:00 +0530] [259509] [INFO] Worker exiting (pid: 259509)
[2024-03-12 18:21:00 +0530] [259511] [INFO] Worker exiting (pid: 259511)
[2024-03-12 18:21:01 +0530] [253581] [ERROR] Worker (pid:259509) exited with code 1
[2024-03-12 18:21:01 +0530] [253581] [ERROR] Worker (pid:259509) exited with code 1.
[2024-03-12 18:21:01 +0530] [253581] [ERROR] Worker (pid:259511) was sent SIGKILL! Perhaps out of memory?
[2024-03-12 18:21:02 +0530] [262549] [INFO] Booting worker with pid: 262549
[2024-03-12 18:21:02 +0530] [262552] [INFO] Booting worker with pid: 262552
[2024-03-12T18:21:05.324+0530] {manager.py:523} INFO - DAG pipeline_variable_dag is missing and will be deactivated.
[2024-03-12T18:21:05.442+0530] {manager.py:535} INFO - Deactivated 1 DAGs which are no longer present in file.
[2024-03-12T18:21:05.783+0530] {manager.py:539} INFO - Deleted DAG pipeline_variable_dag in serialized_dag table
[2024-03-12T18:23:11.674+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-12T18:28:11.754+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-12T18:33:11.996+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-12T18:38:12.029+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-12T18:43:12.170+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-12T18:48:12.225+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-12T18:53:12.369+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-12T18:58:12.441+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-12T19:03:12.506+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-12T19:08:12.562+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-12T19:13:12.601+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-12T19:18:12.662+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-12T19:23:13.220+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-12T19:28:13.277+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-12T19:33:13.313+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-12T19:38:13.348+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-12T19:43:13.585+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-12T19:48:13.616+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-12T19:53:13.663+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-12T19:58:13.699+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-12T20:03:13.940+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-12T20:08:14.084+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-12T20:13:14.144+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-12T20:18:14.175+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-12T20:23:14.206+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-12T20:28:14.244+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-12T20:33:14.487+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-12T20:38:14.544+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-12T20:43:14.595+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-12T20:48:14.861+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-12T20:53:14.893+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-12T20:58:14.935+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-12T21:03:14.966+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-12T21:08:15.004+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-12T21:13:15.038+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-12T21:18:15.254+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-12T21:23:15.305+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-12T21:28:15.943+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-12T21:33:15.979+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-12T21:38:16.157+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-12T21:43:16.211+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-12T21:48:16.232+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-12T21:53:16.264+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-12T21:58:16.401+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-12T22:03:16.666+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-12T22:08:16.726+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-12T22:13:16.762+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-12T22:18:16.921+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-12T22:23:17.037+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-12T22:28:17.075+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-12T22:33:17.116+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-12T22:35:28.212+0530] {dag.py:3834} INFO - Setting next_dagrun for taskflow_branching_operators_dag to 2024-03-12 00:00:00+00:00, run_after=2024-03-13 00:00:00+00:00
[2024-03-12T22:35:28.321+0530] {scheduler_job_runner.py:424} INFO - 1 tasks up for execution:
        <TaskInstance: taskflow_branching_operators_dag.read_data scheduled__2024-03-11T00:00:00+00:00 [scheduled]>
[2024-03-12T22:35:28.321+0530] {scheduler_job_runner.py:487} INFO - DAG taskflow_branching_operators_dag has 0/16 running and queued tasks
[2024-03-12T22:35:28.321+0530] {scheduler_job_runner.py:603} INFO - Setting the following tasks to queued state:
        <TaskInstance: taskflow_branching_operators_dag.read_data scheduled__2024-03-11T00:00:00+00:00 [scheduled]>
[2024-03-12T22:35:28.323+0530] {taskinstance.py:2283} WARNING - cannot record scheduled_duration for task read_data becauseprevious state change time has not been saved
[2024-03-12T22:35:28.324+0530] {scheduler_job_runner.py:646} INFO - Sending TaskInstanceKey(dag_id='taskflow_branching_operators_dag', task_id='read_data', run_id='scheduled__2024-03-11T00:00:00+00:00', try_number=1, map_index=-1) to executor withpriority 6 and queue default
[2024-03-12T22:35:28.325+0530] {base_executor.py:146} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'taskflow_branching_operators_dag', 'read_data', 'scheduled__2024-03-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_branching_operators_dag.py']
[2024-03-12T22:35:28.345+0530] {sequential_executor.py:73} INFO - Executing command: ['airflow', 'tasks', 'run', 'taskflow_branching_operators_dag', 'read_data', 'scheduled__2024-03-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_branching_operators_dag.py']
[2024-03-12T22:35:29.970+0530] {dagbag.py:540} INFO - Filling up the DagBag from /home/kai/airflow/dags/taskflow_branching_operators_dag.py
Changing /home/kai/airflow/logs/dag_id=taskflow_branching_operators_dag/run_id=scheduled__2024-03-11T00:00:00+00:00/task_id=read_data permission to 509
Changing /home/kai/airflow/logs/dag_id=taskflow_branching_operators_dag/run_id=scheduled__2024-03-11T00:00:00+00:00 permission to 509
Changing /home/kai/airflow/logs/dag_id=taskflow_branching_operators_dag permission to 509
[2024-03-12T22:35:31.186+0530] {task_command.py:423} INFO - Running <TaskInstance: taskflow_branching_operators_dag.read_data scheduled__2024-03-11T00:00:00+00:00 [queued]> on host BreakThrough.
[2024-03-12T22:35:32.712+0530] {scheduler_job_runner.py:696} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='taskflow_branching_operators_dag', task_id='read_data', run_id='scheduled__2024-03-11T00:00:00+00:00', try_number=1, map_index=-1)
[2024-03-12T22:35:32.716+0530] {scheduler_job_runner.py:733} INFO - TaskInstance Finished: dag_id=taskflow_branching_operators_dag, task_id=read_data, run_id=scheduled__2024-03-11T00:00:00+00:00, map_index=-1, run_start_date=2024-03-12 17:05:31.279889+00:00, run_end_date=2024-03-12 17:05:32.138122+00:00, run_duration=0.858233, state=failed, executor_state=success, try_number=1, max_tries=0, job_id=202, pool=default_pool, queue=default, priority_weight=6, operator=_PythonDecoratedOperator, queued_dttm=2024-03-12 17:05:28.322295+00:00, queued_by_job_id=172, pid=279439
[2024-03-12T22:35:36.981+0530] {dagrun.py:774} ERROR - Marking run <DagRun taskflow_branching_operators_dag @ 2024-03-11 00:00:00+00:00: scheduled__2024-03-11T00:00:00+00:00, state:running, queued_at: 2024-03-12 17:05:28.201238+00:00. externally triggered: False> failed
[2024-03-12T22:35:36.982+0530] {dagrun.py:846} INFO - DagRun Finished: dag_id=taskflow_branching_operators_dag, execution_date=2024-03-11 00:00:00+00:00, run_id=scheduled__2024-03-11T00:00:00+00:00, run_start_date=2024-03-12 17:05:28.251281+00:00,run_end_date=2024-03-12 17:05:36.982348+00:00, run_duration=8.731067, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-03-11 00:00:00+00:00, data_interval_end=2024-03-12 00:00:00+00:00, dag_hash=a000a4fb1a529224d1c4bc49cab2a792
[2024-03-12T22:35:36.985+0530] {dag.py:3834} INFO - Setting next_dagrun for taskflow_branching_operators_dag to 2024-03-12 00:00:00+00:00, run_after=2024-03-13 00:00:00+00:00
[2024-03-12T22:36:36.535+0530] {scheduler_job_runner.py:424} INFO - 1 tasks up for execution:
        <TaskInstance: taskflow_branching_operators_dag.read_data manual__2024-03-12T17:06:35.145884+00:00 [scheduled]>
[2024-03-12T22:36:36.536+0530] {scheduler_job_runner.py:487} INFO - DAG taskflow_branching_operators_dag has 0/16 running and queued tasks
[2024-03-12T22:36:36.537+0530] {scheduler_job_runner.py:603} INFO - Setting the following tasks to queued state:
        <TaskInstance: taskflow_branching_operators_dag.read_data manual__2024-03-12T17:06:35.145884+00:00 [scheduled]>
[2024-03-12T22:36:36.558+0530] {taskinstance.py:2283} WARNING - cannot record scheduled_duration for task read_data becauseprevious state change time has not been saved
[2024-03-12T22:36:36.566+0530] {scheduler_job_runner.py:646} INFO - Sending TaskInstanceKey(dag_id='taskflow_branching_operators_dag', task_id='read_data', run_id='manual__2024-03-12T17:06:35.145884+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default
[2024-03-12T22:36:36.567+0530] {base_executor.py:146} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'taskflow_branching_operators_dag', 'read_data', 'manual__2024-03-12T17:06:35.145884+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_branching_operators_dag.py']
[2024-03-12T22:36:36.595+0530] {sequential_executor.py:73} INFO - Executing command: ['airflow', 'tasks', 'run', 'taskflow_branching_operators_dag', 'read_data', 'manual__2024-03-12T17:06:35.145884+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_branching_operators_dag.py']
[2024-03-12T22:36:38.065+0530] {dagbag.py:540} INFO - Filling up the DagBag from /home/kai/airflow/dags/taskflow_branching_operators_dag.py
Changing /home/kai/airflow/logs/dag_id=taskflow_branching_operators_dag/run_id=manual__2024-03-12T17:06:35.145884+00:00/task_id=read_data permission to 509
Changing /home/kai/airflow/logs/dag_id=taskflow_branching_operators_dag/run_id=manual__2024-03-12T17:06:35.145884+00:00 permission to 509
[2024-03-12T22:36:38.796+0530] {task_command.py:423} INFO - Running <TaskInstance: taskflow_branching_operators_dag.read_data manual__2024-03-12T17:06:35.145884+00:00 [queued]> on host BreakThrough.
[2024-03-12T22:36:39.687+0530] {scheduler_job_runner.py:696} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='taskflow_branching_operators_dag', task_id='read_data', run_id='manual__2024-03-12T17:06:35.145884+00:00', try_number=1, map_index=-1)
[2024-03-12T22:36:39.691+0530] {scheduler_job_runner.py:733} INFO - TaskInstance Finished: dag_id=taskflow_branching_operators_dag, task_id=read_data, run_id=manual__2024-03-12T17:06:35.145884+00:00, map_index=-1, run_start_date=2024-03-12 17:06:38.885019+00:00, run_end_date=2024-03-12 17:06:39.161633+00:00, run_duration=0.276614, state=failed, executor_state=success, try_number=1, max_tries=0, job_id=203, pool=default_pool, queue=default, priority_weight=6, operator=_PythonDecoratedOperator, queued_dttm=2024-03-12 17:06:36.538748+00:00, queued_by_job_id=172, pid=279570
[2024-03-12T22:36:44.441+0530] {dagrun.py:774} ERROR - Marking run <DagRun taskflow_branching_operators_dag @ 2024-03-12 17:06:35.145884+00:00: manual__2024-03-12T17:06:35.145884+00:00, state:running, queued_at: 2024-03-12 17:06:35.163663+00:00. externally triggered: True> failed
[2024-03-12T22:36:44.441+0530] {dagrun.py:846} INFO - DagRun Finished: dag_id=taskflow_branching_operators_dag, execution_date=2024-03-12 17:06:35.145884+00:00, run_id=manual__2024-03-12T17:06:35.145884+00:00, run_start_date=2024-03-12 17:06:36.356120+00:00, run_end_date=2024-03-12 17:06:44.441730+00:00, run_duration=8.08561, state=failed, external_trigger=True, run_type=manual, data_interval_start=2024-03-11 00:00:00+00:00, data_interval_end=2024-03-12 00:00:00+00:00, dag_hash=a000a4fb1a529224d1c4bc49cab2a792
[2024-03-12T22:36:57.533+0530] {scheduler_job_runner.py:424} INFO - 1 tasks up for execution:
        <TaskInstance: taskflow_branching_operators_dag.read_data manual__2024-03-12T17:06:56.835840+00:00 [scheduled]>
[2024-03-12T22:36:57.533+0530] {scheduler_job_runner.py:487} INFO - DAG taskflow_branching_operators_dag has 0/16 running and queued tasks
[2024-03-12T22:36:57.534+0530] {scheduler_job_runner.py:603} INFO - Setting the following tasks to queued state:
        <TaskInstance: taskflow_branching_operators_dag.read_data manual__2024-03-12T17:06:56.835840+00:00 [scheduled]>
[2024-03-12T22:36:57.539+0530] {taskinstance.py:2283} WARNING - cannot record scheduled_duration for task read_data becauseprevious state change time has not been saved
[2024-03-12T22:36:57.541+0530] {scheduler_job_runner.py:646} INFO - Sending TaskInstanceKey(dag_id='taskflow_branching_operators_dag', task_id='read_data', run_id='manual__2024-03-12T17:06:56.835840+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default
[2024-03-12T22:36:57.543+0530] {base_executor.py:146} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'taskflow_branching_operators_dag', 'read_data', 'manual__2024-03-12T17:06:56.835840+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_branching_operators_dag.py']
[2024-03-12T22:36:57.563+0530] {sequential_executor.py:73} INFO - Executing command: ['airflow', 'tasks', 'run', 'taskflow_branching_operators_dag', 'read_data', 'manual__2024-03-12T17:06:56.835840+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_branching_operators_dag.py']
[2024-03-12T22:36:59.193+0530] {dagbag.py:540} INFO - Filling up the DagBag from /home/kai/airflow/dags/taskflow_branching_operators_dag.py
Changing /home/kai/airflow/logs/dag_id=taskflow_branching_operators_dag/run_id=manual__2024-03-12T17:06:56.835840+00:00/task_id=read_data permission to 509
Changing /home/kai/airflow/logs/dag_id=taskflow_branching_operators_dag/run_id=manual__2024-03-12T17:06:56.835840+00:00 permission to 509
[2024-03-12T22:36:59.811+0530] {task_command.py:423} INFO - Running <TaskInstance: taskflow_branching_operators_dag.read_data manual__2024-03-12T17:06:56.835840+00:00 [queued]> on host BreakThrough.
[2024-03-12T22:37:00.981+0530] {scheduler_job_runner.py:696} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='taskflow_branching_operators_dag', task_id='read_data', run_id='manual__2024-03-12T17:06:56.835840+00:00', try_number=1, map_index=-1)
[2024-03-12T22:37:00.986+0530] {scheduler_job_runner.py:733} INFO - TaskInstance Finished: dag_id=taskflow_branching_operators_dag, task_id=read_data, run_id=manual__2024-03-12T17:06:56.835840+00:00, map_index=-1, run_start_date=2024-03-12 17:06:59.877857+00:00, run_end_date=2024-03-12 17:07:00.350746+00:00, run_duration=0.472889, state=failed, executor_state=success, try_number=1, max_tries=0, job_id=204, pool=default_pool, queue=default, priority_weight=6, operator=_PythonDecoratedOperator, queued_dttm=2024-03-12 17:06:57.537374+00:00, queued_by_job_id=172, pid=279630
[2024-03-12T22:37:04.327+0530] {dagrun.py:774} ERROR - Marking run <DagRun taskflow_branching_operators_dag @ 2024-03-12 17:06:56.835840+00:00: manual__2024-03-12T17:06:56.835840+00:00, state:running, queued_at: 2024-03-12 17:06:56.856102+00:00. externally triggered: True> failed
[2024-03-12T22:37:04.327+0530] {dagrun.py:846} INFO - DagRun Finished: dag_id=taskflow_branching_operators_dag, execution_date=2024-03-12 17:06:56.835840+00:00, run_id=manual__2024-03-12T17:06:56.835840+00:00, run_start_date=2024-03-12 17:06:57.300754+00:00, run_end_date=2024-03-12 17:07:04.327841+00:00, run_duration=7.027087, state=failed, external_trigger=True, run_type=manual, data_interval_start=2024-03-11 00:00:00+00:00, data_interval_end=2024-03-12 00:00:00+00:00, dag_hash=a000a4fb1a529224d1c4bc49cab2a792
[2024-03-12T22:37:58.759+0530] {scheduler_job_runner.py:424} INFO - 1 tasks up for execution:
        <TaskInstance: taskflow_branching_operators_dag.read_data manual__2024-03-12T17:07:57.170946+00:00 [scheduled]>
[2024-03-12T22:37:58.759+0530] {scheduler_job_runner.py:487} INFO - DAG taskflow_branching_operators_dag has 0/16 running and queued tasks
[2024-03-12T22:37:58.760+0530] {scheduler_job_runner.py:603} INFO - Setting the following tasks to queued state:
        <TaskInstance: taskflow_branching_operators_dag.read_data manual__2024-03-12T17:07:57.170946+00:00 [scheduled]>
[2024-03-12T22:37:58.761+0530] {taskinstance.py:2283} WARNING - cannot record scheduled_duration for task read_data becauseprevious state change time has not been saved
[2024-03-12T22:37:58.762+0530] {scheduler_job_runner.py:646} INFO - Sending TaskInstanceKey(dag_id='taskflow_branching_operators_dag', task_id='read_data', run_id='manual__2024-03-12T17:07:57.170946+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default
[2024-03-12T22:37:58.762+0530] {base_executor.py:146} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'taskflow_branching_operators_dag', 'read_data', 'manual__2024-03-12T17:07:57.170946+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_branching_operators_dag.py']
[2024-03-12T22:37:58.785+0530] {sequential_executor.py:73} INFO - Executing command: ['airflow', 'tasks', 'run', 'taskflow_branching_operators_dag', 'read_data', 'manual__2024-03-12T17:07:57.170946+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_branching_operators_dag.py']
[2024-03-12T22:38:00.081+0530] {dagbag.py:540} INFO - Filling up the DagBag from /home/kai/airflow/dags/taskflow_branching_operators_dag.py
Changing /home/kai/airflow/logs/dag_id=taskflow_branching_operators_dag/run_id=manual__2024-03-12T17:07:57.170946+00:00/task_id=read_data permission to 509
Changing /home/kai/airflow/logs/dag_id=taskflow_branching_operators_dag/run_id=manual__2024-03-12T17:07:57.170946+00:00 permission to 509
[2024-03-12T22:38:00.689+0530] {task_command.py:423} INFO - Running <TaskInstance: taskflow_branching_operators_dag.read_data manual__2024-03-12T17:07:57.170946+00:00 [queued]> on host BreakThrough.
[2024-03-12T22:38:01.849+0530] {scheduler_job_runner.py:696} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='taskflow_branching_operators_dag', task_id='read_data', run_id='manual__2024-03-12T17:07:57.170946+00:00', try_number=1, map_index=-1)
[2024-03-12T22:38:01.853+0530] {scheduler_job_runner.py:733} INFO - TaskInstance Finished: dag_id=taskflow_branching_operators_dag, task_id=read_data, run_id=manual__2024-03-12T17:07:57.170946+00:00, map_index=-1, run_start_date=2024-03-12 17:08:00.747620+00:00, run_end_date=2024-03-12 17:08:01.320038+00:00, run_duration=0.572418, state=success, executor_state=success,try_number=1, max_tries=0, job_id=205, pool=default_pool, queue=default, priority_weight=6, operator=_PythonDecoratedOperator, queued_dttm=2024-03-12 17:07:58.760904+00:00, queued_by_job_id=172, pid=279733
[2024-03-12T22:38:01.947+0530] {scheduler_job_runner.py:424} INFO - 1 tasks up for execution:
        <TaskInstance: taskflow_branching_operators_dag.clean_data manual__2024-03-12T17:07:57.170946+00:00 [scheduled]>
[2024-03-12T22:38:01.948+0530] {scheduler_job_runner.py:487} INFO - DAG taskflow_branching_operators_dag has 0/16 running and queued tasks
[2024-03-12T22:38:01.948+0530] {scheduler_job_runner.py:603} INFO - Setting the following tasks to queued state:
        <TaskInstance: taskflow_branching_operators_dag.clean_data manual__2024-03-12T17:07:57.170946+00:00 [scheduled]>
[2024-03-12T22:38:01.949+0530] {taskinstance.py:2283} WARNING - cannot record scheduled_duration for task clean_data because previous state change time has not been saved
[2024-03-12T22:38:01.950+0530] {scheduler_job_runner.py:646} INFO - Sending TaskInstanceKey(dag_id='taskflow_branching_operators_dag', task_id='clean_data', run_id='manual__2024-03-12T17:07:57.170946+00:00', try_number=1, map_index=-1) to executorwith priority 5 and queue default
[2024-03-12T22:38:01.950+0530] {base_executor.py:146} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'taskflow_branching_operators_dag', 'clean_data', 'manual__2024-03-12T17:07:57.170946+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_branching_operators_dag.py']
[2024-03-12T22:38:01.977+0530] {sequential_executor.py:73} INFO - Executing command: ['airflow', 'tasks', 'run', 'taskflow_branching_operators_dag', 'clean_data', 'manual__2024-03-12T17:07:57.170946+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_branching_operators_dag.py']
[2024-03-12T22:38:03.115+0530] {dagbag.py:540} INFO - Filling up the DagBag from /home/kai/airflow/dags/taskflow_branching_operators_dag.py
Changing /home/kai/airflow/logs/dag_id=taskflow_branching_operators_dag/run_id=manual__2024-03-12T17:07:57.170946+00:00/task_id=clean_data permission to 509
[2024-03-12T22:38:03.685+0530] {task_command.py:423} INFO - Running <TaskInstance: taskflow_branching_operators_dag.clean_data manual__2024-03-12T17:07:57.170946+00:00 [queued]> on host BreakThrough.
[2024-03-12T22:38:04.601+0530] {scheduler_job_runner.py:696} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='taskflow_branching_operators_dag', task_id='clean_data', run_id='manual__2024-03-12T17:07:57.170946+00:00', try_number=1, map_index=-1)
[2024-03-12T22:38:04.606+0530] {scheduler_job_runner.py:733} INFO - TaskInstance Finished: dag_id=taskflow_branching_operators_dag, task_id=clean_data, run_id=manual__2024-03-12T17:07:57.170946+00:00, map_index=-1, run_start_date=2024-03-12 17:08:03.742938+00:00, run_end_date=2024-03-12 17:08:04.052429+00:00, run_duration=0.309491, state=success, executor_state=success, try_number=1, max_tries=0, job_id=206, pool=default_pool, queue=default, priority_weight=5, operator=_PythonDecoratedOperator, queued_dttm=2024-03-12 17:08:01.948967+00:00, queued_by_job_id=172, pid=279742
[2024-03-12T22:38:04.663+0530] {scheduler_job_runner.py:424} INFO - 1 tasks up for execution:
        <TaskInstance: taskflow_branching_operators_dag.branch manual__2024-03-12T17:07:57.170946+00:00 [scheduled]>
[2024-03-12T22:38:04.663+0530] {scheduler_job_runner.py:487} INFO - DAG taskflow_branching_operators_dag has 0/16 running and queued tasks
[2024-03-12T22:38:04.664+0530] {scheduler_job_runner.py:603} INFO - Setting the following tasks to queued state:
        <TaskInstance: taskflow_branching_operators_dag.branch manual__2024-03-12T17:07:57.170946+00:00 [scheduled]>
[2024-03-12T22:38:04.665+0530] {taskinstance.py:2283} WARNING - cannot record scheduled_duration for task branch because previous state change time has not been saved
[2024-03-12T22:38:04.665+0530] {scheduler_job_runner.py:646} INFO - Sending TaskInstanceKey(dag_id='taskflow_branching_operators_dag', task_id='branch', run_id='manual__2024-03-12T17:07:57.170946+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default
[2024-03-12T22:38:04.665+0530] {base_executor.py:146} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'taskflow_branching_operators_dag', 'branch', 'manual__2024-03-12T17:07:57.170946+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_branching_operators_dag.py']
[2024-03-12T22:38:04.687+0530] {sequential_executor.py:73} INFO - Executing command: ['airflow', 'tasks', 'run', 'taskflow_branching_operators_dag', 'branch', 'manual__2024-03-12T17:07:57.170946+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_branching_operators_dag.py']
[2024-03-12T22:38:05.770+0530] {dagbag.py:540} INFO - Filling up the DagBag from /home/kai/airflow/dags/taskflow_branching_operators_dag.py
Changing /home/kai/airflow/logs/dag_id=taskflow_branching_operators_dag/run_id=manual__2024-03-12T17:07:57.170946+00:00/task_id=branch permission to 509
[2024-03-12T22:38:06.433+0530] {task_command.py:423} INFO - Running <TaskInstance: taskflow_branching_operators_dag.branch manual__2024-03-12T17:07:57.170946+00:00 [queued]> on host BreakThrough.
[2024-03-12T22:38:07.363+0530] {scheduler_job_runner.py:696} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='taskflow_branching_operators_dag', task_id='branch', run_id='manual__2024-03-12T17:07:57.170946+00:00', try_number=1, map_index=-1)
[2024-03-12T22:38:07.367+0530] {scheduler_job_runner.py:733} INFO - TaskInstance Finished: dag_id=taskflow_branching_operators_dag, task_id=branch, run_id=manual__2024-03-12T17:07:57.170946+00:00, map_index=-1, run_start_date=2024-03-12 17:08:06.516014+00:00, run_end_date=2024-03-12 17:08:06.816457+00:00, run_duration=0.300443, state=success, executor_state=success, try_number=1, max_tries=0, job_id=207, pool=default_pool, queue=default, priority_weight=4, operator=_BranchPythonDecoratedOperator, queued_dttm=2024-03-12 17:08:04.664552+00:00, queued_by_job_id=172, pid=279751
[2024-03-12T22:38:07.461+0530] {scheduler_job_runner.py:424} INFO - 1 tasks up for execution:
        <TaskInstance: taskflow_branching_operators_dag.filter_by_weight manual__2024-03-12T17:07:57.170946+00:00 [scheduled]>
[2024-03-12T22:38:07.461+0530] {scheduler_job_runner.py:487} INFO - DAG taskflow_branching_operators_dag has 0/16 running and queued tasks
[2024-03-12T22:38:07.462+0530] {scheduler_job_runner.py:603} INFO - Setting the following tasks to queued state:
        <TaskInstance: taskflow_branching_operators_dag.filter_by_weight manual__2024-03-12T17:07:57.170946+00:00 [scheduled]>
[2024-03-12T22:38:07.463+0530] {taskinstance.py:2283} WARNING - cannot record scheduled_duration for task filter_by_weight because previous state change time has not been saved
[2024-03-12T22:38:07.464+0530] {scheduler_job_runner.py:646} INFO - Sending TaskInstanceKey(dag_id='taskflow_branching_operators_dag', task_id='filter_by_weight', run_id='manual__2024-03-12T17:07:57.170946+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default
[2024-03-12T22:38:07.464+0530] {base_executor.py:146} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'taskflow_branching_operators_dag', 'filter_by_weight', 'manual__2024-03-12T17:07:57.170946+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_branching_operators_dag.py']
[2024-03-12T22:38:07.487+0530] {sequential_executor.py:73} INFO - Executing command: ['airflow', 'tasks', 'run', 'taskflow_branching_operators_dag', 'filter_by_weight', 'manual__2024-03-12T17:07:57.170946+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_branching_operators_dag.py']
[2024-03-12T22:38:08.506+0530] {dagbag.py:540} INFO - Filling up the DagBag from /home/kai/airflow/dags/taskflow_branching_operators_dag.py
Changing /home/kai/airflow/logs/dag_id=taskflow_branching_operators_dag/run_id=manual__2024-03-12T17:07:57.170946+00:00/task_id=filter_by_weight permission to 509
[2024-03-12T22:38:09.061+0530] {task_command.py:423} INFO - Running <TaskInstance: taskflow_branching_operators_dag.filter_by_weight manual__2024-03-12T17:07:57.170946+00:00 [queued]> on host BreakThrough.
[2024-03-12T22:38:09.823+0530] {scheduler_job_runner.py:696} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='taskflow_branching_operators_dag', task_id='filter_by_weight', run_id='manual__2024-03-12T17:07:57.170946+00:00', try_number=1, map_index=-1)
[2024-03-12T22:38:09.830+0530] {scheduler_job_runner.py:733} INFO - TaskInstance Finished: dag_id=taskflow_branching_operators_dag, task_id=filter_by_weight, run_id=manual__2024-03-12T17:07:57.170946+00:00, map_index=-1, run_start_date=2024-03-12 17:08:09.123950+00:00, run_end_date=2024-03-12 17:08:09.322380+00:00, run_duration=0.19843, state=failed, executor_state=success, try_number=1, max_tries=0, job_id=208, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2024-03-12 17:08:07.462732+00:00, queued_by_job_id=172, pid=279760
[2024-03-12T22:38:09.856+0530] {dagrun.py:774} ERROR - Marking run <DagRun taskflow_branching_operators_dag @ 2024-03-12 17:07:57.170946+00:00: manual__2024-03-12T17:07:57.170946+00:00, state:running, queued_at: 2024-03-12 17:07:57.187706+00:00. externally triggered: True> failed
[2024-03-12T22:38:09.857+0530] {dagrun.py:846} INFO - DagRun Finished: dag_id=taskflow_branching_operators_dag, execution_date=2024-03-12 17:07:57.170946+00:00, run_id=manual__2024-03-12T17:07:57.170946+00:00, run_start_date=2024-03-12 17:07:58.652066+00:00, run_end_date=2024-03-12 17:08:09.857193+00:00, run_duration=11.205127, state=failed, external_trigger=True, run_type=manual, data_interval_start=2024-03-11 00:00:00+00:00, data_interval_end=2024-03-12 00:00:00+00:00, dag_hash=a000a4fb1a529224d1c4bc49cab2a792
[2024-03-12T22:38:17.372+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-12T22:39:02.421+0530] {scheduler_job_runner.py:424} INFO - 1 tasks up for execution:
        <TaskInstance: taskflow_branching_operators_dag.read_data manual__2024-03-12T17:09:01.323586+00:00 [scheduled]>
[2024-03-12T22:39:02.421+0530] {scheduler_job_runner.py:487} INFO - DAG taskflow_branching_operators_dag has 0/16 running and queued tasks
[2024-03-12T22:39:02.422+0530] {scheduler_job_runner.py:603} INFO - Setting the following tasks to queued state:
        <TaskInstance: taskflow_branching_operators_dag.read_data manual__2024-03-12T17:09:01.323586+00:00 [scheduled]>
[2024-03-12T22:39:02.858+0530] {taskinstance.py:2283} WARNING - cannot record scheduled_duration for task read_data becauseprevious state change time has not been saved
[2024-03-12T22:39:02.860+0530] {scheduler_job_runner.py:646} INFO - Sending TaskInstanceKey(dag_id='taskflow_branching_operators_dag', task_id='read_data', run_id='manual__2024-03-12T17:09:01.323586+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default
[2024-03-12T22:39:02.860+0530] {base_executor.py:146} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'taskflow_branching_operators_dag', 'read_data', 'manual__2024-03-12T17:09:01.323586+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_branching_operators_dag.py']
[2024-03-12T22:39:02.886+0530] {sequential_executor.py:73} INFO - Executing command: ['airflow', 'tasks', 'run', 'taskflow_branching_operators_dag', 'read_data', 'manual__2024-03-12T17:09:01.323586+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_branching_operators_dag.py']
[2024-03-12T22:39:04.211+0530] {dagbag.py:540} INFO - Filling up the DagBag from /home/kai/airflow/dags/taskflow_branching_operators_dag.py
Changing /home/kai/airflow/logs/dag_id=taskflow_branching_operators_dag/run_id=manual__2024-03-12T17:09:01.323586+00:00/task_id=read_data permission to 509
Changing /home/kai/airflow/logs/dag_id=taskflow_branching_operators_dag/run_id=manual__2024-03-12T17:09:01.323586+00:00 permission to 509
[2024-03-12T22:39:04.893+0530] {task_command.py:423} INFO - Running <TaskInstance: taskflow_branching_operators_dag.read_data manual__2024-03-12T17:09:01.323586+00:00 [queued]> on host BreakThrough.
[2024-03-12T22:39:05.959+0530] {scheduler_job_runner.py:696} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='taskflow_branching_operators_dag', task_id='read_data', run_id='manual__2024-03-12T17:09:01.323586+00:00', try_number=1, map_index=-1)
[2024-03-12T22:39:05.963+0530] {scheduler_job_runner.py:733} INFO - TaskInstance Finished: dag_id=taskflow_branching_operators_dag, task_id=read_data, run_id=manual__2024-03-12T17:09:01.323586+00:00, map_index=-1, run_start_date=2024-03-12 17:09:04.959009+00:00, run_end_date=2024-03-12 17:09:05.402175+00:00, run_duration=0.443166, state=success, executor_state=success,try_number=1, max_tries=0, job_id=209, pool=default_pool, queue=default, priority_weight=6, operator=_PythonDecoratedOperator, queued_dttm=2024-03-12 17:09:02.424982+00:00, queued_by_job_id=172, pid=279876
[2024-03-12T22:39:06.072+0530] {scheduler_job_runner.py:424} INFO - 1 tasks up for execution:
        <TaskInstance: taskflow_branching_operators_dag.clean_data manual__2024-03-12T17:09:01.323586+00:00 [scheduled]>
[2024-03-12T22:39:06.072+0530] {scheduler_job_runner.py:487} INFO - DAG taskflow_branching_operators_dag has 0/16 running and queued tasks
[2024-03-12T22:39:06.072+0530] {scheduler_job_runner.py:603} INFO - Setting the following tasks to queued state:
        <TaskInstance: taskflow_branching_operators_dag.clean_data manual__2024-03-12T17:09:01.323586+00:00 [scheduled]>
[2024-03-12T22:39:06.073+0530] {taskinstance.py:2283} WARNING - cannot record scheduled_duration for task clean_data because previous state change time has not been saved
[2024-03-12T22:39:06.074+0530] {scheduler_job_runner.py:646} INFO - Sending TaskInstanceKey(dag_id='taskflow_branching_operators_dag', task_id='clean_data', run_id='manual__2024-03-12T17:09:01.323586+00:00', try_number=1, map_index=-1) to executorwith priority 5 and queue default
[2024-03-12T22:39:06.074+0530] {base_executor.py:146} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'taskflow_branching_operators_dag', 'clean_data', 'manual__2024-03-12T17:09:01.323586+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_branching_operators_dag.py']
[2024-03-12T22:39:06.097+0530] {sequential_executor.py:73} INFO - Executing command: ['airflow', 'tasks', 'run', 'taskflow_branching_operators_dag', 'clean_data', 'manual__2024-03-12T17:09:01.323586+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_branching_operators_dag.py']
[2024-03-12T22:39:07.395+0530] {dagbag.py:540} INFO - Filling up the DagBag from /home/kai/airflow/dags/taskflow_branching_operators_dag.py
Changing /home/kai/airflow/logs/dag_id=taskflow_branching_operators_dag/run_id=manual__2024-03-12T17:09:01.323586+00:00/task_id=clean_data permission to 509
[2024-03-12T22:39:08.004+0530] {task_command.py:423} INFO - Running <TaskInstance: taskflow_branching_operators_dag.clean_data manual__2024-03-12T17:09:01.323586+00:00 [queued]> on host BreakThrough.
[2024-03-12T22:39:08.868+0530] {scheduler_job_runner.py:696} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='taskflow_branching_operators_dag', task_id='clean_data', run_id='manual__2024-03-12T17:09:01.323586+00:00', try_number=1, map_index=-1)
[2024-03-12T22:39:08.872+0530] {scheduler_job_runner.py:733} INFO - TaskInstance Finished: dag_id=taskflow_branching_operators_dag, task_id=clean_data, run_id=manual__2024-03-12T17:09:01.323586+00:00, map_index=-1, run_start_date=2024-03-12 17:09:08.062790+00:00, run_end_date=2024-03-12 17:09:08.325766+00:00, run_duration=0.262976, state=success, executor_state=success, try_number=1, max_tries=0, job_id=210, pool=default_pool, queue=default, priority_weight=5, operator=_PythonDecoratedOperator, queued_dttm=2024-03-12 17:09:06.073137+00:00, queued_by_job_id=172, pid=279885
[2024-03-12T22:39:08.940+0530] {scheduler_job_runner.py:424} INFO - 1 tasks up for execution:
        <TaskInstance: taskflow_branching_operators_dag.branch manual__2024-03-12T17:09:01.323586+00:00 [scheduled]>
[2024-03-12T22:39:08.940+0530] {scheduler_job_runner.py:487} INFO - DAG taskflow_branching_operators_dag has 0/16 running and queued tasks
[2024-03-12T22:39:08.940+0530] {scheduler_job_runner.py:603} INFO - Setting the following tasks to queued state:
        <TaskInstance: taskflow_branching_operators_dag.branch manual__2024-03-12T17:09:01.323586+00:00 [scheduled]>
[2024-03-12T22:39:08.941+0530] {taskinstance.py:2283} WARNING - cannot record scheduled_duration for task branch because previous state change time has not been saved
[2024-03-12T22:39:08.942+0530] {scheduler_job_runner.py:646} INFO - Sending TaskInstanceKey(dag_id='taskflow_branching_operators_dag', task_id='branch', run_id='manual__2024-03-12T17:09:01.323586+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default
[2024-03-12T22:39:08.942+0530] {base_executor.py:146} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'taskflow_branching_operators_dag', 'branch', 'manual__2024-03-12T17:09:01.323586+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_branching_operators_dag.py']
[2024-03-12T22:39:08.966+0530] {sequential_executor.py:73} INFO - Executing command: ['airflow', 'tasks', 'run', 'taskflow_branching_operators_dag', 'branch', 'manual__2024-03-12T17:09:01.323586+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_branching_operators_dag.py']
[2024-03-12T22:39:09.992+0530] {dagbag.py:540} INFO - Filling up the DagBag from /home/kai/airflow/dags/taskflow_branching_operators_dag.py
Changing /home/kai/airflow/logs/dag_id=taskflow_branching_operators_dag/run_id=manual__2024-03-12T17:09:01.323586+00:00/task_id=branch permission to 509
[2024-03-12T22:39:10.556+0530] {task_command.py:423} INFO - Running <TaskInstance: taskflow_branching_operators_dag.branch manual__2024-03-12T17:09:01.323586+00:00 [queued]> on host BreakThrough.
[2024-03-12T22:39:11.408+0530] {scheduler_job_runner.py:696} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='taskflow_branching_operators_dag', task_id='branch', run_id='manual__2024-03-12T17:09:01.323586+00:00', try_number=1, map_index=-1)
[2024-03-12T22:39:11.412+0530] {scheduler_job_runner.py:733} INFO - TaskInstance Finished: dag_id=taskflow_branching_operators_dag, task_id=branch, run_id=manual__2024-03-12T17:09:01.323586+00:00, map_index=-1, run_start_date=2024-03-12 17:09:10.627138+00:00, run_end_date=2024-03-12 17:09:10.917157+00:00, run_duration=0.290019, state=success, executor_state=success, try_number=1, max_tries=0, job_id=211, pool=default_pool, queue=default, priority_weight=4, operator=_BranchPythonDecoratedOperator, queued_dttm=2024-03-12 17:09:08.941193+00:00, queued_by_job_id=172, pid=279894
[2024-03-12T22:39:11.492+0530] {scheduler_job_runner.py:424} INFO - 1 tasks up for execution:
        <TaskInstance: taskflow_branching_operators_dag.filter_by_weight manual__2024-03-12T17:09:01.323586+00:00 [scheduled]>
[2024-03-12T22:39:11.492+0530] {scheduler_job_runner.py:487} INFO - DAG taskflow_branching_operators_dag has 0/16 running and queued tasks
[2024-03-12T22:39:11.493+0530] {scheduler_job_runner.py:603} INFO - Setting the following tasks to queued state:
        <TaskInstance: taskflow_branching_operators_dag.filter_by_weight manual__2024-03-12T17:09:01.323586+00:00 [scheduled]>
[2024-03-12T22:39:11.494+0530] {taskinstance.py:2283} WARNING - cannot record scheduled_duration for task filter_by_weight because previous state change time has not been saved
[2024-03-12T22:39:11.494+0530] {scheduler_job_runner.py:646} INFO - Sending TaskInstanceKey(dag_id='taskflow_branching_operators_dag', task_id='filter_by_weight', run_id='manual__2024-03-12T17:09:01.323586+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default
[2024-03-12T22:39:11.495+0530] {base_executor.py:146} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'taskflow_branching_operators_dag', 'filter_by_weight', 'manual__2024-03-12T17:09:01.323586+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_branching_operators_dag.py']
[2024-03-12T22:39:11.515+0530] {sequential_executor.py:73} INFO - Executing command: ['airflow', 'tasks', 'run', 'taskflow_branching_operators_dag', 'filter_by_weight', 'manual__2024-03-12T17:09:01.323586+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_branching_operators_dag.py']
[2024-03-12T22:39:12.471+0530] {dagbag.py:540} INFO - Filling up the DagBag from /home/kai/airflow/dags/taskflow_branching_operators_dag.py
Changing /home/kai/airflow/logs/dag_id=taskflow_branching_operators_dag/run_id=manual__2024-03-12T17:09:01.323586+00:00/task_id=filter_by_weight permission to 509
[2024-03-12T22:39:13.065+0530] {task_command.py:423} INFO - Running <TaskInstance: taskflow_branching_operators_dag.filter_by_weight manual__2024-03-12T17:09:01.323586+00:00 [queued]> on host BreakThrough.
[2024-03-12T22:39:13.932+0530] {scheduler_job_runner.py:696} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='taskflow_branching_operators_dag', task_id='filter_by_weight', run_id='manual__2024-03-12T17:09:01.323586+00:00', try_number=1, map_index=-1)
[2024-03-12T22:39:13.936+0530] {scheduler_job_runner.py:733} INFO - TaskInstance Finished: dag_id=taskflow_branching_operators_dag, task_id=filter_by_weight, run_id=manual__2024-03-12T17:09:01.323586+00:00, map_index=-1, run_start_date=2024-03-12 17:09:13.143331+00:00, run_end_date=2024-03-12 17:09:13.435648+00:00, run_duration=0.292317, state=success, executor_state=success, try_number=1, max_tries=0, job_id=212, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2024-03-12 17:09:11.493669+00:00, queued_by_job_id=172, pid=279903
[2024-03-12T22:39:14.215+0530] {scheduler_job_runner.py:424} INFO - 1 tasks up for execution:
        <TaskInstance: taskflow_branching_operators_dag.write_csv manual__2024-03-12T17:09:01.323586+00:00 [scheduled]>
[2024-03-12T22:39:14.216+0530] {scheduler_job_runner.py:487} INFO - DAG taskflow_branching_operators_dag has 0/16 running and queued tasks
[2024-03-12T22:39:14.217+0530] {scheduler_job_runner.py:603} INFO - Setting the following tasks to queued state:
        <TaskInstance: taskflow_branching_operators_dag.write_csv manual__2024-03-12T17:09:01.323586+00:00 [scheduled]>
[2024-03-12T22:39:14.219+0530] {taskinstance.py:2283} WARNING - cannot record scheduled_duration for task write_csv becauseprevious state change time has not been saved
[2024-03-12T22:39:14.220+0530] {scheduler_job_runner.py:646} INFO - Sending TaskInstanceKey(dag_id='taskflow_branching_operators_dag', task_id='write_csv', run_id='manual__2024-03-12T17:09:01.323586+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default
[2024-03-12T22:39:14.220+0530] {base_executor.py:146} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'taskflow_branching_operators_dag', 'write_csv', 'manual__2024-03-12T17:09:01.323586+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_branching_operators_dag.py']
[2024-03-12T22:39:14.242+0530] {sequential_executor.py:73} INFO - Executing command: ['airflow', 'tasks', 'run', 'taskflow_branching_operators_dag', 'write_csv', 'manual__2024-03-12T17:09:01.323586+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_branching_operators_dag.py']
[2024-03-12T22:39:15.245+0530] {dagbag.py:540} INFO - Filling up the DagBag from /home/kai/airflow/dags/taskflow_branching_operators_dag.py
Changing /home/kai/airflow/logs/dag_id=taskflow_branching_operators_dag/run_id=manual__2024-03-12T17:09:01.323586+00:00/task_id=write_csv permission to 509
[2024-03-12T22:39:15.798+0530] {task_command.py:423} INFO - Running <TaskInstance: taskflow_branching_operators_dag.write_csv manual__2024-03-12T17:09:01.323586+00:00 [queued]> on host BreakThrough.
[2024-03-12T22:39:16.522+0530] {scheduler_job_runner.py:696} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='taskflow_branching_operators_dag', task_id='write_csv', run_id='manual__2024-03-12T17:09:01.323586+00:00', try_number=1, map_index=-1)
[2024-03-12T22:39:16.526+0530] {scheduler_job_runner.py:733} INFO - TaskInstance Finished: dag_id=taskflow_branching_operators_dag, task_id=write_csv, run_id=manual__2024-03-12T17:09:01.323586+00:00, map_index=-1, run_start_date=2024-03-12 17:09:15.858100+00:00, run_end_date=2024-03-12 17:09:16.041292+00:00, run_duration=0.183192, state=failed, executor_state=success, try_number=1, max_tries=0, job_id=213, pool=default_pool, queue=default, priority_weight=1, operator=_PythonDecoratedOperator, queued_dttm=2024-03-12 17:09:14.218153+00:00, queued_by_job_id=172, pid=279913
[2024-03-12T22:39:16.808+0530] {dagrun.py:774} ERROR - Marking run <DagRun taskflow_branching_operators_dag @ 2024-03-12 17:09:01.323586+00:00: manual__2024-03-12T17:09:01.323586+00:00, state:running, queued_at: 2024-03-12 17:09:01.331495+00:00. externally triggered: True> failed
[2024-03-12T22:39:16.809+0530] {dagrun.py:846} INFO - DagRun Finished: dag_id=taskflow_branching_operators_dag, execution_date=2024-03-12 17:09:01.323586+00:00, run_id=manual__2024-03-12T17:09:01.323586+00:00, run_start_date=2024-03-12 17:09:02.132523+00:00, run_end_date=2024-03-12 17:09:16.809168+00:00, run_duration=14.676645, state=failed, external_trigger=True, run_type=manual, data_interval_start=2024-03-11 00:00:00+00:00, data_interval_end=2024-03-12 00:00:00+00:00, dag_hash=a000a4fb1a529224d1c4bc49cab2a792
[2024-03-12T22:39:39.944+0530] {scheduler_job_runner.py:424} INFO - 1 tasks up for execution:
        <TaskInstance: taskflow_branching_operators_dag.read_data manual__2024-03-12T17:09:38.794089+00:00 [scheduled]>
[2024-03-12T22:39:39.945+0530] {scheduler_job_runner.py:487} INFO - DAG taskflow_branching_operators_dag has 0/16 running and queued tasks
[2024-03-12T22:39:39.946+0530] {scheduler_job_runner.py:603} INFO - Setting the following tasks to queued state:
        <TaskInstance: taskflow_branching_operators_dag.read_data manual__2024-03-12T17:09:38.794089+00:00 [scheduled]>
[2024-03-12T22:39:39.948+0530] {taskinstance.py:2283} WARNING - cannot record scheduled_duration for task read_data becauseprevious state change time has not been saved
[2024-03-12T22:39:39.949+0530] {scheduler_job_runner.py:646} INFO - Sending TaskInstanceKey(dag_id='taskflow_branching_operators_dag', task_id='read_data', run_id='manual__2024-03-12T17:09:38.794089+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default
[2024-03-12T22:39:39.949+0530] {base_executor.py:146} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'taskflow_branching_operators_dag', 'read_data', 'manual__2024-03-12T17:09:38.794089+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_branching_operators_dag.py']
[2024-03-12T22:39:39.977+0530] {sequential_executor.py:73} INFO - Executing command: ['airflow', 'tasks', 'run', 'taskflow_branching_operators_dag', 'read_data', 'manual__2024-03-12T17:09:38.794089+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_branching_operators_dag.py']
[2024-03-12T22:39:41.537+0530] {dagbag.py:540} INFO - Filling up the DagBag from /home/kai/airflow/dags/taskflow_branching_operators_dag.py
Changing /home/kai/airflow/logs/dag_id=taskflow_branching_operators_dag/run_id=manual__2024-03-12T17:09:38.794089+00:00/task_id=read_data permission to 509
Changing /home/kai/airflow/logs/dag_id=taskflow_branching_operators_dag/run_id=manual__2024-03-12T17:09:38.794089+00:00 permission to 509
[2024-03-12T22:39:42.183+0530] {task_command.py:423} INFO - Running <TaskInstance: taskflow_branching_operators_dag.read_data manual__2024-03-12T17:09:38.794089+00:00 [queued]> on host BreakThrough.
[2024-03-12T22:39:43.239+0530] {scheduler_job_runner.py:696} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='taskflow_branching_operators_dag', task_id='read_data', run_id='manual__2024-03-12T17:09:38.794089+00:00', try_number=1, map_index=-1)
[2024-03-12T22:39:43.246+0530] {scheduler_job_runner.py:733} INFO - TaskInstance Finished: dag_id=taskflow_branching_operators_dag, task_id=read_data, run_id=manual__2024-03-12T17:09:38.794089+00:00, map_index=-1, run_start_date=2024-03-12 17:09:42.270685+00:00, run_end_date=2024-03-12 17:09:42.605180+00:00, run_duration=0.334495, state=success, executor_state=success,try_number=1, max_tries=0, job_id=214, pool=default_pool, queue=default, priority_weight=6, operator=_PythonDecoratedOperator, queued_dttm=2024-03-12 17:09:39.947385+00:00, queued_by_job_id=172, pid=279981
[2024-03-12T22:39:43.426+0530] {scheduler_job_runner.py:424} INFO - 1 tasks up for execution:
        <TaskInstance: taskflow_branching_operators_dag.clean_data manual__2024-03-12T17:09:38.794089+00:00 [scheduled]>
[2024-03-12T22:39:43.427+0530] {scheduler_job_runner.py:487} INFO - DAG taskflow_branching_operators_dag has 0/16 running and queued tasks
[2024-03-12T22:39:43.427+0530] {scheduler_job_runner.py:603} INFO - Setting the following tasks to queued state:
        <TaskInstance: taskflow_branching_operators_dag.clean_data manual__2024-03-12T17:09:38.794089+00:00 [scheduled]>
[2024-03-12T22:39:43.430+0530] {taskinstance.py:2283} WARNING - cannot record scheduled_duration for task clean_data because previous state change time has not been saved
[2024-03-12T22:39:43.431+0530] {scheduler_job_runner.py:646} INFO - Sending TaskInstanceKey(dag_id='taskflow_branching_operators_dag', task_id='clean_data', run_id='manual__2024-03-12T17:09:38.794089+00:00', try_number=1, map_index=-1) to executorwith priority 5 and queue default
[2024-03-12T22:39:43.432+0530] {base_executor.py:146} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'taskflow_branching_operators_dag', 'clean_data', 'manual__2024-03-12T17:09:38.794089+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_branching_operators_dag.py']
[2024-03-12T22:39:43.456+0530] {sequential_executor.py:73} INFO - Executing command: ['airflow', 'tasks', 'run', 'taskflow_branching_operators_dag', 'clean_data', 'manual__2024-03-12T17:09:38.794089+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_branching_operators_dag.py']
[2024-03-12T22:39:44.582+0530] {dagbag.py:540} INFO - Filling up the DagBag from /home/kai/airflow/dags/taskflow_branching_operators_dag.py
Changing /home/kai/airflow/logs/dag_id=taskflow_branching_operators_dag/run_id=manual__2024-03-12T17:09:38.794089+00:00/task_id=clean_data permission to 509
[2024-03-12T22:39:45.138+0530] {task_command.py:423} INFO - Running <TaskInstance: taskflow_branching_operators_dag.clean_data manual__2024-03-12T17:09:38.794089+00:00 [queued]> on host BreakThrough.
[2024-03-12T22:39:45.988+0530] {scheduler_job_runner.py:696} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='taskflow_branching_operators_dag', task_id='clean_data', run_id='manual__2024-03-12T17:09:38.794089+00:00', try_number=1, map_index=-1)
[2024-03-12T22:39:45.992+0530] {scheduler_job_runner.py:733} INFO - TaskInstance Finished: dag_id=taskflow_branching_operators_dag, task_id=clean_data, run_id=manual__2024-03-12T17:09:38.794089+00:00, map_index=-1, run_start_date=2024-03-12 17:09:45.199248+00:00, run_end_date=2024-03-12 17:09:45.484504+00:00, run_duration=0.285256, state=success, executor_state=success, try_number=1, max_tries=0, job_id=215, pool=default_pool, queue=default, priority_weight=5, operator=_PythonDecoratedOperator, queued_dttm=2024-03-12 17:09:43.428470+00:00, queued_by_job_id=172, pid=279990
[2024-03-12T22:39:46.291+0530] {scheduler_job_runner.py:424} INFO - 1 tasks up for execution:
        <TaskInstance: taskflow_branching_operators_dag.branch manual__2024-03-12T17:09:38.794089+00:00 [scheduled]>
[2024-03-12T22:39:46.292+0530] {scheduler_job_runner.py:487} INFO - DAG taskflow_branching_operators_dag has 0/16 running and queued tasks
[2024-03-12T22:39:46.292+0530] {scheduler_job_runner.py:603} INFO - Setting the following tasks to queued state:
        <TaskInstance: taskflow_branching_operators_dag.branch manual__2024-03-12T17:09:38.794089+00:00 [scheduled]>
[2024-03-12T22:39:46.294+0530] {taskinstance.py:2283} WARNING - cannot record scheduled_duration for task branch because previous state change time has not been saved
[2024-03-12T22:39:46.294+0530] {scheduler_job_runner.py:646} INFO - Sending TaskInstanceKey(dag_id='taskflow_branching_operators_dag', task_id='branch', run_id='manual__2024-03-12T17:09:38.794089+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default
[2024-03-12T22:39:46.294+0530] {base_executor.py:146} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'taskflow_branching_operators_dag', 'branch', 'manual__2024-03-12T17:09:38.794089+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_branching_operators_dag.py']
[2024-03-12T22:39:46.315+0530] {sequential_executor.py:73} INFO - Executing command: ['airflow', 'tasks', 'run', 'taskflow_branching_operators_dag', 'branch', 'manual__2024-03-12T17:09:38.794089+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_branching_operators_dag.py']
[2024-03-12T22:39:47.301+0530] {dagbag.py:540} INFO - Filling up the DagBag from /home/kai/airflow/dags/taskflow_branching_operators_dag.py
Changing /home/kai/airflow/logs/dag_id=taskflow_branching_operators_dag/run_id=manual__2024-03-12T17:09:38.794089+00:00/task_id=branch permission to 509
[2024-03-12T22:39:47.901+0530] {task_command.py:423} INFO - Running <TaskInstance: taskflow_branching_operators_dag.branch manual__2024-03-12T17:09:38.794089+00:00 [queued]> on host BreakThrough.
[2024-03-12T22:39:48.780+0530] {scheduler_job_runner.py:696} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='taskflow_branching_operators_dag', task_id='branch', run_id='manual__2024-03-12T17:09:38.794089+00:00', try_number=1, map_index=-1)
[2024-03-12T22:39:48.785+0530] {scheduler_job_runner.py:733} INFO - TaskInstance Finished: dag_id=taskflow_branching_operators_dag, task_id=branch, run_id=manual__2024-03-12T17:09:38.794089+00:00, map_index=-1, run_start_date=2024-03-12 17:09:47.983138+00:00, run_end_date=2024-03-12 17:09:48.295398+00:00, run_duration=0.31226, state=success, executor_state=success, try_number=1, max_tries=0, job_id=216, pool=default_pool, queue=default, priority_weight=4, operator=_BranchPythonDecoratedOperator, queued_dttm=2024-03-12 17:09:46.292977+00:00, queued_by_job_id=172, pid=280000
[2024-03-12T22:39:49.114+0530] {scheduler_job_runner.py:424} INFO - 1 tasks up for execution:
        <TaskInstance: taskflow_branching_operators_dag.filter_by_weight manual__2024-03-12T17:09:38.794089+00:00 [scheduled]>
[2024-03-12T22:39:49.114+0530] {scheduler_job_runner.py:487} INFO - DAG taskflow_branching_operators_dag has 0/16 running and queued tasks
[2024-03-12T22:39:49.114+0530] {scheduler_job_runner.py:603} INFO - Setting the following tasks to queued state:
        <TaskInstance: taskflow_branching_operators_dag.filter_by_weight manual__2024-03-12T17:09:38.794089+00:00 [scheduled]>
[2024-03-12T22:39:49.116+0530] {taskinstance.py:2283} WARNING - cannot record scheduled_duration for task filter_by_weight because previous state change time has not been saved
[2024-03-12T22:39:49.116+0530] {scheduler_job_runner.py:646} INFO - Sending TaskInstanceKey(dag_id='taskflow_branching_operators_dag', task_id='filter_by_weight', run_id='manual__2024-03-12T17:09:38.794089+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default
[2024-03-12T22:39:49.117+0530] {base_executor.py:146} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'taskflow_branching_operators_dag', 'filter_by_weight', 'manual__2024-03-12T17:09:38.794089+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_branching_operators_dag.py']
[2024-03-12T22:39:49.138+0530] {sequential_executor.py:73} INFO - Executing command: ['airflow', 'tasks', 'run', 'taskflow_branching_operators_dag', 'filter_by_weight', 'manual__2024-03-12T17:09:38.794089+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_branching_operators_dag.py']
[2024-03-12T22:39:50.821+0530] {dagbag.py:540} INFO - Filling up the DagBag from /home/kai/airflow/dags/taskflow_branching_operators_dag.py
Changing /home/kai/airflow/logs/dag_id=taskflow_branching_operators_dag/run_id=manual__2024-03-12T17:09:38.794089+00:00/task_id=filter_by_weight permission to 509
[2024-03-12T22:39:51.441+0530] {task_command.py:423} INFO - Running <TaskInstance: taskflow_branching_operators_dag.filter_by_weight manual__2024-03-12T17:09:38.794089+00:00 [queued]> on host BreakThrough.
[2024-03-12T22:39:52.195+0530] {scheduler_job_runner.py:696} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='taskflow_branching_operators_dag', task_id='filter_by_weight', run_id='manual__2024-03-12T17:09:38.794089+00:00', try_number=1, map_index=-1)
[2024-03-12T22:39:52.201+0530] {scheduler_job_runner.py:733} INFO - TaskInstance Finished: dag_id=taskflow_branching_operators_dag, task_id=filter_by_weight, run_id=manual__2024-03-12T17:09:38.794089+00:00, map_index=-1, run_start_date=2024-03-12 17:09:51.506958+00:00, run_end_date=2024-03-12 17:09:51.765962+00:00, run_duration=0.259004, state=success, executor_state=success, try_number=1, max_tries=0, job_id=217, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2024-03-12 17:09:49.115017+00:00, queued_by_job_id=172, pid=280010
[2024-03-12T22:39:52.765+0530] {scheduler_job_runner.py:424} INFO - 1 tasks up for execution:
        <TaskInstance: taskflow_branching_operators_dag.write_csv manual__2024-03-12T17:09:38.794089+00:00 [scheduled]>
[2024-03-12T22:39:52.765+0530] {scheduler_job_runner.py:487} INFO - DAG taskflow_branching_operators_dag has 0/16 running and queued tasks
[2024-03-12T22:39:52.765+0530] {scheduler_job_runner.py:603} INFO - Setting the following tasks to queued state:
        <TaskInstance: taskflow_branching_operators_dag.write_csv manual__2024-03-12T17:09:38.794089+00:00 [scheduled]>
[2024-03-12T22:39:52.767+0530] {taskinstance.py:2283} WARNING - cannot record scheduled_duration for task write_csv becauseprevious state change time has not been saved
[2024-03-12T22:39:52.767+0530] {scheduler_job_runner.py:646} INFO - Sending TaskInstanceKey(dag_id='taskflow_branching_operators_dag', task_id='write_csv', run_id='manual__2024-03-12T17:09:38.794089+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default
[2024-03-12T22:39:52.767+0530] {base_executor.py:146} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'taskflow_branching_operators_dag', 'write_csv', 'manual__2024-03-12T17:09:38.794089+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_branching_operators_dag.py']
[2024-03-12T22:39:52.790+0530] {sequential_executor.py:73} INFO - Executing command: ['airflow', 'tasks', 'run', 'taskflow_branching_operators_dag', 'write_csv', 'manual__2024-03-12T17:09:38.794089+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_branching_operators_dag.py']
[2024-03-12T22:39:53.824+0530] {dagbag.py:540} INFO - Filling up the DagBag from /home/kai/airflow/dags/taskflow_branching_operators_dag.py
Changing /home/kai/airflow/logs/dag_id=taskflow_branching_operators_dag/run_id=manual__2024-03-12T17:09:38.794089+00:00/task_id=write_csv permission to 509
[2024-03-12T22:39:54.416+0530] {task_command.py:423} INFO - Running <TaskInstance: taskflow_branching_operators_dag.write_csv manual__2024-03-12T17:09:38.794089+00:00 [queued]> on host BreakThrough.
[2024-03-12T22:39:55.195+0530] {scheduler_job_runner.py:696} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='taskflow_branching_operators_dag', task_id='write_csv', run_id='manual__2024-03-12T17:09:38.794089+00:00', try_number=1, map_index=-1)
[2024-03-12T22:39:55.199+0530] {scheduler_job_runner.py:733} INFO - TaskInstance Finished: dag_id=taskflow_branching_operators_dag, task_id=write_csv, run_id=manual__2024-03-12T17:09:38.794089+00:00, map_index=-1, run_start_date=2024-03-12 17:09:54.490233+00:00, run_end_date=2024-03-12 17:09:54.755187+00:00, run_duration=0.264954, state=failed, executor_state=success, try_number=1, max_tries=0, job_id=218, pool=default_pool, queue=default, priority_weight=1, operator=_PythonDecoratedOperator, queued_dttm=2024-03-12 17:09:52.766311+00:00, queued_by_job_id=172, pid=280027
[2024-03-12T22:39:55.482+0530] {dagrun.py:774} ERROR - Marking run <DagRun taskflow_branching_operators_dag @ 2024-03-12 17:09:38.794089+00:00: manual__2024-03-12T17:09:38.794089+00:00, state:running, queued_at: 2024-03-12 17:09:38.808523+00:00. externally triggered: True> failed
[2024-03-12T22:39:55.482+0530] {dagrun.py:846} INFO - DagRun Finished: dag_id=taskflow_branching_operators_dag, execution_date=2024-03-12 17:09:38.794089+00:00, run_id=manual__2024-03-12T17:09:38.794089+00:00, run_start_date=2024-03-12 17:09:39.858817+00:00, run_end_date=2024-03-12 17:09:55.482615+00:00, run_duration=15.623798, state=failed, external_trigger=True, run_type=manual, data_interval_start=2024-03-11 00:00:00+00:00, data_interval_end=2024-03-12 00:00:00+00:00, dag_hash=a000a4fb1a529224d1c4bc49cab2a792
[2024-03-12T22:40:29.757+0530] {scheduler_job_runner.py:424} INFO - 1 tasks up for execution:
        <TaskInstance: taskflow_branching_operators_dag.read_data manual__2024-03-12T17:10:28.193064+00:00 [scheduled]>
[2024-03-12T22:40:29.759+0530] {scheduler_job_runner.py:487} INFO - DAG taskflow_branching_operators_dag has 0/16 running and queued tasks
[2024-03-12T22:40:29.761+0530] {scheduler_job_runner.py:603} INFO - Setting the following tasks to queued state:
        <TaskInstance: taskflow_branching_operators_dag.read_data manual__2024-03-12T17:10:28.193064+00:00 [scheduled]>
[2024-03-12T22:40:29.766+0530] {taskinstance.py:2283} WARNING - cannot record scheduled_duration for task read_data becauseprevious state change time has not been saved
[2024-03-12T22:40:29.768+0530] {scheduler_job_runner.py:646} INFO - Sending TaskInstanceKey(dag_id='taskflow_branching_operators_dag', task_id='read_data', run_id='manual__2024-03-12T17:10:28.193064+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default
[2024-03-12T22:40:29.769+0530] {base_executor.py:146} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'taskflow_branching_operators_dag', 'read_data', 'manual__2024-03-12T17:10:28.193064+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_branching_operators_dag.py']
[2024-03-12T22:40:29.791+0530] {sequential_executor.py:73} INFO - Executing command: ['airflow', 'tasks', 'run', 'taskflow_branching_operators_dag', 'read_data', 'manual__2024-03-12T17:10:28.193064+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_branching_operators_dag.py']
[2024-03-12T22:40:31.066+0530] {dagbag.py:540} INFO - Filling up the DagBag from /home/kai/airflow/dags/taskflow_branching_operators_dag.py
Changing /home/kai/airflow/logs/dag_id=taskflow_branching_operators_dag/run_id=manual__2024-03-12T17:10:28.193064+00:00/task_id=read_data permission to 509
Changing /home/kai/airflow/logs/dag_id=taskflow_branching_operators_dag/run_id=manual__2024-03-12T17:10:28.193064+00:00 permission to 509
[2024-03-12T22:40:31.680+0530] {task_command.py:423} INFO - Running <TaskInstance: taskflow_branching_operators_dag.read_data manual__2024-03-12T17:10:28.193064+00:00 [queued]> on host BreakThrough.
[2024-03-12T22:40:32.515+0530] {scheduler_job_runner.py:696} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='taskflow_branching_operators_dag', task_id='read_data', run_id='manual__2024-03-12T17:10:28.193064+00:00', try_number=1, map_index=-1)
[2024-03-12T22:40:32.520+0530] {scheduler_job_runner.py:733} INFO - TaskInstance Finished: dag_id=taskflow_branching_operators_dag, task_id=read_data, run_id=manual__2024-03-12T17:10:28.193064+00:00, map_index=-1, run_start_date=2024-03-12 17:10:31.753196+00:00, run_end_date=2024-03-12 17:10:32.009108+00:00, run_duration=0.255912, state=success, executor_state=success,try_number=1, max_tries=0, job_id=219, pool=default_pool, queue=default, priority_weight=6, operator=_PythonDecoratedOperator, queued_dttm=2024-03-12 17:10:29.764090+00:00, queued_by_job_id=172, pid=280102
[2024-03-12T22:40:32.816+0530] {scheduler_job_runner.py:424} INFO - 1 tasks up for execution:
        <TaskInstance: taskflow_branching_operators_dag.clean_data manual__2024-03-12T17:10:28.193064+00:00 [scheduled]>
[2024-03-12T22:40:32.816+0530] {scheduler_job_runner.py:487} INFO - DAG taskflow_branching_operators_dag has 0/16 running and queued tasks
[2024-03-12T22:40:32.817+0530] {scheduler_job_runner.py:603} INFO - Setting the following tasks to queued state:
        <TaskInstance: taskflow_branching_operators_dag.clean_data manual__2024-03-12T17:10:28.193064+00:00 [scheduled]>
[2024-03-12T22:40:32.818+0530] {taskinstance.py:2283} WARNING - cannot record scheduled_duration for task clean_data because previous state change time has not been saved
[2024-03-12T22:40:32.818+0530] {scheduler_job_runner.py:646} INFO - Sending TaskInstanceKey(dag_id='taskflow_branching_operators_dag', task_id='clean_data', run_id='manual__2024-03-12T17:10:28.193064+00:00', try_number=1, map_index=-1) to executorwith priority 5 and queue default
[2024-03-12T22:40:32.819+0530] {base_executor.py:146} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'taskflow_branching_operators_dag', 'clean_data', 'manual__2024-03-12T17:10:28.193064+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_branching_operators_dag.py']
[2024-03-12T22:40:32.837+0530] {sequential_executor.py:73} INFO - Executing command: ['airflow', 'tasks', 'run', 'taskflow_branching_operators_dag', 'clean_data', 'manual__2024-03-12T17:10:28.193064+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_branching_operators_dag.py']
[2024-03-12T22:40:33.969+0530] {dagbag.py:540} INFO - Filling up the DagBag from /home/kai/airflow/dags/taskflow_branching_operators_dag.py
Changing /home/kai/airflow/logs/dag_id=taskflow_branching_operators_dag/run_id=manual__2024-03-12T17:10:28.193064+00:00/task_id=clean_data permission to 509
[2024-03-12T22:40:34.579+0530] {task_command.py:423} INFO - Running <TaskInstance: taskflow_branching_operators_dag.clean_data manual__2024-03-12T17:10:28.193064+00:00 [queued]> on host BreakThrough.
[2024-03-12T22:40:35.507+0530] {scheduler_job_runner.py:696} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='taskflow_branching_operators_dag', task_id='clean_data', run_id='manual__2024-03-12T17:10:28.193064+00:00', try_number=1, map_index=-1)
[2024-03-12T22:40:35.513+0530] {scheduler_job_runner.py:733} INFO - TaskInstance Finished: dag_id=taskflow_branching_operators_dag, task_id=clean_data, run_id=manual__2024-03-12T17:10:28.193064+00:00, map_index=-1, run_start_date=2024-03-12 17:10:34.643888+00:00, run_end_date=2024-03-12 17:10:34.927528+00:00, run_duration=0.28364, state=success, executor_state=success,try_number=1, max_tries=0, job_id=220, pool=default_pool, queue=default, priority_weight=5, operator=_PythonDecoratedOperator, queued_dttm=2024-03-12 17:10:32.817513+00:00, queued_by_job_id=172, pid=280112
[2024-03-12T22:40:36.311+0530] {scheduler_job_runner.py:424} INFO - 1 tasks up for execution:
        <TaskInstance: taskflow_branching_operators_dag.branch manual__2024-03-12T17:10:28.193064+00:00 [scheduled]>
[2024-03-12T22:40:36.312+0530] {scheduler_job_runner.py:487} INFO - DAG taskflow_branching_operators_dag has 0/16 running and queued tasks
[2024-03-12T22:40:36.312+0530] {scheduler_job_runner.py:603} INFO - Setting the following tasks to queued state:
        <TaskInstance: taskflow_branching_operators_dag.branch manual__2024-03-12T17:10:28.193064+00:00 [scheduled]>
[2024-03-12T22:40:36.313+0530] {taskinstance.py:2283} WARNING - cannot record scheduled_duration for task branch because previous state change time has not been saved
[2024-03-12T22:40:36.314+0530] {scheduler_job_runner.py:646} INFO - Sending TaskInstanceKey(dag_id='taskflow_branching_operators_dag', task_id='branch', run_id='manual__2024-03-12T17:10:28.193064+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default
[2024-03-12T22:40:36.314+0530] {base_executor.py:146} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'taskflow_branching_operators_dag', 'branch', 'manual__2024-03-12T17:10:28.193064+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_branching_operators_dag.py']
[2024-03-12T22:40:36.335+0530] {sequential_executor.py:73} INFO - Executing command: ['airflow', 'tasks', 'run', 'taskflow_branching_operators_dag', 'branch', 'manual__2024-03-12T17:10:28.193064+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_branching_operators_dag.py']
[2024-03-12T22:40:37.478+0530] {dagbag.py:540} INFO - Filling up the DagBag from /home/kai/airflow/dags/taskflow_branching_operators_dag.py
Changing /home/kai/airflow/logs/dag_id=taskflow_branching_operators_dag/run_id=manual__2024-03-12T17:10:28.193064+00:00/task_id=branch permission to 509
[2024-03-12T22:40:38.050+0530] {task_command.py:423} INFO - Running <TaskInstance: taskflow_branching_operators_dag.branch manual__2024-03-12T17:10:28.193064+00:00 [queued]> on host BreakThrough.
[2024-03-12T22:40:39.074+0530] {scheduler_job_runner.py:696} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='taskflow_branching_operators_dag', task_id='branch', run_id='manual__2024-03-12T17:10:28.193064+00:00', try_number=1, map_index=-1)
[2024-03-12T22:40:39.078+0530] {scheduler_job_runner.py:733} INFO - TaskInstance Finished: dag_id=taskflow_branching_operators_dag, task_id=branch, run_id=manual__2024-03-12T17:10:28.193064+00:00, map_index=-1, run_start_date=2024-03-12 17:10:38.116841+00:00, run_end_date=2024-03-12 17:10:38.414342+00:00, run_duration=0.297501, state=success, executor_state=success, try_number=1, max_tries=0, job_id=221, pool=default_pool, queue=default, priority_weight=4, operator=_BranchPythonDecoratedOperator, queued_dttm=2024-03-12 17:10:36.312999+00:00, queued_by_job_id=172, pid=280129
[2024-03-12T22:40:39.402+0530] {scheduler_job_runner.py:424} INFO - 1 tasks up for execution:
        <TaskInstance: taskflow_branching_operators_dag.filter_by_weight manual__2024-03-12T17:10:28.193064+00:00 [scheduled]>
[2024-03-12T22:40:39.403+0530] {scheduler_job_runner.py:487} INFO - DAG taskflow_branching_operators_dag has 0/16 running and queued tasks
[2024-03-12T22:40:39.403+0530] {scheduler_job_runner.py:603} INFO - Setting the following tasks to queued state:
        <TaskInstance: taskflow_branching_operators_dag.filter_by_weight manual__2024-03-12T17:10:28.193064+00:00 [scheduled]>
[2024-03-12T22:40:39.405+0530] {taskinstance.py:2283} WARNING - cannot record scheduled_duration for task filter_by_weight because previous state change time has not been saved
[2024-03-12T22:40:39.406+0530] {scheduler_job_runner.py:646} INFO - Sending TaskInstanceKey(dag_id='taskflow_branching_operators_dag', task_id='filter_by_weight', run_id='manual__2024-03-12T17:10:28.193064+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default
[2024-03-12T22:40:39.407+0530] {base_executor.py:146} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'taskflow_branching_operators_dag', 'filter_by_weight', 'manual__2024-03-12T17:10:28.193064+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_branching_operators_dag.py']
[2024-03-12T22:40:39.429+0530] {sequential_executor.py:73} INFO - Executing command: ['airflow', 'tasks', 'run', 'taskflow_branching_operators_dag', 'filter_by_weight', 'manual__2024-03-12T17:10:28.193064+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_branching_operators_dag.py']
[2024-03-12T22:40:40.503+0530] {dagbag.py:540} INFO - Filling up the DagBag from /home/kai/airflow/dags/taskflow_branching_operators_dag.py
Changing /home/kai/airflow/logs/dag_id=taskflow_branching_operators_dag/run_id=manual__2024-03-12T17:10:28.193064+00:00/task_id=filter_by_weight permission to 509
[2024-03-12T22:40:41.094+0530] {task_command.py:423} INFO - Running <TaskInstance: taskflow_branching_operators_dag.filter_by_weight manual__2024-03-12T17:10:28.193064+00:00 [queued]> on host BreakThrough.
[2024-03-12T22:40:41.958+0530] {scheduler_job_runner.py:696} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='taskflow_branching_operators_dag', task_id='filter_by_weight', run_id='manual__2024-03-12T17:10:28.193064+00:00', try_number=1, map_index=-1)
[2024-03-12T22:40:41.962+0530] {scheduler_job_runner.py:733} INFO - TaskInstance Finished: dag_id=taskflow_branching_operators_dag, task_id=filter_by_weight, run_id=manual__2024-03-12T17:10:28.193064+00:00, map_index=-1, run_start_date=2024-03-12 17:10:41.167398+00:00, run_end_date=2024-03-12 17:10:41.466419+00:00, run_duration=0.299021, state=success, executor_state=success, try_number=1, max_tries=0, job_id=222, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2024-03-12 17:10:39.404652+00:00, queued_by_job_id=172, pid=280139
[2024-03-12T22:40:42.235+0530] {scheduler_job_runner.py:424} INFO - 1 tasks up for execution:
        <TaskInstance: taskflow_branching_operators_dag.write_csv manual__2024-03-12T17:10:28.193064+00:00 [scheduled]>
[2024-03-12T22:40:42.236+0530] {scheduler_job_runner.py:487} INFO - DAG taskflow_branching_operators_dag has 0/16 running and queued tasks
[2024-03-12T22:40:42.236+0530] {scheduler_job_runner.py:603} INFO - Setting the following tasks to queued state:
        <TaskInstance: taskflow_branching_operators_dag.write_csv manual__2024-03-12T17:10:28.193064+00:00 [scheduled]>
[2024-03-12T22:40:42.239+0530] {taskinstance.py:2283} WARNING - cannot record scheduled_duration for task write_csv becauseprevious state change time has not been saved
[2024-03-12T22:40:42.240+0530] {scheduler_job_runner.py:646} INFO - Sending TaskInstanceKey(dag_id='taskflow_branching_operators_dag', task_id='write_csv', run_id='manual__2024-03-12T17:10:28.193064+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default
[2024-03-12T22:40:42.240+0530] {base_executor.py:146} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'taskflow_branching_operators_dag', 'write_csv', 'manual__2024-03-12T17:10:28.193064+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_branching_operators_dag.py']
[2024-03-12T22:40:42.266+0530] {sequential_executor.py:73} INFO - Executing command: ['airflow', 'tasks', 'run', 'taskflow_branching_operators_dag', 'write_csv', 'manual__2024-03-12T17:10:28.193064+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_branching_operators_dag.py']
[2024-03-12T22:40:43.331+0530] {dagbag.py:540} INFO - Filling up the DagBag from /home/kai/airflow/dags/taskflow_branching_operators_dag.py
Changing /home/kai/airflow/logs/dag_id=taskflow_branching_operators_dag/run_id=manual__2024-03-12T17:10:28.193064+00:00/task_id=write_csv permission to 509
[2024-03-12T22:40:43.906+0530] {task_command.py:423} INFO - Running <TaskInstance: taskflow_branching_operators_dag.write_csv manual__2024-03-12T17:10:28.193064+00:00 [queued]> on host BreakThrough.
[2024-03-12T22:40:44.674+0530] {scheduler_job_runner.py:696} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='taskflow_branching_operators_dag', task_id='write_csv', run_id='manual__2024-03-12T17:10:28.193064+00:00', try_number=1, map_index=-1)
[2024-03-12T22:40:44.679+0530] {scheduler_job_runner.py:733} INFO - TaskInstance Finished: dag_id=taskflow_branching_operators_dag, task_id=write_csv, run_id=manual__2024-03-12T17:10:28.193064+00:00, map_index=-1, run_start_date=2024-03-12 17:10:43.967469+00:00, run_end_date=2024-03-12 17:10:44.206864+00:00, run_duration=0.239395, state=success, executor_state=success,try_number=1, max_tries=0, job_id=223, pool=default_pool, queue=default, priority_weight=1, operator=_PythonDecoratedOperator, queued_dttm=2024-03-12 17:10:42.237807+00:00, queued_by_job_id=172, pid=280149
[2024-03-12T22:40:45.367+0530] {dagrun.py:795} INFO - Marking run <DagRun taskflow_branching_operators_dag @ 2024-03-12 17:10:28.193064+00:00: manual__2024-03-12T17:10:28.193064+00:00, state:running, queued_at: 2024-03-12 17:10:28.202501+00:00. externally triggered: True> successful
[2024-03-12T22:40:45.367+0530] {dagrun.py:846} INFO - DagRun Finished: dag_id=taskflow_branching_operators_dag, execution_date=2024-03-12 17:10:28.193064+00:00, run_id=manual__2024-03-12T17:10:28.193064+00:00, run_start_date=2024-03-12 17:10:29.613500+00:00, run_end_date=2024-03-12 17:10:45.367492+00:00, run_duration=15.753992, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-03-11 00:00:00+00:00, data_interval_end=2024-03-12 00:00:00+00:00, dag_hash=a000a4fb1a529224d1c4bc49cab2a792
[2024-03-12T22:43:17.409+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-12T22:48:17.419+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-12T22:53:17.664+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-12T22:57:12.868+0530] {dag.py:3834} INFO - Setting next_dagrun for taskflow_and_operators_interoperating to 2024-03-12 00:00:00+00:00, run_after=2024-03-13 00:00:00+00:00
[2024-03-12T22:57:12.965+0530] {scheduler_job_runner.py:424} INFO - 1 tasks up for execution:
        <TaskInstance: taskflow_and_operators_interoperating.task_a scheduled__2024-03-11T00:00:00+00:00 [scheduled]>
[2024-03-12T22:57:12.965+0530] {scheduler_job_runner.py:487} INFO - DAG taskflow_and_operators_interoperating has 0/16 running and queued tasks
[2024-03-12T22:57:12.965+0530] {scheduler_job_runner.py:603} INFO - Setting the following tasks to queued state:
        <TaskInstance: taskflow_and_operators_interoperating.task_a scheduled__2024-03-11T00:00:00+00:00 [scheduled]>
[2024-03-12T22:57:12.967+0530] {taskinstance.py:2283} WARNING - cannot record scheduled_duration for task task_a because previous state change time has not been saved
[2024-03-12T22:57:12.967+0530] {scheduler_job_runner.py:646} INFO - Sending TaskInstanceKey(dag_id='taskflow_and_operators_interoperating', task_id='task_a', run_id='scheduled__2024-03-11T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default
[2024-03-12T22:57:12.968+0530] {base_executor.py:146} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'taskflow_and_operators_interoperating', 'task_a', 'scheduled__2024-03-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_and_operators_interoperating.py']
[2024-03-12T22:57:12.990+0530] {sequential_executor.py:73} INFO - Executing command: ['airflow', 'tasks', 'run', 'taskflow_and_operators_interoperating', 'task_a', 'scheduled__2024-03-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_and_operators_interoperating.py']
[2024-03-12T22:57:14.107+0530] {dagbag.py:540} INFO - Filling up the DagBag from /home/kai/airflow/dags/taskflow_and_operators_interoperating.py
Changing /home/kai/airflow/logs/dag_id=taskflow_and_operators_interoperating/run_id=scheduled__2024-03-11T00:00:00+00:00/task_id=task_a permission to 509
Changing /home/kai/airflow/logs/dag_id=taskflow_and_operators_interoperating/run_id=scheduled__2024-03-11T00:00:00+00:00 permission to 509
Changing /home/kai/airflow/logs/dag_id=taskflow_and_operators_interoperating permission to 509
[2024-03-12T22:57:14.296+0530] {task_command.py:423} INFO - Running <TaskInstance: taskflow_and_operators_interoperating.task_a scheduled__2024-03-11T00:00:00+00:00 [queued]> on host BreakThrough.
[2024-03-12T22:57:15.066+0530] {scheduler_job_runner.py:696} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='taskflow_and_operators_interoperating', task_id='task_a', run_id='scheduled__2024-03-11T00:00:00+00:00', try_number=1, map_index=-1)
[2024-03-12T22:57:15.071+0530] {scheduler_job_runner.py:733} INFO - TaskInstance Finished: dag_id=taskflow_and_operators_interoperating, task_id=task_a, run_id=scheduled__2024-03-11T00:00:00+00:00, map_index=-1, run_start_date=2024-03-12 17:27:14.365131+00:00, run_end_date=2024-03-12 17:27:14.599285+00:00, run_duration=0.234154, state=success, executor_state=success, try_number=1, max_tries=0, job_id=224, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-03-12 17:27:12.966170+00:00, queued_by_job_id=172, pid=281588
[2024-03-12T22:57:15.133+0530] {scheduler_job_runner.py:424} INFO - 1 tasks up for execution:
        <TaskInstance: taskflow_and_operators_interoperating.task_b scheduled__2024-03-11T00:00:00+00:00 [scheduled]>
[2024-03-12T22:57:15.134+0530] {scheduler_job_runner.py:487} INFO - DAG taskflow_and_operators_interoperating has 0/16 running and queued tasks
[2024-03-12T22:57:15.134+0530] {scheduler_job_runner.py:603} INFO - Setting the following tasks to queued state:
        <TaskInstance: taskflow_and_operators_interoperating.task_b scheduled__2024-03-11T00:00:00+00:00 [scheduled]>
[2024-03-12T22:57:15.135+0530] {taskinstance.py:2283} WARNING - cannot record scheduled_duration for task task_b because previous state change time has not been saved
[2024-03-12T22:57:15.136+0530] {scheduler_job_runner.py:646} INFO - Sending TaskInstanceKey(dag_id='taskflow_and_operators_interoperating', task_id='task_b', run_id='scheduled__2024-03-11T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default
[2024-03-12T22:57:15.136+0530] {base_executor.py:146} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'taskflow_and_operators_interoperating', 'task_b', 'scheduled__2024-03-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_and_operators_interoperating.py']
[2024-03-12T22:57:15.155+0530] {sequential_executor.py:73} INFO - Executing command: ['airflow', 'tasks', 'run', 'taskflow_and_operators_interoperating', 'task_b', 'scheduled__2024-03-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_and_operators_interoperating.py']
[2024-03-12T22:57:16.207+0530] {dagbag.py:540} INFO - Filling up the DagBag from /home/kai/airflow/dags/taskflow_and_operators_interoperating.py
Changing /home/kai/airflow/logs/dag_id=taskflow_and_operators_interoperating/run_id=scheduled__2024-03-11T00:00:00+00:00/task_id=task_b permission to 509
[2024-03-12T22:57:16.418+0530] {task_command.py:423} INFO - Running <TaskInstance: taskflow_and_operators_interoperating.task_b scheduled__2024-03-11T00:00:00+00:00 [queued]> on host BreakThrough.
[2024-03-12T22:57:17.186+0530] {scheduler_job_runner.py:696} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='taskflow_and_operators_interoperating', task_id='task_b', run_id='scheduled__2024-03-11T00:00:00+00:00', try_number=1, map_index=-1)
[2024-03-12T22:57:17.190+0530] {scheduler_job_runner.py:733} INFO - TaskInstance Finished: dag_id=taskflow_and_operators_interoperating, task_id=task_b, run_id=scheduled__2024-03-11T00:00:00+00:00, map_index=-1, run_start_date=2024-03-12 17:27:16.483552+00:00, run_end_date=2024-03-12 17:27:16.749476+00:00, run_duration=0.265924, state=success, executor_state=success, try_number=1, max_tries=0, job_id=225, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2024-03-12 17:27:15.135011+00:00, queued_by_job_id=172, pid=281590
[2024-03-12T22:57:17.800+0530] {scheduler_job_runner.py:424} INFO - 1 tasks up for execution:
        <TaskInstance: taskflow_and_operators_interoperating.task_c scheduled__2024-03-11T00:00:00+00:00 [scheduled]>
[2024-03-12T22:57:17.800+0530] {scheduler_job_runner.py:487} INFO - DAG taskflow_and_operators_interoperating has 0/16 running and queued tasks
[2024-03-12T22:57:17.800+0530] {scheduler_job_runner.py:603} INFO - Setting the following tasks to queued state:
        <TaskInstance: taskflow_and_operators_interoperating.task_c scheduled__2024-03-11T00:00:00+00:00 [scheduled]>
[2024-03-12T22:57:17.802+0530] {taskinstance.py:2283} WARNING - cannot record scheduled_duration for task task_c because previous state change time has not been saved
[2024-03-12T22:57:17.802+0530] {scheduler_job_runner.py:646} INFO - Sending TaskInstanceKey(dag_id='taskflow_and_operators_interoperating', task_id='task_c', run_id='scheduled__2024-03-11T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default
[2024-03-12T22:57:17.802+0530] {base_executor.py:146} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'taskflow_and_operators_interoperating', 'task_c', 'scheduled__2024-03-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_and_operators_interoperating.py']
[2024-03-12T22:57:17.828+0530] {sequential_executor.py:73} INFO - Executing command: ['airflow', 'tasks', 'run', 'taskflow_and_operators_interoperating', 'task_c', 'scheduled__2024-03-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_and_operators_interoperating.py']
[2024-03-12T22:57:18.954+0530] {dagbag.py:540} INFO - Filling up the DagBag from /home/kai/airflow/dags/taskflow_and_operators_interoperating.py
Changing /home/kai/airflow/logs/dag_id=taskflow_and_operators_interoperating/run_id=scheduled__2024-03-11T00:00:00+00:00/task_id=task_c permission to 509
[2024-03-12T22:57:19.157+0530] {task_command.py:423} INFO - Running <TaskInstance: taskflow_and_operators_interoperating.task_c scheduled__2024-03-11T00:00:00+00:00 [queued]> on host BreakThrough.
[2024-03-12T22:57:19.814+0530] {scheduler_job_runner.py:696} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='taskflow_and_operators_interoperating', task_id='task_c', run_id='scheduled__2024-03-11T00:00:00+00:00', try_number=1, map_index=-1)
[2024-03-12T22:57:19.818+0530] {scheduler_job_runner.py:733} INFO - TaskInstance Finished: dag_id=taskflow_and_operators_interoperating, task_id=task_c, run_id=scheduled__2024-03-11T00:00:00+00:00, map_index=-1, run_start_date=2024-03-12 17:27:19.230175+00:00, run_end_date=2024-03-12 17:27:19.416293+00:00, run_duration=0.186118, state=success, executor_state=success, try_number=1, max_tries=0, job_id=226, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-03-12 17:27:17.801455+00:00, queued_by_job_id=172, pid=281602
[2024-03-12T22:57:19.853+0530] {dagrun.py:795} INFO - Marking run <DagRun taskflow_and_operators_interoperating @ 2024-03-11 00:00:00+00:00: scheduled__2024-03-11T00:00:00+00:00, state:running, queued_at: 2024-03-12 17:27:12.860698+00:00. externally triggered: False> successful
[2024-03-12T22:57:19.854+0530] {dagrun.py:846} INFO - DagRun Finished: dag_id=taskflow_and_operators_interoperating, execution_date=2024-03-11 00:00:00+00:00, run_id=scheduled__2024-03-11T00:00:00+00:00, run_start_date=2024-03-12 17:27:12.905473+00:00, run_end_date=2024-03-12 17:27:19.854016+00:00, run_duration=6.948543, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-03-11 00:00:00+00:00, data_interval_end=2024-03-12 00:00:00+00:00, dag_hash=660c3a429e0726c8da77390dc0442516
[2024-03-12T22:57:19.857+0530] {dag.py:3834} INFO - Setting next_dagrun for taskflow_and_operators_interoperating to 2024-03-12 00:00:00+00:00, run_after=2024-03-13 00:00:00+00:00
[2024-03-12T22:58:17.704+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-12T23:03:17.961+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-12T23:08:20.719+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-13 08:06:25 +0530] [253581] [CRITICAL] WORKER TIMEOUT (pid:262549)
[2024-03-13 08:06:25 +0530] [253581] [CRITICAL] WORKER TIMEOUT (pid:262552)
[2024-03-13 08:06:26 +0530] [262552] [INFO] Worker exiting (pid: 262552)
[2024-03-13 08:06:26 +0530] [262549] [INFO] Worker exiting (pid: 262549)
[2024-03-13 08:06:26 +0530] [253581] [ERROR] Worker (pid:262552) exited with code 1
[2024-03-13 08:06:26 +0530] [253581] [ERROR] Worker (pid:262552) exited with code 1.
[2024-03-13 08:06:26 +0530] [253581] [ERROR] Worker (pid:262549) was sent SIGKILL! Perhaps out of memory?
[2024-03-13 08:06:27 +0530] [282539] [INFO] Booting worker with pid: 282539
[2024-03-13 08:06:27 +0530] [282541] [INFO] Booting worker with pid: 282541
[2024-03-13T08:06:28.457+0530] {dag.py:3834} INFO - Setting next_dagrun for taskflow_and_operators_interoperating to 2024-03-13 00:00:00+00:00, run_after=2024-03-14 00:00:00+00:00
[2024-03-13T08:06:28.835+0530] {scheduler_job_runner.py:424} INFO - 1 tasks up for execution:
        <TaskInstance: taskflow_and_operators_interoperating.task_a scheduled__2024-03-12T00:00:00+00:00 [scheduled]>
[2024-03-13T08:06:28.844+0530] {scheduler_job_runner.py:487} INFO - DAG taskflow_and_operators_interoperating has 0/16 running and queued tasks
[2024-03-13T08:06:28.845+0530] {scheduler_job_runner.py:603} INFO - Setting the following tasks to queued state:
        <TaskInstance: taskflow_and_operators_interoperating.task_a scheduled__2024-03-12T00:00:00+00:00 [scheduled]>
[2024-03-13T08:06:28.847+0530] {taskinstance.py:2283} WARNING - cannot record scheduled_duration for task task_a because previous state change time has not been saved
[2024-03-13T08:06:28.849+0530] {scheduler_job_runner.py:646} INFO - Sending TaskInstanceKey(dag_id='taskflow_and_operators_interoperating', task_id='task_a', run_id='scheduled__2024-03-12T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default
[2024-03-13T08:06:28.849+0530] {base_executor.py:146} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'taskflow_and_operators_interoperating', 'task_a', 'scheduled__2024-03-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_and_operators_interoperating.py']
[2024-03-13T08:06:28.916+0530] {sequential_executor.py:73} INFO - Executing command: ['airflow', 'tasks', 'run', 'taskflow_and_operators_interoperating', 'task_a', 'scheduled__2024-03-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_and_operators_interoperating.py']
[2024-03-13T08:06:33.183+0530] {dagbag.py:540} INFO - Filling up the DagBag from /home/kai/airflow/dags/taskflow_and_operators_interoperating.py
Changing /home/kai/airflow/logs/dag_id=taskflow_and_operators_interoperating/run_id=scheduled__2024-03-12T00:00:00+00:00/task_id=task_a permission to 509
Changing /home/kai/airflow/logs/dag_id=taskflow_and_operators_interoperating/run_id=scheduled__2024-03-12T00:00:00+00:00 permission to 509
[2024-03-13T08:06:33.591+0530] {task_command.py:423} INFO - Running <TaskInstance: taskflow_and_operators_interoperating.task_a scheduled__2024-03-12T00:00:00+00:00 [queued]> on host BreakThrough.
[2024-03-13T08:06:34.642+0530] {scheduler_job_runner.py:696} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='taskflow_and_operators_interoperating', task_id='task_a', run_id='scheduled__2024-03-12T00:00:00+00:00', try_number=1, map_index=-1)
[2024-03-13T08:06:34.653+0530] {scheduler_job_runner.py:733} INFO - TaskInstance Finished: dag_id=taskflow_and_operators_interoperating, task_id=task_a, run_id=scheduled__2024-03-12T00:00:00+00:00, map_index=-1, run_start_date=2024-03-13 02:36:33.704424+00:00, run_end_date=2024-03-13 02:36:34.107579+00:00, run_duration=0.403155, state=success, executor_state=success, try_number=1, max_tries=0, job_id=227, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-03-13 02:36:28.846085+00:00, queued_by_job_id=172, pid=282576
[2024-03-13T08:06:35.065+0530] {scheduler_job_runner.py:424} INFO - 1 tasks up for execution:
        <TaskInstance: taskflow_and_operators_interoperating.task_b scheduled__2024-03-12T00:00:00+00:00 [scheduled]>
[2024-03-13T08:06:35.065+0530] {scheduler_job_runner.py:487} INFO - DAG taskflow_and_operators_interoperating has 0/16 running and queued tasks
[2024-03-13T08:06:35.066+0530] {scheduler_job_runner.py:603} INFO - Setting the following tasks to queued state:
        <TaskInstance: taskflow_and_operators_interoperating.task_b scheduled__2024-03-12T00:00:00+00:00 [scheduled]>
[2024-03-13T08:06:35.070+0530] {taskinstance.py:2283} WARNING - cannot record scheduled_duration for task task_b because previous state change time has not been saved
[2024-03-13T08:06:35.071+0530] {scheduler_job_runner.py:646} INFO - Sending TaskInstanceKey(dag_id='taskflow_and_operators_interoperating', task_id='task_b', run_id='scheduled__2024-03-12T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default
[2024-03-13T08:06:35.071+0530] {base_executor.py:146} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'taskflow_and_operators_interoperating', 'task_b', 'scheduled__2024-03-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_and_operators_interoperating.py']
[2024-03-13T08:06:35.090+0530] {sequential_executor.py:73} INFO - Executing command: ['airflow', 'tasks', 'run', 'taskflow_and_operators_interoperating', 'task_b', 'scheduled__2024-03-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_and_operators_interoperating.py']
[2024-03-13T08:06:36.721+0530] {dagbag.py:540} INFO - Filling up the DagBag from /home/kai/airflow/dags/taskflow_and_operators_interoperating.py
Changing /home/kai/airflow/logs/dag_id=taskflow_and_operators_interoperating/run_id=scheduled__2024-03-12T00:00:00+00:00/task_id=task_b permission to 509
[2024-03-13T08:06:37.000+0530] {task_command.py:423} INFO - Running <TaskInstance: taskflow_and_operators_interoperating.task_b scheduled__2024-03-12T00:00:00+00:00 [queued]> on host BreakThrough.
[2024-03-13T08:06:37.950+0530] {scheduler_job_runner.py:696} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='taskflow_and_operators_interoperating', task_id='task_b', run_id='scheduled__2024-03-12T00:00:00+00:00', try_number=1, map_index=-1)
[2024-03-13T08:06:37.954+0530] {scheduler_job_runner.py:733} INFO - TaskInstance Finished: dag_id=taskflow_and_operators_interoperating, task_id=task_b, run_id=scheduled__2024-03-12T00:00:00+00:00, map_index=-1, run_start_date=2024-03-13 02:36:37.090199+00:00, run_end_date=2024-03-13 02:36:37.371556+00:00, run_duration=0.281357, state=success, executor_state=success, try_number=1, max_tries=0, job_id=228, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2024-03-13 02:36:35.069002+00:00, queued_by_job_id=172, pid=282579
[2024-03-13T08:06:38.819+0530] {scheduler_job_runner.py:424} INFO - 1 tasks up for execution:
        <TaskInstance: taskflow_and_operators_interoperating.task_c scheduled__2024-03-12T00:00:00+00:00 [scheduled]>
[2024-03-13T08:06:38.819+0530] {scheduler_job_runner.py:487} INFO - DAG taskflow_and_operators_interoperating has 0/16 running and queued tasks
[2024-03-13T08:06:38.819+0530] {scheduler_job_runner.py:603} INFO - Setting the following tasks to queued state:
        <TaskInstance: taskflow_and_operators_interoperating.task_c scheduled__2024-03-12T00:00:00+00:00 [scheduled]>
[2024-03-13T08:06:38.820+0530] {taskinstance.py:2283} WARNING - cannot record scheduled_duration for task task_c because previous state change time has not been saved
[2024-03-13T08:06:38.821+0530] {scheduler_job_runner.py:646} INFO - Sending TaskInstanceKey(dag_id='taskflow_and_operators_interoperating', task_id='task_c', run_id='scheduled__2024-03-12T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default
[2024-03-13T08:06:38.821+0530] {base_executor.py:146} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'taskflow_and_operators_interoperating', 'task_c', 'scheduled__2024-03-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_and_operators_interoperating.py']
[2024-03-13T08:06:38.840+0530] {sequential_executor.py:73} INFO - Executing command: ['airflow', 'tasks', 'run', 'taskflow_and_operators_interoperating', 'task_c', 'scheduled__2024-03-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_and_operators_interoperating.py']
[2024-03-13T08:06:40.021+0530] {dagbag.py:540} INFO - Filling up the DagBag from /home/kai/airflow/dags/taskflow_and_operators_interoperating.py
Changing /home/kai/airflow/logs/dag_id=taskflow_and_operators_interoperating/run_id=scheduled__2024-03-12T00:00:00+00:00/task_id=task_c permission to 509
[2024-03-13T08:06:40.214+0530] {task_command.py:423} INFO - Running <TaskInstance: taskflow_and_operators_interoperating.task_c scheduled__2024-03-12T00:00:00+00:00 [queued]> on host BreakThrough.
[2024-03-13T08:06:40.900+0530] {scheduler_job_runner.py:696} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='taskflow_and_operators_interoperating', task_id='task_c', run_id='scheduled__2024-03-12T00:00:00+00:00', try_number=1, map_index=-1)
[2024-03-13T08:06:40.904+0530] {scheduler_job_runner.py:733} INFO - TaskInstance Finished: dag_id=taskflow_and_operators_interoperating, task_id=task_c, run_id=scheduled__2024-03-12T00:00:00+00:00, map_index=-1, run_start_date=2024-03-13 02:36:40.285746+00:00, run_end_date=2024-03-13 02:36:40.500608+00:00, run_duration=0.214862, state=success, executor_state=success, try_number=1, max_tries=0, job_id=229, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-03-13 02:36:38.820081+00:00, queued_by_job_id=172, pid=282589
[2024-03-13T08:06:41.168+0530] {dagrun.py:795} INFO - Marking run <DagRun taskflow_and_operators_interoperating @ 2024-03-12 00:00:00+00:00: scheduled__2024-03-12T00:00:00+00:00, state:running, queued_at: 2024-03-13 02:36:28.438368+00:00. externally triggered: False> successful
[2024-03-13T08:06:41.168+0530] {dagrun.py:846} INFO - DagRun Finished: dag_id=taskflow_and_operators_interoperating, execution_date=2024-03-12 00:00:00+00:00, run_id=scheduled__2024-03-12T00:00:00+00:00, run_start_date=2024-03-13 02:36:28.551248+00:00, run_end_date=2024-03-13 02:36:41.168613+00:00, run_duration=12.617365, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-03-12 00:00:00+00:00, data_interval_end=2024-03-13 00:00:00+00:00, dag_hash=c020a48d01cabd2306e9d30de320238d
[2024-03-13T08:06:41.173+0530] {dag.py:3834} INFO - Setting next_dagrun for taskflow_and_operators_interoperating to 2024-03-13 00:00:00+00:00, run_after=2024-03-14 00:00:00+00:00
[2024-03-13T08:10:48.296+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-13T08:15:48.334+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-13T08:20:48.373+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-13T08:25:48.614+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-13T08:30:48.646+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-13T08:35:48.680+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-13T08:40:48.714+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-13T08:45:48.858+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-13 18:48:10 +0530] [253581] [CRITICAL] WORKER TIMEOUT (pid:282539)
[2024-03-13 18:48:10 +0530] [253581] [CRITICAL] WORKER TIMEOUT (pid:282541)
[2024-03-13 18:48:10 +0530] [282539] [INFO] Worker exiting (pid: 282539)
[2024-03-13 18:48:10 +0530] [282541] [INFO] Worker exiting (pid: 282541)
[2024-03-13 18:48:11 +0530] [253581] [ERROR] Worker (pid:282541) exited with code 1
[2024-03-13 18:48:11 +0530] [253581] [ERROR] Worker (pid:282541) exited with code 1.
[2024-03-13 18:48:11 +0530] [253581] [ERROR] Worker (pid:282539) exited with code 1
[2024-03-13 18:48:11 +0530] [253581] [ERROR] Worker (pid:282539) exited with code 1.
[2024-03-13 18:48:12 +0530] [285956] [INFO] Booting worker with pid: 285956
[2024-03-13 18:48:12 +0530] [285957] [INFO] Booting worker with pid: 285957
[2024-03-13T18:48:16.430+0530] {manager.py:523} INFO - DAG catchup_dag is missing and will be deactivated.
[2024-03-13T18:48:16.494+0530] {manager.py:535} INFO - Deactivated 1 DAGs which are no longer present in file.
[2024-03-13T18:48:16.964+0530] {manager.py:539} INFO - Deleted DAG catchup_dag in serialized_dag table
[2024-03-13T18:51:20.199+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-13T18:56:20.952+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-13T19:01:21.104+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-13T19:06:21.135+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-13T19:11:21.687+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-13T19:16:21.728+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-13T19:21:22.201+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-13T19:26:22.237+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-13T19:31:22.269+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-13T19:36:22.308+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-13T19:41:22.338+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-13T19:46:22.477+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-13T19:51:22.740+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-13T19:56:23.021+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-13T19:58:31.794+0530] {dag.py:3834} INFO - Setting next_dagrun for taskflow_sql_operations_dag to 2024-03-13 00:00:00+00:00, run_after=2024-03-14 00:00:00+00:00
[2024-03-13T19:58:31.909+0530] {scheduler_job_runner.py:424} INFO - 1 tasks up for execution:
        <TaskInstance: taskflow_sql_operations_dag.read_task scheduled__2024-03-12T00:00:00+00:00 [scheduled]>
[2024-03-13T19:58:31.910+0530] {scheduler_job_runner.py:487} INFO - DAG taskflow_sql_operations_dag has 0/16 running and queued tasks
[2024-03-13T19:58:31.910+0530] {scheduler_job_runner.py:603} INFO - Setting the following tasks to queued state:
        <TaskInstance: taskflow_sql_operations_dag.read_task scheduled__2024-03-12T00:00:00+00:00 [scheduled]>
[2024-03-13T19:58:31.911+0530] {taskinstance.py:2283} WARNING - cannot record scheduled_duration for task read_task becauseprevious state change time has not been saved
[2024-03-13T19:58:31.914+0530] {scheduler_job_runner.py:646} INFO - Sending TaskInstanceKey(dag_id='taskflow_sql_operations_dag', task_id='read_task', run_id='scheduled__2024-03-12T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default
[2024-03-13T19:58:31.915+0530] {base_executor.py:146} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'taskflow_sql_operations_dag', 'read_task', 'scheduled__2024-03-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_sql_operations.py']
[2024-03-13T19:58:31.938+0530] {sequential_executor.py:73} INFO - Executing command: ['airflow', 'tasks', 'run', 'taskflow_sql_operations_dag', 'read_task', 'scheduled__2024-03-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_sql_operations.py']
[2024-03-13T19:58:33.512+0530] {dagbag.py:540} INFO - Filling up the DagBag from /home/kai/airflow/dags/taskflow_sql_operations.py
Changing /home/kai/airflow/logs/dag_id=taskflow_sql_operations_dag/run_id=scheduled__2024-03-12T00:00:00+00:00/task_id=read_task permission to 509
Changing /home/kai/airflow/logs/dag_id=taskflow_sql_operations_dag/run_id=scheduled__2024-03-12T00:00:00+00:00 permission to 509
Changing /home/kai/airflow/logs/dag_id=taskflow_sql_operations_dag permission to 509
[2024-03-13T19:58:33.764+0530] {task_command.py:423} INFO - Running <TaskInstance: taskflow_sql_operations_dag.read_task scheduled__2024-03-12T00:00:00+00:00 [queued]> on host BreakThrough.
[2024-03-13T19:58:34.474+0530] {scheduler_job_runner.py:696} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='taskflow_sql_operations_dag', task_id='read_task', run_id='scheduled__2024-03-12T00:00:00+00:00', try_number=1, map_index=-1)
[2024-03-13T19:58:34.478+0530] {scheduler_job_runner.py:733} INFO - TaskInstance Finished: dag_id=taskflow_sql_operations_dag, task_id=read_task, run_id=scheduled__2024-03-12T00:00:00+00:00, map_index=-1, run_start_date=2024-03-13 14:28:33.832258+00:00, run_end_date=2024-03-13 14:28:34.046608+00:00, run_duration=0.21435, state=failed, executor_state=success, try_number=1, max_tries=0, job_id=230, pool=default_pool, queue=default, priority_weight=3, operator=_PythonDecoratedOperator, queued_dttm=2024-03-13 14:28:31.910960+00:00, queued_by_job_id=172, pid=291882
[2024-03-13T19:58:36.467+0530] {dagrun.py:774} ERROR - Marking run <DagRun taskflow_sql_operations_dag @ 2024-03-12 00:00:00+00:00: scheduled__2024-03-12T00:00:00+00:00, state:running, queued_at: 2024-03-13 14:28:31.788986+00:00. externally triggered: False> failed
[2024-03-13T19:58:36.467+0530] {dagrun.py:846} INFO - DagRun Finished: dag_id=taskflow_sql_operations_dag, execution_date=2024-03-12 00:00:00+00:00, run_id=scheduled__2024-03-12T00:00:00+00:00, run_start_date=2024-03-13 14:28:31.845683+00:00, run_end_date=2024-03-13 14:28:36.467533+00:00, run_duration=4.62185, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-03-12 00:00:00+00:00, data_interval_end=2024-03-13 00:00:00+00:00, dag_hash=5cb6eebc966087ce9aca99fee72d6ce2
[2024-03-13T19:58:36.470+0530] {dag.py:3834} INFO - Setting next_dagrun for taskflow_sql_operations_dag to 2024-03-13 00:00:00+00:00, run_after=2024-03-14 00:00:00+00:00
[2024-03-13T19:59:00.435+0530] {scheduler_job_runner.py:424} INFO - 1 tasks up for execution:
        <TaskInstance: taskflow_sql_operations_dag.read_task manual__2024-03-13T14:28:58.712500+00:00 [scheduled]>
[2024-03-13T19:59:00.435+0530] {scheduler_job_runner.py:487} INFO - DAG taskflow_sql_operations_dag has 0/16 running and queued tasks
[2024-03-13T19:59:00.436+0530] {scheduler_job_runner.py:603} INFO - Setting the following tasks to queued state:
        <TaskInstance: taskflow_sql_operations_dag.read_task manual__2024-03-13T14:28:58.712500+00:00 [scheduled]>
[2024-03-13T19:59:00.437+0530] {taskinstance.py:2283} WARNING - cannot record scheduled_duration for task read_task becauseprevious state change time has not been saved
[2024-03-13T19:59:00.439+0530] {scheduler_job_runner.py:646} INFO - Sending TaskInstanceKey(dag_id='taskflow_sql_operations_dag', task_id='read_task', run_id='manual__2024-03-13T14:28:58.712500+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default
[2024-03-13T19:59:00.440+0530] {base_executor.py:146} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'taskflow_sql_operations_dag', 'read_task', 'manual__2024-03-13T14:28:58.712500+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_sql_operations.py']
[2024-03-13T19:59:00.463+0530] {sequential_executor.py:73} INFO - Executing command: ['airflow', 'tasks', 'run', 'taskflow_sql_operations_dag', 'read_task', 'manual__2024-03-13T14:28:58.712500+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_sql_operations.py']
[2024-03-13T19:59:01.829+0530] {dagbag.py:540} INFO - Filling up the DagBag from /home/kai/airflow/dags/taskflow_sql_operations.py
Changing /home/kai/airflow/logs/dag_id=taskflow_sql_operations_dag/run_id=manual__2024-03-13T14:28:58.712500+00:00/task_id=read_task permission to 509
Changing /home/kai/airflow/logs/dag_id=taskflow_sql_operations_dag/run_id=manual__2024-03-13T14:28:58.712500+00:00 permission to 509
[2024-03-13T19:59:02.547+0530] {task_command.py:423} INFO - Running <TaskInstance: taskflow_sql_operations_dag.read_task manual__2024-03-13T14:28:58.712500+00:00 [queued]> on host BreakThrough.
[2024-03-13T19:59:03.512+0530] {scheduler_job_runner.py:696} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='taskflow_sql_operations_dag', task_id='read_task', run_id='manual__2024-03-13T14:28:58.712500+00:00', try_number=1, map_index=-1)
[2024-03-13T19:59:03.516+0530] {scheduler_job_runner.py:733} INFO - TaskInstance Finished: dag_id=taskflow_sql_operations_dag, task_id=read_task, run_id=manual__2024-03-13T14:28:58.712500+00:00, map_index=-1, run_start_date=2024-03-13 14:29:02.608318+00:00, run_end_date=2024-03-13 14:29:02.997465+00:00, run_duration=0.389147, state=success, executor_state=success, try_number=1, max_tries=0, job_id=231, pool=default_pool, queue=default, priority_weight=3, operator=_PythonDecoratedOperator, queued_dttm=2024-03-13 14:29:00.436834+00:00, queued_by_job_id=172, pid=291954
[2024-03-13T19:59:03.612+0530] {scheduler_job_runner.py:424} INFO - 1 tasks up for execution:
        <TaskInstance: taskflow_sql_operations_dag.create_table manual__2024-03-13T14:28:58.712500+00:00 [scheduled]>
[2024-03-13T19:59:03.613+0530] {scheduler_job_runner.py:487} INFO - DAG taskflow_sql_operations_dag has 0/16 running and queued tasks
[2024-03-13T19:59:03.613+0530] {scheduler_job_runner.py:603} INFO - Setting the following tasks to queued state:
        <TaskInstance: taskflow_sql_operations_dag.create_table manual__2024-03-13T14:28:58.712500+00:00 [scheduled]>
[2024-03-13T19:59:03.615+0530] {taskinstance.py:2283} WARNING - cannot record scheduled_duration for task create_table because previous state change time has not been saved
[2024-03-13T19:59:03.616+0530] {scheduler_job_runner.py:646} INFO - Sending TaskInstanceKey(dag_id='taskflow_sql_operations_dag', task_id='create_table', run_id='manual__2024-03-13T14:28:58.712500+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default
[2024-03-13T19:59:03.616+0530] {base_executor.py:146} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'taskflow_sql_operations_dag', 'create_table', 'manual__2024-03-13T14:28:58.712500+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_sql_operations.py']
[2024-03-13T19:59:03.636+0530] {sequential_executor.py:73} INFO - Executing command: ['airflow', 'tasks', 'run', 'taskflow_sql_operations_dag', 'create_table', 'manual__2024-03-13T14:28:58.712500+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_sql_operations.py']
[2024-03-13T19:59:04.757+0530] {dagbag.py:540} INFO - Filling up the DagBag from /home/kai/airflow/dags/taskflow_sql_operations.py
Changing /home/kai/airflow/logs/dag_id=taskflow_sql_operations_dag/run_id=manual__2024-03-13T14:28:58.712500+00:00/task_id=create_table permission to 509
[2024-03-13T19:59:05.385+0530] {task_command.py:423} INFO - Running <TaskInstance: taskflow_sql_operations_dag.create_tablemanual__2024-03-13T14:28:58.712500+00:00 [queued]> on host BreakThrough.
[2024-03-13T19:59:06.246+0530] {scheduler_job_runner.py:696} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='taskflow_sql_operations_dag', task_id='create_table', run_id='manual__2024-03-13T14:28:58.712500+00:00', try_number=1, map_index=-1)
[2024-03-13T19:59:06.250+0530] {scheduler_job_runner.py:733} INFO - TaskInstance Finished: dag_id=taskflow_sql_operations_dag, task_id=create_table, run_id=manual__2024-03-13T14:28:58.712500+00:00, map_index=-1, run_start_date=2024-03-13 14:29:05.449645+00:00, run_end_date=2024-03-13 14:29:05.745167+00:00, run_duration=0.295522, state=success, executor_state=success, try_number=1, max_tries=0, job_id=232, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2024-03-13 14:29:03.614498+00:00, queued_by_job_id=172, pid=291963
[2024-03-13T19:59:06.946+0530] {scheduler_job_runner.py:424} INFO - 1 tasks up for execution:
        <TaskInstance: taskflow_sql_operations_dag.insert_records manual__2024-03-13T14:28:58.712500+00:00 [scheduled]>
[2024-03-13T19:59:06.946+0530] {scheduler_job_runner.py:487} INFO - DAG taskflow_sql_operations_dag has 0/16 running and queued tasks
[2024-03-13T19:59:06.946+0530] {scheduler_job_runner.py:603} INFO - Setting the following tasks to queued state:
        <TaskInstance: taskflow_sql_operations_dag.insert_records manual__2024-03-13T14:28:58.712500+00:00 [scheduled]>
[2024-03-13T19:59:06.950+0530] {taskinstance.py:2283} WARNING - cannot record scheduled_duration for task insert_records because previous state change time has not been saved
[2024-03-13T19:59:06.951+0530] {scheduler_job_runner.py:646} INFO - Sending TaskInstanceKey(dag_id='taskflow_sql_operations_dag', task_id='insert_records', run_id='manual__2024-03-13T14:28:58.712500+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default
[2024-03-13T19:59:06.951+0530] {base_executor.py:146} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'taskflow_sql_operations_dag', 'insert_records', 'manual__2024-03-13T14:28:58.712500+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_sql_operations.py']
[2024-03-13T19:59:06.970+0530] {sequential_executor.py:73} INFO - Executing command: ['airflow', 'tasks', 'run', 'taskflow_sql_operations_dag', 'insert_records', 'manual__2024-03-13T14:28:58.712500+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_sql_operations.py']
[2024-03-13T19:59:07.997+0530] {dagbag.py:540} INFO - Filling up the DagBag from /home/kai/airflow/dags/taskflow_sql_operations.py
Changing /home/kai/airflow/logs/dag_id=taskflow_sql_operations_dag/run_id=manual__2024-03-13T14:28:58.712500+00:00/task_id=insert_records permission to 509
[2024-03-13T19:59:08.564+0530] {task_command.py:423} INFO - Running <TaskInstance: taskflow_sql_operations_dag.insert_records manual__2024-03-13T14:28:58.712500+00:00 [queued]> on host BreakThrough.
[2024-03-13T19:59:09.309+0530] {scheduler_job_runner.py:696} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='taskflow_sql_operations_dag', task_id='insert_records', run_id='manual__2024-03-13T14:28:58.712500+00:00', try_number=1, map_index=-1)
[2024-03-13T19:59:09.314+0530] {scheduler_job_runner.py:733} INFO - TaskInstance Finished: dag_id=taskflow_sql_operations_dag, task_id=insert_records, run_id=manual__2024-03-13T14:28:58.712500+00:00, map_index=-1, run_start_date=2024-03-13 14:29:08.622718+00:00, run_end_date=2024-03-13 14:29:08.816765+00:00, run_duration=0.194047, state=failed, executor_state=success, try_number=1, max_tries=0, job_id=233, pool=default_pool, queue=default, priority_weight=1, operator=_PythonDecoratedOperator, queued_dttm=2024-03-13 14:29:06.947298+00:00, queued_by_job_id=172, pid=291980
[2024-03-13T19:59:09.990+0530] {dagrun.py:774} ERROR - Marking run <DagRun taskflow_sql_operations_dag @ 2024-03-13 14:28:58.712500+00:00: manual__2024-03-13T14:28:58.712500+00:00, state:running, queued_at: 2024-03-13 14:28:58.731216+00:00. externally triggered: True> failed
[2024-03-13T19:59:09.990+0530] {dagrun.py:846} INFO - DagRun Finished: dag_id=taskflow_sql_operations_dag, execution_date=2024-03-13 14:28:58.712500+00:00, run_id=manual__2024-03-13T14:28:58.712500+00:00, run_start_date=2024-03-13 14:29:00.377367+00:00, run_end_date=2024-03-13 14:29:09.990877+00:00, run_duration=9.61351, state=failed, external_trigger=True, run_type=manual, data_interval_start=2024-03-12 00:00:00+00:00, data_interval_end=2024-03-13 00:00:00+00:00, dag_hash=5cb6eebc966087ce9aca99fee72d6ce2
[2024-03-13T19:59:35.409+0530] {scheduler_job_runner.py:424} INFO - 1 tasks up for execution:
        <TaskInstance: taskflow_sql_operations_dag.read_task manual__2024-03-13T14:29:34.273446+00:00 [scheduled]>
[2024-03-13T19:59:35.410+0530] {scheduler_job_runner.py:487} INFO - DAG taskflow_sql_operations_dag has 0/16 running and queued tasks
[2024-03-13T19:59:35.411+0530] {scheduler_job_runner.py:603} INFO - Setting the following tasks to queued state:
        <TaskInstance: taskflow_sql_operations_dag.read_task manual__2024-03-13T14:29:34.273446+00:00 [scheduled]>
[2024-03-13T19:59:35.414+0530] {taskinstance.py:2283} WARNING - cannot record scheduled_duration for task read_task becauseprevious state change time has not been saved
[2024-03-13T19:59:35.415+0530] {scheduler_job_runner.py:646} INFO - Sending TaskInstanceKey(dag_id='taskflow_sql_operations_dag', task_id='read_task', run_id='manual__2024-03-13T14:29:34.273446+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default
[2024-03-13T19:59:35.415+0530] {base_executor.py:146} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'taskflow_sql_operations_dag', 'read_task', 'manual__2024-03-13T14:29:34.273446+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_sql_operations.py']
[2024-03-13T19:59:35.441+0530] {sequential_executor.py:73} INFO - Executing command: ['airflow', 'tasks', 'run', 'taskflow_sql_operations_dag', 'read_task', 'manual__2024-03-13T14:29:34.273446+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_sql_operations.py']
[2024-03-13T19:59:36.980+0530] {dagbag.py:540} INFO - Filling up the DagBag from /home/kai/airflow/dags/taskflow_sql_operations.py
Changing /home/kai/airflow/logs/dag_id=taskflow_sql_operations_dag/run_id=manual__2024-03-13T14:29:34.273446+00:00/task_id=read_task permission to 509
Changing /home/kai/airflow/logs/dag_id=taskflow_sql_operations_dag/run_id=manual__2024-03-13T14:29:34.273446+00:00 permission to 509
[2024-03-13T19:59:37.695+0530] {task_command.py:423} INFO - Running <TaskInstance: taskflow_sql_operations_dag.read_task manual__2024-03-13T14:29:34.273446+00:00 [queued]> on host BreakThrough.
[2024-03-13T19:59:38.774+0530] {scheduler_job_runner.py:696} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='taskflow_sql_operations_dag', task_id='read_task', run_id='manual__2024-03-13T14:29:34.273446+00:00', try_number=1, map_index=-1)
[2024-03-13T19:59:38.778+0530] {scheduler_job_runner.py:733} INFO - TaskInstance Finished: dag_id=taskflow_sql_operations_dag, task_id=read_task, run_id=manual__2024-03-13T14:29:34.273446+00:00, map_index=-1, run_start_date=2024-03-13 14:29:37.766648+00:00, run_end_date=2024-03-13 14:29:38.132440+00:00, run_duration=0.365792, state=success, executor_state=success, try_number=1, max_tries=0, job_id=234, pool=default_pool, queue=default, priority_weight=3, operator=_PythonDecoratedOperator, queued_dttm=2024-03-13 14:29:35.412051+00:00, queued_by_job_id=172, pid=292050
[2024-03-13T19:59:40.012+0530] {scheduler_job_runner.py:424} INFO - 1 tasks up for execution:
        <TaskInstance: taskflow_sql_operations_dag.create_table manual__2024-03-13T14:29:34.273446+00:00 [scheduled]>
[2024-03-13T19:59:40.012+0530] {scheduler_job_runner.py:487} INFO - DAG taskflow_sql_operations_dag has 0/16 running and queued tasks
[2024-03-13T19:59:40.012+0530] {scheduler_job_runner.py:603} INFO - Setting the following tasks to queued state:
        <TaskInstance: taskflow_sql_operations_dag.create_table manual__2024-03-13T14:29:34.273446+00:00 [scheduled]>
[2024-03-13T19:59:40.014+0530] {taskinstance.py:2283} WARNING - cannot record scheduled_duration for task create_table because previous state change time has not been saved
[2024-03-13T19:59:40.014+0530] {scheduler_job_runner.py:646} INFO - Sending TaskInstanceKey(dag_id='taskflow_sql_operations_dag', task_id='create_table', run_id='manual__2024-03-13T14:29:34.273446+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default
[2024-03-13T19:59:40.015+0530] {base_executor.py:146} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'taskflow_sql_operations_dag', 'create_table', 'manual__2024-03-13T14:29:34.273446+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_sql_operations.py']
[2024-03-13T19:59:40.040+0530] {sequential_executor.py:73} INFO - Executing command: ['airflow', 'tasks', 'run', 'taskflow_sql_operations_dag', 'create_table', 'manual__2024-03-13T14:29:34.273446+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_sql_operations.py']
[2024-03-13T19:59:41.131+0530] {dagbag.py:540} INFO - Filling up the DagBag from /home/kai/airflow/dags/taskflow_sql_operations.py
Changing /home/kai/airflow/logs/dag_id=taskflow_sql_operations_dag/run_id=manual__2024-03-13T14:29:34.273446+00:00/task_id=create_table permission to 509
[2024-03-13T19:59:41.772+0530] {task_command.py:423} INFO - Running <TaskInstance: taskflow_sql_operations_dag.create_tablemanual__2024-03-13T14:29:34.273446+00:00 [queued]> on host BreakThrough.
[2024-03-13T19:59:42.855+0530] {scheduler_job_runner.py:696} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='taskflow_sql_operations_dag', task_id='create_table', run_id='manual__2024-03-13T14:29:34.273446+00:00', try_number=1, map_index=-1)
[2024-03-13T19:59:42.859+0530] {scheduler_job_runner.py:733} INFO - TaskInstance Finished: dag_id=taskflow_sql_operations_dag, task_id=create_table, run_id=manual__2024-03-13T14:29:34.273446+00:00, map_index=-1, run_start_date=2024-03-13 14:29:41.884044+00:00, run_end_date=2024-03-13 14:29:42.279799+00:00, run_duration=0.395755, state=success, executor_state=success, try_number=1, max_tries=0, job_id=235, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2024-03-13 14:29:40.013421+00:00, queued_by_job_id=172, pid=292067
[2024-03-13T19:59:43.564+0530] {scheduler_job_runner.py:424} INFO - 1 tasks up for execution:
        <TaskInstance: taskflow_sql_operations_dag.insert_records manual__2024-03-13T14:29:34.273446+00:00 [scheduled]>
[2024-03-13T19:59:43.564+0530] {scheduler_job_runner.py:487} INFO - DAG taskflow_sql_operations_dag has 0/16 running and queued tasks
[2024-03-13T19:59:43.565+0530] {scheduler_job_runner.py:603} INFO - Setting the following tasks to queued state:
        <TaskInstance: taskflow_sql_operations_dag.insert_records manual__2024-03-13T14:29:34.273446+00:00 [scheduled]>
[2024-03-13T19:59:43.566+0530] {taskinstance.py:2283} WARNING - cannot record scheduled_duration for task insert_records because previous state change time has not been saved
[2024-03-13T19:59:43.567+0530] {scheduler_job_runner.py:646} INFO - Sending TaskInstanceKey(dag_id='taskflow_sql_operations_dag', task_id='insert_records', run_id='manual__2024-03-13T14:29:34.273446+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default
[2024-03-13T19:59:43.567+0530] {base_executor.py:146} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'taskflow_sql_operations_dag', 'insert_records', 'manual__2024-03-13T14:29:34.273446+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_sql_operations.py']
[2024-03-13T19:59:43.592+0530] {sequential_executor.py:73} INFO - Executing command: ['airflow', 'tasks', 'run', 'taskflow_sql_operations_dag', 'insert_records', 'manual__2024-03-13T14:29:34.273446+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_sql_operations.py']
[2024-03-13T19:59:44.602+0530] {dagbag.py:540} INFO - Filling up the DagBag from /home/kai/airflow/dags/taskflow_sql_operations.py
Changing /home/kai/airflow/logs/dag_id=taskflow_sql_operations_dag/run_id=manual__2024-03-13T14:29:34.273446+00:00/task_id=insert_records permission to 509
[2024-03-13T19:59:45.171+0530] {task_command.py:423} INFO - Running <TaskInstance: taskflow_sql_operations_dag.insert_records manual__2024-03-13T14:29:34.273446+00:00 [queued]> on host BreakThrough.
[2024-03-13T19:59:46.344+0530] {scheduler_job_runner.py:696} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='taskflow_sql_operations_dag', task_id='insert_records', run_id='manual__2024-03-13T14:29:34.273446+00:00', try_number=1, map_index=-1)
[2024-03-13T19:59:46.348+0530] {scheduler_job_runner.py:733} INFO - TaskInstance Finished: dag_id=taskflow_sql_operations_dag, task_id=insert_records, run_id=manual__2024-03-13T14:29:34.273446+00:00, map_index=-1, run_start_date=2024-03-13 14:29:45.238185+00:00, run_end_date=2024-03-13 14:29:45.817284+00:00, run_duration=0.579099, state=failed, executor_state=success, try_number=1, max_tries=0, job_id=236, pool=default_pool, queue=default, priority_weight=1, operator=_PythonDecoratedOperator, queued_dttm=2024-03-13 14:29:43.565870+00:00, queued_by_job_id=172, pid=292084
[2024-03-13T19:59:46.619+0530] {dagrun.py:774} ERROR - Marking run <DagRun taskflow_sql_operations_dag @ 2024-03-13 14:29:34.273446+00:00: manual__2024-03-13T14:29:34.273446+00:00, state:running, queued_at: 2024-03-13 14:29:34.282639+00:00. externally triggered: True> failed
[2024-03-13T19:59:46.619+0530] {dagrun.py:846} INFO - DagRun Finished: dag_id=taskflow_sql_operations_dag, execution_date=2024-03-13 14:29:34.273446+00:00, run_id=manual__2024-03-13T14:29:34.273446+00:00, run_start_date=2024-03-13 14:29:35.279477+00:00, run_end_date=2024-03-13 14:29:46.619461+00:00, run_duration=11.339984, state=failed, external_trigger=True, run_type=manual, data_interval_start=2024-03-12 00:00:00+00:00, data_interval_end=2024-03-13 00:00:00+00:00, dag_hash=5cb6eebc966087ce9aca99fee72d6ce2
[2024-03-13T20:00:42.752+0530] {scheduler_job_runner.py:424} INFO - 1 tasks up for execution:
        <TaskInstance: taskflow_sql_operations_dag.read_task manual__2024-03-13T14:30:40.410737+00:00 [scheduled]>
[2024-03-13T20:00:42.753+0530] {scheduler_job_runner.py:487} INFO - DAG taskflow_sql_operations_dag has 0/16 running and queued tasks
[2024-03-13T20:00:42.755+0530] {scheduler_job_runner.py:603} INFO - Setting the following tasks to queued state:
        <TaskInstance: taskflow_sql_operations_dag.read_task manual__2024-03-13T14:30:40.410737+00:00 [scheduled]>
[2024-03-13T20:00:42.761+0530] {taskinstance.py:2283} WARNING - cannot record scheduled_duration for task read_task becauseprevious state change time has not been saved
[2024-03-13T20:00:42.765+0530] {scheduler_job_runner.py:646} INFO - Sending TaskInstanceKey(dag_id='taskflow_sql_operations_dag', task_id='read_task', run_id='manual__2024-03-13T14:30:40.410737+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default
[2024-03-13T20:00:42.766+0530] {base_executor.py:146} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'taskflow_sql_operations_dag', 'read_task', 'manual__2024-03-13T14:30:40.410737+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_sql_operations.py']
[2024-03-13T20:00:42.799+0530] {sequential_executor.py:73} INFO - Executing command: ['airflow', 'tasks', 'run', 'taskflow_sql_operations_dag', 'read_task', 'manual__2024-03-13T14:30:40.410737+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_sql_operations.py']
[2024-03-13T20:00:44.048+0530] {dagbag.py:540} INFO - Filling up the DagBag from /home/kai/airflow/dags/taskflow_sql_operations.py
Changing /home/kai/airflow/logs/dag_id=taskflow_sql_operations_dag/run_id=manual__2024-03-13T14:30:40.410737+00:00/task_id=read_task permission to 509
Changing /home/kai/airflow/logs/dag_id=taskflow_sql_operations_dag/run_id=manual__2024-03-13T14:30:40.410737+00:00 permission to 509
[2024-03-13T20:00:44.756+0530] {task_command.py:423} INFO - Running <TaskInstance: taskflow_sql_operations_dag.read_task manual__2024-03-13T14:30:40.410737+00:00 [queued]> on host BreakThrough.
[2024-03-13T20:00:45.683+0530] {scheduler_job_runner.py:696} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='taskflow_sql_operations_dag', task_id='read_task', run_id='manual__2024-03-13T14:30:40.410737+00:00', try_number=1, map_index=-1)
[2024-03-13T20:00:45.687+0530] {scheduler_job_runner.py:733} INFO - TaskInstance Finished: dag_id=taskflow_sql_operations_dag, task_id=read_task, run_id=manual__2024-03-13T14:30:40.410737+00:00, map_index=-1, run_start_date=2024-03-13 14:30:44.818293+00:00, run_end_date=2024-03-13 14:30:45.108855+00:00, run_duration=0.290562, state=success, executor_state=success, try_number=1, max_tries=0, job_id=237, pool=default_pool, queue=default, priority_weight=3, operator=_PythonDecoratedOperator, queued_dttm=2024-03-13 14:30:42.758123+00:00, queued_by_job_id=172, pid=292210
[2024-03-13T20:00:46.391+0530] {scheduler_job_runner.py:424} INFO - 1 tasks up for execution:
        <TaskInstance: taskflow_sql_operations_dag.create_table manual__2024-03-13T14:30:40.410737+00:00 [scheduled]>
[2024-03-13T20:00:46.392+0530] {scheduler_job_runner.py:487} INFO - DAG taskflow_sql_operations_dag has 0/16 running and queued tasks
[2024-03-13T20:00:46.392+0530] {scheduler_job_runner.py:603} INFO - Setting the following tasks to queued state:
        <TaskInstance: taskflow_sql_operations_dag.create_table manual__2024-03-13T14:30:40.410737+00:00 [scheduled]>
[2024-03-13T20:00:46.393+0530] {taskinstance.py:2283} WARNING - cannot record scheduled_duration for task create_table because previous state change time has not been saved
[2024-03-13T20:00:46.394+0530] {scheduler_job_runner.py:646} INFO - Sending TaskInstanceKey(dag_id='taskflow_sql_operations_dag', task_id='create_table', run_id='manual__2024-03-13T14:30:40.410737+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default
[2024-03-13T20:00:46.394+0530] {base_executor.py:146} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'taskflow_sql_operations_dag', 'create_table', 'manual__2024-03-13T14:30:40.410737+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_sql_operations.py']
[2024-03-13T20:00:46.417+0530] {sequential_executor.py:73} INFO - Executing command: ['airflow', 'tasks', 'run', 'taskflow_sql_operations_dag', 'create_table', 'manual__2024-03-13T14:30:40.410737+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_sql_operations.py']
[2024-03-13T20:00:47.666+0530] {dagbag.py:540} INFO - Filling up the DagBag from /home/kai/airflow/dags/taskflow_sql_operations.py
Changing /home/kai/airflow/logs/dag_id=taskflow_sql_operations_dag/run_id=manual__2024-03-13T14:30:40.410737+00:00/task_id=create_table permission to 509
[2024-03-13T20:00:48.268+0530] {task_command.py:423} INFO - Running <TaskInstance: taskflow_sql_operations_dag.create_tablemanual__2024-03-13T14:30:40.410737+00:00 [queued]> on host BreakThrough.
[2024-03-13T20:00:49.064+0530] {scheduler_job_runner.py:696} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='taskflow_sql_operations_dag', task_id='create_table', run_id='manual__2024-03-13T14:30:40.410737+00:00', try_number=1, map_index=-1)
[2024-03-13T20:00:49.069+0530] {scheduler_job_runner.py:733} INFO - TaskInstance Finished: dag_id=taskflow_sql_operations_dag, task_id=create_table, run_id=manual__2024-03-13T14:30:40.410737+00:00, map_index=-1, run_start_date=2024-03-13 14:30:48.327345+00:00, run_end_date=2024-03-13 14:30:48.551257+00:00, run_duration=0.223912, state=success, executor_state=success, try_number=1, max_tries=0, job_id=238, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2024-03-13 14:30:46.392893+00:00, queued_by_job_id=172, pid=292227
[2024-03-13T20:00:49.339+0530] {scheduler_job_runner.py:424} INFO - 1 tasks up for execution:
        <TaskInstance: taskflow_sql_operations_dag.insert_records manual__2024-03-13T14:30:40.410737+00:00 [scheduled]>
[2024-03-13T20:00:49.339+0530] {scheduler_job_runner.py:487} INFO - DAG taskflow_sql_operations_dag has 0/16 running and queued tasks
[2024-03-13T20:00:49.340+0530] {scheduler_job_runner.py:603} INFO - Setting the following tasks to queued state:
        <TaskInstance: taskflow_sql_operations_dag.insert_records manual__2024-03-13T14:30:40.410737+00:00 [scheduled]>
[2024-03-13T20:00:49.341+0530] {taskinstance.py:2283} WARNING - cannot record scheduled_duration for task insert_records because previous state change time has not been saved
[2024-03-13T20:00:49.341+0530] {scheduler_job_runner.py:646} INFO - Sending TaskInstanceKey(dag_id='taskflow_sql_operations_dag', task_id='insert_records', run_id='manual__2024-03-13T14:30:40.410737+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default
[2024-03-13T20:00:49.342+0530] {base_executor.py:146} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'taskflow_sql_operations_dag', 'insert_records', 'manual__2024-03-13T14:30:40.410737+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_sql_operations.py']
[2024-03-13T20:00:49.368+0530] {sequential_executor.py:73} INFO - Executing command: ['airflow', 'tasks', 'run', 'taskflow_sql_operations_dag', 'insert_records', 'manual__2024-03-13T14:30:40.410737+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_sql_operations.py']
[2024-03-13T20:00:50.413+0530] {dagbag.py:540} INFO - Filling up the DagBag from /home/kai/airflow/dags/taskflow_sql_operations.py
Changing /home/kai/airflow/logs/dag_id=taskflow_sql_operations_dag/run_id=manual__2024-03-13T14:30:40.410737+00:00/task_id=insert_records permission to 509
[2024-03-13T20:00:51.105+0530] {task_command.py:423} INFO - Running <TaskInstance: taskflow_sql_operations_dag.insert_records manual__2024-03-13T14:30:40.410737+00:00 [queued]> on host BreakThrough.
[2024-03-13T20:00:52.986+0530] {scheduler_job_runner.py:696} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='taskflow_sql_operations_dag', task_id='insert_records', run_id='manual__2024-03-13T14:30:40.410737+00:00', try_number=1, map_index=-1)
[2024-03-13T20:00:52.993+0530] {scheduler_job_runner.py:733} INFO - TaskInstance Finished: dag_id=taskflow_sql_operations_dag, task_id=insert_records, run_id=manual__2024-03-13T14:30:40.410737+00:00, map_index=-1, run_start_date=2024-03-13 14:30:51.173843+00:00, run_end_date=2024-03-13 14:30:52.451713+00:00, run_duration=1.27787, state=success, executor_state=success, try_number=1, max_tries=0, job_id=239, pool=default_pool, queue=default, priority_weight=1, operator=_PythonDecoratedOperator, queued_dttm=2024-03-13 14:30:49.340560+00:00, queued_by_job_id=172, pid=292237
[2024-03-13T20:00:53.361+0530] {dagrun.py:795} INFO - Marking run <DagRun taskflow_sql_operations_dag @ 2024-03-13 14:30:40.410737+00:00: manual__2024-03-13T14:30:40.410737+00:00, state:running, queued_at: 2024-03-13 14:30:40.426907+00:00. externally triggered: True> successful
[2024-03-13T20:00:53.362+0530] {dagrun.py:846} INFO - DagRun Finished: dag_id=taskflow_sql_operations_dag, execution_date=2024-03-13 14:30:40.410737+00:00, run_id=manual__2024-03-13T14:30:40.410737+00:00, run_start_date=2024-03-13 14:30:42.664559+00:00, run_end_date=2024-03-13 14:30:53.362078+00:00, run_duration=10.697519, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-03-12 00:00:00+00:00, data_interval_end=2024-03-13 00:00:00+00:00, dag_hash=5cb6eebc966087ce9aca99fee72d6ce2
[2024-03-13T20:01:23.055+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-13T20:06:23.087+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-13T20:11:23.329+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-13T20:16:23.470+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-13T20:21:24.059+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-13T20:24:58.343+0530] {scheduler_job_runner.py:424} INFO - 3 tasks up for execution:
        <TaskInstance: taskflow_sql_operations_dag.read_task manual__2024-03-13T14:54:57.959540+00:00 [scheduled]>
        <TaskInstance: taskflow_sql_operations_dag.insert_records manual__2024-03-13T14:54:57.959540+00:00 [scheduled]>
        <TaskInstance: taskflow_sql_operations_dag.insert_dummy_records manual__2024-03-13T14:54:57.959540+00:00 [scheduled]>
[2024-03-13T20:24:58.344+0530] {scheduler_job_runner.py:487} INFO - DAG taskflow_sql_operations_dag has 0/16 running and queued tasks
[2024-03-13T20:24:58.346+0530] {scheduler_job_runner.py:487} INFO - DAG taskflow_sql_operations_dag has 1/16 running and queued tasks
[2024-03-13T20:24:58.347+0530] {scheduler_job_runner.py:487} INFO - DAG taskflow_sql_operations_dag has 2/16 running and queued tasks
[2024-03-13T20:24:58.347+0530] {scheduler_job_runner.py:603} INFO - Setting the following tasks to queued state:
        <TaskInstance: taskflow_sql_operations_dag.read_task manual__2024-03-13T14:54:57.959540+00:00 [scheduled]>
        <TaskInstance: taskflow_sql_operations_dag.insert_records manual__2024-03-13T14:54:57.959540+00:00 [scheduled]>
        <TaskInstance: taskflow_sql_operations_dag.insert_dummy_records manual__2024-03-13T14:54:57.959540+00:00 [scheduled]>
[2024-03-13T20:24:58.412+0530] {taskinstance.py:2283} WARNING - cannot record scheduled_duration for task read_task becauseprevious state change time has not been saved
[2024-03-13T20:24:58.413+0530] {taskinstance.py:2283} WARNING - cannot record scheduled_duration for task insert_records because previous state change time has not been saved
[2024-03-13T20:24:58.414+0530] {taskinstance.py:2283} WARNING - cannot record scheduled_duration for task insert_dummy_records because previous state change time has not been saved
[2024-03-13T20:24:58.415+0530] {scheduler_job_runner.py:646} INFO - Sending TaskInstanceKey(dag_id='taskflow_sql_operations_dag', task_id='read_task', run_id='manual__2024-03-13T14:54:57.959540+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default
[2024-03-13T20:24:58.416+0530] {base_executor.py:146} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'taskflow_sql_operations_dag', 'read_task', 'manual__2024-03-13T14:54:57.959540+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_sql_operations.py']
[2024-03-13T20:24:58.416+0530] {scheduler_job_runner.py:646} INFO - Sending TaskInstanceKey(dag_id='taskflow_sql_operations_dag', task_id='insert_records', run_id='manual__2024-03-13T14:54:57.959540+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default
[2024-03-13T20:24:58.417+0530] {base_executor.py:146} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'taskflow_sql_operations_dag', 'insert_records', 'manual__2024-03-13T14:54:57.959540+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_sql_operations.py']
[2024-03-13T20:24:58.418+0530] {scheduler_job_runner.py:646} INFO - Sending TaskInstanceKey(dag_id='taskflow_sql_operations_dag', task_id='insert_dummy_records', run_id='manual__2024-03-13T14:54:57.959540+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default
[2024-03-13T20:24:58.419+0530] {base_executor.py:146} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'taskflow_sql_operations_dag', 'insert_dummy_records', 'manual__2024-03-13T14:54:57.959540+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_sql_operations.py']
[2024-03-13T20:24:58.443+0530] {sequential_executor.py:73} INFO - Executing command: ['airflow', 'tasks', 'run', 'taskflow_sql_operations_dag', 'read_task', 'manual__2024-03-13T14:54:57.959540+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_sql_operations.py']
[2024-03-13T20:25:00.183+0530] {dagbag.py:540} INFO - Filling up the DagBag from /home/kai/airflow/dags/taskflow_sql_operations.py
Changing /home/kai/airflow/logs/dag_id=taskflow_sql_operations_dag/run_id=manual__2024-03-13T14:54:57.959540+00:00/task_id=read_task permission to 509
Changing /home/kai/airflow/logs/dag_id=taskflow_sql_operations_dag/run_id=manual__2024-03-13T14:54:57.959540+00:00 permission to 509
[2024-03-13T20:25:00.812+0530] {task_command.py:423} INFO - Running <TaskInstance: taskflow_sql_operations_dag.read_task manual__2024-03-13T14:54:57.959540+00:00 [queued]> on host BreakThrough.
[2024-03-13T20:25:02.005+0530] {sequential_executor.py:73} INFO - Executing command: ['airflow', 'tasks', 'run', 'taskflow_sql_operations_dag', 'insert_records', 'manual__2024-03-13T14:54:57.959540+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_sql_operations.py']
[2024-03-13T20:25:03.249+0530] {dagbag.py:540} INFO - Filling up the DagBag from /home/kai/airflow/dags/taskflow_sql_operations.py
Changing /home/kai/airflow/logs/dag_id=taskflow_sql_operations_dag/run_id=manual__2024-03-13T14:54:57.959540+00:00/task_id=insert_records permission to 509
[2024-03-13T20:25:03.961+0530] {task_command.py:423} INFO - Running <TaskInstance: taskflow_sql_operations_dag.insert_records manual__2024-03-13T14:54:57.959540+00:00 [queued]> on host BreakThrough.
[2024-03-13T20:25:05.970+0530] {sequential_executor.py:73} INFO - Executing command: ['airflow', 'tasks', 'run', 'taskflow_sql_operations_dag', 'insert_dummy_records', 'manual__2024-03-13T14:54:57.959540+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_sql_operations.py']
[2024-03-13T20:25:06.938+0530] {dagbag.py:540} INFO - Filling up the DagBag from /home/kai/airflow/dags/taskflow_sql_operations.py
Changing /home/kai/airflow/logs/dag_id=taskflow_sql_operations_dag/run_id=manual__2024-03-13T14:54:57.959540+00:00/task_id=insert_dummy_records permission to 509
[2024-03-13T20:25:07.497+0530] {task_command.py:423} INFO - Running <TaskInstance: taskflow_sql_operations_dag.insert_dummy_records manual__2024-03-13T14:54:57.959540+00:00 [queued]> on host BreakThrough.
[2024-03-13T20:25:08.276+0530] {scheduler_job_runner.py:696} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='taskflow_sql_operations_dag', task_id='read_task', run_id='manual__2024-03-13T14:54:57.959540+00:00', try_number=1, map_index=-1)
[2024-03-13T20:25:08.276+0530] {scheduler_job_runner.py:696} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='taskflow_sql_operations_dag', task_id='insert_records', run_id='manual__2024-03-13T14:54:57.959540+00:00', try_number=1, map_index=-1)
[2024-03-13T20:25:08.276+0530] {scheduler_job_runner.py:696} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='taskflow_sql_operations_dag', task_id='insert_dummy_records', run_id='manual__2024-03-13T14:54:57.959540+00:00', try_number=1, map_index=-1)
[2024-03-13T20:25:08.280+0530] {scheduler_job_runner.py:733} INFO - TaskInstance Finished: dag_id=taskflow_sql_operations_dag, task_id=insert_dummy_records, run_id=manual__2024-03-13T14:54:57.959540+00:00, map_index=-1, run_start_date=2024-03-13 14:55:07.559760+00:00, run_end_date=2024-03-13 14:55:07.816844+00:00, run_duration=0.257084, state=success, executor_state=success, try_number=1, max_tries=0, job_id=242, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2024-03-13 14:54:58.350348+00:00, queued_by_job_id=172, pid=294638
[2024-03-13T20:25:08.281+0530] {scheduler_job_runner.py:733} INFO - TaskInstance Finished: dag_id=taskflow_sql_operations_dag, task_id=insert_records, run_id=manual__2024-03-13T14:54:57.959540+00:00, map_index=-1, run_start_date=2024-03-13 14:55:04.084203+00:00, run_end_date=2024-03-13 14:55:05.464304+00:00, run_duration=1.380101, state=success, executor_state=success,try_number=1, max_tries=0, job_id=241, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2024-03-13 14:54:58.350348+00:00, queued_by_job_id=172, pid=294629
[2024-03-13T20:25:08.281+0530] {scheduler_job_runner.py:733} INFO - TaskInstance Finished: dag_id=taskflow_sql_operations_dag, task_id=read_task, run_id=manual__2024-03-13T14:54:57.959540+00:00, map_index=-1, run_start_date=2024-03-13 14:55:00.887888+00:00, run_end_date=2024-03-13 14:55:01.445667+00:00, run_duration=0.557779, state=success, executor_state=success, try_number=1, max_tries=0, job_id=240, pool=default_pool, queue=default, priority_weight=3, operator=_PythonDecoratedOperator, queued_dttm=2024-03-13 14:54:58.350348+00:00, queued_by_job_id=172, pid=294620
[2024-03-13T20:25:08.878+0530] {scheduler_job_runner.py:424} INFO - 1 tasks up for execution:
        <TaskInstance: taskflow_sql_operations_dag.create_table manual__2024-03-13T14:54:57.959540+00:00 [scheduled]>
[2024-03-13T20:25:08.878+0530] {scheduler_job_runner.py:487} INFO - DAG taskflow_sql_operations_dag has 0/16 running and queued tasks
[2024-03-13T20:25:08.879+0530] {scheduler_job_runner.py:603} INFO - Setting the following tasks to queued state:
        <TaskInstance: taskflow_sql_operations_dag.create_table manual__2024-03-13T14:54:57.959540+00:00 [scheduled]>
[2024-03-13T20:25:08.880+0530] {taskinstance.py:2283} WARNING - cannot record scheduled_duration for task create_table because previous state change time has not been saved
[2024-03-13T20:25:08.880+0530] {scheduler_job_runner.py:646} INFO - Sending TaskInstanceKey(dag_id='taskflow_sql_operations_dag', task_id='create_table', run_id='manual__2024-03-13T14:54:57.959540+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default
[2024-03-13T20:25:08.880+0530] {base_executor.py:146} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'taskflow_sql_operations_dag', 'create_table', 'manual__2024-03-13T14:54:57.959540+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_sql_operations.py']
[2024-03-13T20:25:08.905+0530] {sequential_executor.py:73} INFO - Executing command: ['airflow', 'tasks', 'run', 'taskflow_sql_operations_dag', 'create_table', 'manual__2024-03-13T14:54:57.959540+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_sql_operations.py']
[2024-03-13T20:25:09.909+0530] {dagbag.py:540} INFO - Filling up the DagBag from /home/kai/airflow/dags/taskflow_sql_operations.py
Changing /home/kai/airflow/logs/dag_id=taskflow_sql_operations_dag/run_id=manual__2024-03-13T14:54:57.959540+00:00/task_id=create_table permission to 509
[2024-03-13T20:25:10.465+0530] {task_command.py:423} INFO - Running <TaskInstance: taskflow_sql_operations_dag.create_tablemanual__2024-03-13T14:54:57.959540+00:00 [queued]> on host BreakThrough.
[2024-03-13T20:25:11.295+0530] {scheduler_job_runner.py:696} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='taskflow_sql_operations_dag', task_id='create_table', run_id='manual__2024-03-13T14:54:57.959540+00:00', try_number=1, map_index=-1)
[2024-03-13T20:25:11.299+0530] {scheduler_job_runner.py:733} INFO - TaskInstance Finished: dag_id=taskflow_sql_operations_dag, task_id=create_table, run_id=manual__2024-03-13T14:54:57.959540+00:00, map_index=-1, run_start_date=2024-03-13 14:55:10.540801+00:00, run_end_date=2024-03-13 14:55:10.789056+00:00, run_duration=0.248255, state=success, executor_state=success, try_number=1, max_tries=0, job_id=243, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2024-03-13 14:55:08.879492+00:00, queued_by_job_id=172, pid=294655
[2024-03-13T20:25:11.461+0530] {scheduler_job_runner.py:424} INFO - 1 tasks up for execution:
        <TaskInstance: taskflow_sql_operations_dag.join manual__2024-03-13T14:54:57.959540+00:00 [scheduled]>
[2024-03-13T20:25:11.461+0530] {scheduler_job_runner.py:487} INFO - DAG taskflow_sql_operations_dag has 0/16 running and queued tasks
[2024-03-13T20:25:11.461+0530] {scheduler_job_runner.py:603} INFO - Setting the following tasks to queued state:
        <TaskInstance: taskflow_sql_operations_dag.join manual__2024-03-13T14:54:57.959540+00:00 [scheduled]>
[2024-03-13T20:25:11.462+0530] {taskinstance.py:2283} WARNING - cannot record scheduled_duration for task join because previous state change time has not been saved
[2024-03-13T20:25:11.463+0530] {scheduler_job_runner.py:646} INFO - Sending TaskInstanceKey(dag_id='taskflow_sql_operations_dag', task_id='join', run_id='manual__2024-03-13T14:54:57.959540+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default
[2024-03-13T20:25:11.463+0530] {base_executor.py:146} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'taskflow_sql_operations_dag', 'join', 'manual__2024-03-13T14:54:57.959540+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_sql_operations.py']
[2024-03-13T20:25:11.490+0530] {sequential_executor.py:73} INFO - Executing command: ['airflow', 'tasks', 'run', 'taskflow_sql_operations_dag', 'join', 'manual__2024-03-13T14:54:57.959540+00:00', '--local', '--subdir', 'DAGS_FOLDER/taskflow_sql_operations.py']
[2024-03-13T20:25:12.592+0530] {dagbag.py:540} INFO - Filling up the DagBag from /home/kai/airflow/dags/taskflow_sql_operations.py
Changing /home/kai/airflow/logs/dag_id=taskflow_sql_operations_dag/run_id=manual__2024-03-13T14:54:57.959540+00:00/task_id=join permission to 509
[2024-03-13T20:25:13.166+0530] {task_command.py:423} INFO - Running <TaskInstance: taskflow_sql_operations_dag.join manual__2024-03-13T14:54:57.959540+00:00 [queued]> on host BreakThrough.
[2024-03-13T20:25:13.980+0530] {scheduler_job_runner.py:696} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='taskflow_sql_operations_dag', task_id='join', run_id='manual__2024-03-13T14:54:57.959540+00:00', try_number=1, map_index=-1)
[2024-03-13T20:25:13.984+0530] {scheduler_job_runner.py:733} INFO - TaskInstance Finished: dag_id=taskflow_sql_operations_dag, task_id=join, run_id=manual__2024-03-13T14:54:57.959540+00:00, map_index=-1, run_start_date=2024-03-13 14:55:13.231080+00:00, run_end_date=2024-03-13 14:55:13.503014+00:00, run_duration=0.271934, state=success, executor_state=success, try_number=1, max_tries=0, job_id=244, pool=default_pool, queue=default, priority_weight=1, operator=_PythonDecoratedOperator, queued_dttm=2024-03-13 14:55:11.462201+00:00, queued_by_job_id=172, pid=294665
[2024-03-13T20:25:14.143+0530] {dagrun.py:795} INFO - Marking run <DagRun taskflow_sql_operations_dag @ 2024-03-13 14:54:57.959540+00:00: manual__2024-03-13T14:54:57.959540+00:00, state:running, queued_at: 2024-03-13 14:54:57.977584+00:00. externally triggered: True> successful
[2024-03-13T20:25:14.143+0530] {dagrun.py:846} INFO - DagRun Finished: dag_id=taskflow_sql_operations_dag, execution_date=2024-03-13 14:54:57.959540+00:00, run_id=manual__2024-03-13T14:54:57.959540+00:00, run_start_date=2024-03-13 14:54:58.265164+00:00, run_end_date=2024-03-13 14:55:14.143567+00:00, run_duration=15.878403, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-03-12 00:00:00+00:00, data_interval_end=2024-03-13 00:00:00+00:00, dag_hash=624011e580e9a4c7e6f8529a7e366a03
[2024-03-13T20:26:24.580+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-13T20:31:24.587+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-13T20:36:24.618+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
^C[2024-03-13T20:37:59.548+0530] {scheduler_job_runner.py:258} INFO - Exiting gracefully upon receiving signal 2
[2024-03-13 20:37:59 +0530] [253581] [INFO] Handling signal: int
[2024-03-13 20:37:59 +0530] [285957] [INFO] Worker exiting (pid: 285957)
[2024-03-13 20:37:59 +0530] [285956] [INFO] Worker exiting (pid: 285956)
[2024-03-13 20:37:59 +0530] [253581] [INFO] Shutting down: Master
^C[2024-03-13T20:37:59.938+0530] {scheduler_job_runner.py:258} INFO - Exiting gracefully upon receiving signal 2
^C[2024-03-13T20:38:00.511+0530] {scheduler_job_runner.py:258} INFO - Exiting gracefully upon receiving signal 2
^C[2024-03-13T20:38:00.740+0530] {scheduler_job_runner.py:258} INFO - Exiting gracefully upon receiving signal 2
^C[2024-03-13T20:38:00.941+0530] {scheduler_job_runner.py:258} INFO - Exiting gracefully upon receiving signal 2
^C[2024-03-13T20:38:01.100+0530] {scheduler_job_runner.py:258} INFO - Exiting gracefully upon receiving signal 2
^C[2024-03-13T20:38:01.255+0530] {scheduler_job_runner.py:258} INFO - Exiting gracefully upon receiving signal 2
[2024-03-13T20:38:02.269+0530] {process_utils.py:131} INFO - Sending Signals.SIGTERM to group 253583. PIDs of all processesin the group: [253583]
[2024-03-13T20:38:02.270+0530] {process_utils.py:86} INFO - Sending the signal Signals.SIGTERM to group 253583
[2024-03-13T20:38:02.486+0530] {process_utils.py:79} INFO - Process psutil.Process(pid=253583, status='terminated', exitcode=0, started='11:01:06') (253583) terminated with exit code 0
[2024-03-13T20:38:02.489+0530] {process_utils.py:131} INFO - Sending Signals.SIGTERM to group 253583. PIDs of all processesin the group: []
[2024-03-13T20:38:02.489+0530] {process_utils.py:86} INFO - Sending the signal Signals.SIGTERM to group 253583
[2024-03-13T20:38:02.489+0530] {process_utils.py:100} INFO - Sending the signal Signals.SIGTERM to process 253583 as process group is missing.
[2024-03-13T20:38:02.490+0530] {scheduler_job_runner.py:884} INFO - Exited execute loop
kai@BreakThrough:~$ sudo apt install postgresql
[sudo] password for kai:
Reading package lists... Done
Building dependency tree... Done
Reading state information... Done
The following additional packages will be installed:
  libcommon-sense-perl libjson-perl libjson-xs-perl libllvm14 libpq5 libsensors-config libsensors5
  libtypes-serialiser-perl libxslt1.1 postgresql-14 postgresql-client-14 postgresql-client-common postgresql-common
  ssl-cert sysstat
Suggested packages:
  lm-sensors postgresql-doc postgresql-doc-14 isag
The following NEW packages will be installed:
  libcommon-sense-perl libjson-perl libjson-xs-perl libllvm14 libpq5 libsensors-config libsensors5
  libtypes-serialiser-perl libxslt1.1 postgresql postgresql-14 postgresql-client-14 postgresql-client-common
  postgresql-common ssl-cert sysstat
0 upgraded, 16 newly installed, 0 to remove and 0 not upgraded.
Need to get 42.6 MB of archives.
After this operation, 162 MB of additional disk space will be used.
Do you want to continue? [Y/n] y
Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libcommon-sense-perl amd64 3.75-2build1 [21.1 kB]
Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libjson-perl all 4.04000-1 [81.8 kB]
Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libtypes-serialiser-perl all 1.01-1 [11.6 kB]
Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 libjson-xs-perl amd64 4.030-1build3 [87.2 kB]
Get:5 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libllvm14 amd64 1:14.0.0-1ubuntu1.1 [24.0 MB]
Get:6 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpq5 amd64 14.11-0ubuntu0.22.04.1 [144 kB]
Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 libsensors-config all 1:3.6.0-7ubuntu1 [5274 B]
Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libsensors5 amd64 1:3.6.0-7ubuntu1 [26.3 kB]
Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libxslt1.1 amd64 1.1.34-4ubuntu0.22.04.1 [164 kB]
Get:10 http://archive.ubuntu.com/ubuntu jammy/main amd64 postgresql-client-common all 238 [29.6 kB]
Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 postgresql-client-14 amd64 14.11-0ubuntu0.22.04.1 [1222 kB]
Get:12 http://archive.ubuntu.com/ubuntu jammy/main amd64 ssl-cert all 1.1.2 [17.4 kB]
Get:13 http://archive.ubuntu.com/ubuntu jammy/main amd64 postgresql-common all 238 [169 kB]
Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 postgresql-14 amd64 14.11-0ubuntu0.22.04.1 [16.2 MB]
Get:15 http://archive.ubuntu.com/ubuntu jammy/main amd64 postgresql all 14+238 [3288 B]
Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 sysstat amd64 12.5.2-2ubuntu0.2 [487 kB]
Fetched 42.6 MB in 9s (4697 kB/s)
Preconfiguring packages ...
Selecting previously unselected package libcommon-sense-perl:amd64.
(Reading database ... 31422 files and directories currently installed.)
Preparing to unpack .../00-libcommon-sense-perl_3.75-2build1_amd64.deb ...
Unpacking libcommon-sense-perl:amd64 (3.75-2build1) ...
Selecting previously unselected package libjson-perl.
Preparing to unpack .../01-libjson-perl_4.04000-1_all.deb ...
Unpacking libjson-perl (4.04000-1) ...
Selecting previously unselected package libtypes-serialiser-perl.
Preparing to unpack .../02-libtypes-serialiser-perl_1.01-1_all.deb ...
Unpacking libtypes-serialiser-perl (1.01-1) ...
Selecting previously unselected package libjson-xs-perl.
Preparing to unpack .../03-libjson-xs-perl_4.030-1build3_amd64.deb ...
Unpacking libjson-xs-perl (4.030-1build3) ...
Selecting previously unselected package libllvm14:amd64.
Preparing to unpack .../04-libllvm14_1%3a14.0.0-1ubuntu1.1_amd64.deb ...
Unpacking libllvm14:amd64 (1:14.0.0-1ubuntu1.1) ...
Selecting previously unselected package libpq5:amd64.
Preparing to unpack .../05-libpq5_14.11-0ubuntu0.22.04.1_amd64.deb ...
Unpacking libpq5:amd64 (14.11-0ubuntu0.22.04.1) ...
Selecting previously unselected package libsensors-config.
Preparing to unpack .../06-libsensors-config_1%3a3.6.0-7ubuntu1_all.deb ...
Unpacking libsensors-config (1:3.6.0-7ubuntu1) ...
Selecting previously unselected package libsensors5:amd64.
Preparing to unpack .../07-libsensors5_1%3a3.6.0-7ubuntu1_amd64.deb ...
Unpacking libsensors5:amd64 (1:3.6.0-7ubuntu1) ...
Selecting previously unselected package libxslt1.1:amd64.
Preparing to unpack .../08-libxslt1.1_1.1.34-4ubuntu0.22.04.1_amd64.deb ...
Unpacking libxslt1.1:amd64 (1.1.34-4ubuntu0.22.04.1) ...
Selecting previously unselected package postgresql-client-common.
Preparing to unpack .../09-postgresql-client-common_238_all.deb ...
Unpacking postgresql-client-common (238) ...
Selecting previously unselected package postgresql-client-14.
Preparing to unpack .../10-postgresql-client-14_14.11-0ubuntu0.22.04.1_amd64.deb ...
Unpacking postgresql-client-14 (14.11-0ubuntu0.22.04.1) ...
Selecting previously unselected package ssl-cert.
Preparing to unpack .../11-ssl-cert_1.1.2_all.deb ...
Unpacking ssl-cert (1.1.2) ...
Selecting previously unselected package postgresql-common.
Preparing to unpack .../12-postgresql-common_238_all.deb ...
Adding 'diversion of /usr/bin/pg_config to /usr/bin/pg_config.libpq-dev by postgresql-common'
Unpacking postgresql-common (238) ...
Selecting previously unselected package postgresql-14.
Preparing to unpack .../13-postgresql-14_14.11-0ubuntu0.22.04.1_amd64.deb ...
Unpacking postgresql-14 (14.11-0ubuntu0.22.04.1) ...
Selecting previously unselected package postgresql.
Preparing to unpack .../14-postgresql_14+238_all.deb ...
Unpacking postgresql (14+238) ...
Selecting previously unselected package sysstat.
Preparing to unpack .../15-sysstat_12.5.2-2ubuntu0.2_amd64.deb ...
Unpacking sysstat (12.5.2-2ubuntu0.2) ...
Setting up postgresql-client-common (238) ...
Setting up libsensors-config (1:3.6.0-7ubuntu1) ...
Setting up libpq5:amd64 (14.11-0ubuntu0.22.04.1) ...
Setting up libcommon-sense-perl:amd64 (3.75-2build1) ...
Setting up postgresql-client-14 (14.11-0ubuntu0.22.04.1) ...
update-alternatives: using /usr/share/postgresql/14/man/man1/psql.1.gz to provide /usr/share/man/man1/psql.1.gz (psql.1.gz)in auto mode
Setting up ssl-cert (1.1.2) ...
Setting up libsensors5:amd64 (1:3.6.0-7ubuntu1) ...
Setting up libllvm14:amd64 (1:14.0.0-1ubuntu1.1) ...
Setting up libtypes-serialiser-perl (1.01-1) ...
Setting up libjson-perl (4.04000-1) ...
Setting up libxslt1.1:amd64 (1.1.34-4ubuntu0.22.04.1) ...
Setting up sysstat (12.5.2-2ubuntu0.2) ...

Creating config file /etc/default/sysstat with new version
update-alternatives: using /usr/bin/sar.sysstat to provide /usr/bin/sar (sar) in auto mode
Created symlink /etc/systemd/system/sysstat.service.wants/sysstat-collect.timer  /lib/systemd/system/sysstat-collect.timer.
Created symlink /etc/systemd/system/sysstat.service.wants/sysstat-summary.timer  /lib/systemd/system/sysstat-summary.timer.
Created symlink /etc/systemd/system/multi-user.target.wants/sysstat.service  /lib/systemd/system/sysstat.service.
Setting up libjson-xs-perl (4.030-1build3) ...
Setting up postgresql-common (238) ...
Adding user postgres to group ssl-cert

Creating config file /etc/postgresql-common/createcluster.conf with new version
Building PostgreSQL dictionaries from installed myspell/hunspell packages...
Removing obsolete dictionary files:
Created symlink /etc/systemd/system/multi-user.target.wants/postgresql.service  /lib/systemd/system/postgresql.service.
Setting up postgresql-14 (14.11-0ubuntu0.22.04.1) ...
Creating new PostgreSQL cluster 14/main ...
/usr/lib/postgresql/14/bin/initdb -D /var/lib/postgresql/14/main --auth-local peer --auth-host scram-sha-256 --no-instructions
The files belonging to this database system will be owned by user "postgres".
This user must also own the server process.

The database cluster will be initialized with locale "C.UTF-8".
The default database encoding has accordingly been set to "UTF8".
The default text search configuration will be set to "english".

Data page checksums are disabled.

fixing permissions on existing directory /var/lib/postgresql/14/main ... ok
creating subdirectories ... ok
selecting dynamic shared memory implementation ... posix
selecting default max_connections ... 100
selecting default shared_buffers ... 128MB
selecting default time zone ... Asia/Kolkata
creating configuration files ... ok
running bootstrap script ... ok
performing post-bootstrap initialization ... ok
syncing data to disk ... ok
update-alternatives: using /usr/share/postgresql/14/man/man1/postmaster.1.gz to provide /usr/share/man/man1/postmaster.1.gz(postmaster.1.gz) in auto mode
Setting up postgresql (14+238) ...
Processing triggers for man-db (2.10.2-1) ...
Processing triggers for libc-bin (2.35-0ubuntu3.6) ...
kai@BreakThrough:~$ sudo service postgresql status
 postgresql.service - PostgreSQL RDBMS
     Loaded: loaded (/lib/systemd/system/postgresql.service; enabled; vendor preset: enabled)
     Active: active (exited) since Wed 2024-03-13 20:38:57 IST; 12s ago
    Process: 297883 ExecStart=/bin/true (code=exited, status=0/SUCCESS)
   Main PID: 297883 (code=exited, status=0/SUCCESS)

Mar 13 20:38:57 BreakThrough systemd[1]: Starting PostgreSQL RDBMS...
Mar 13 20:38:57 BreakThrough systemd[1]: Finished PostgreSQL RDBMS.
kai@BreakThrough:~$ sudo service postgresql start
kai@BreakThrough:~$ sudo service postgresql status
 postgresql.service - PostgreSQL RDBMS
     Loaded: loaded (/lib/systemd/system/postgresql.service; enabled; vendor preset: enabled)
     Active: active (exited) since Wed 2024-03-13 20:38:57 IST; 25s ago
    Process: 297883 ExecStart=/bin/true (code=exited, status=0/SUCCESS)
   Main PID: 297883 (code=exited, status=0/SUCCESS)

Mar 13 20:38:57 BreakThrough systemd[1]: Starting PostgreSQL RDBMS...
Mar 13 20:38:57 BreakThrough systemd[1]: Finished PostgreSQL RDBMS.
kai@BreakThrough:~$ cd airflow/
kai@BreakThrough:~/airflow$ ls
README.md  airflow-webserver.pid  airflow.cfg  airflow.db  dags  database  history3.txt  logs  webserver_config.py
kai@BreakThrough:~/airflow$ history >> history3.txt
kai@BreakThrough:~/airflow$ sudo su - postgres
Welcome to Ubuntu 22.04.4 LTS (GNU/Linux 5.15.133.1-microsoft-standard-WSL2 x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/pro

 * Strictly confined Kubernetes makes edge and IoT secure. Learn how MicroK8s
   just raised the bar for easy, resilient and secure K8s cluster deployment.

   https://ubuntu.com/engage/secure-kubernetes-at-the-edge

This message is shown once a day. To disable it please create the
/var/lib/postgresql/.hushlogin file.
postgres@BreakThrough:~$ psql
psql (14.11 (Ubuntu 14.11-0ubuntu0.22.04.1))
Type "help" for help.

postgres=# \du
                                   List of roles
 Role name |                         Attributes                         | Member of
-----------+------------------------------------------------------------+-----------
 postgres  | Superuser, Create role, Create DB, Replication, Bypass RLS | {}

postgres=# create user kai with password 'kai' superuser;
CREATE ROLE
postgres=# \du
                                   List of roles
 Role name |                         Attributes                         | Member of
-----------+------------------------------------------------------------+-----------
 kai       | Superuser                                                  | {}
 postgres  | Superuser, Create role, Create DB, Replication, Bypass RLS | {}

postgres=# exit
postgres@BreakThrough:~$ exit
logout
kai@BreakThrough:~/airflow$ history
    1  clear
    2  sudo apt update
    3  sudo apt upgrade
    4  sudo apt install python3-pip
    5  clear
    6  py --version
    7  python --version
    8  python3 --version
    9  pip install 'apache-airflow==2.8.2' --constraint "https://raw.githubusercontent.com/apache/airflow/constraints-2.8.2/constraints-3.10.txt"
   10  airflow version
   11  export AIRFLOW_HOME=/home/kai/.local/bin
   12  airflow version
   13  cd airflow/
   14  history >> history.txt
   15  airflow -h
   16  airflow cheat-sheet
   17  airflow info
   18  ls
   19  cd airflow
   20  ls
   21  airflow config get-value webserver base-url
   22  airflow config get-value webserver base_url
   23  clear
   24  airflow db init
   25  ls -l
   26  history > command_history.txt
   27  airflow users -h
   28  airflow users list
   29  airflow users create -e kuregaikai@gmail.com-f kai-l kuregami-p kai-r Admin-u kai.user
   30  airflow users create -e kuregaikai@gmail.com -f kai -l kuregami -p kai -r Admin -u kai.user
   31  airflow users list
   32  airflow scheduler
   33  history >> history3.txt
   34  airflow scheduler
   35  airflow webserver
   36  pip install virtualenv
   37  airflow webserver
   38  history >> history2.txt
   39  airflow webserver
   40  cd airflow/
   41  ls
   42  cd dags
   43  ls
   44  cd ..
   45  pip install pandas
   46  mkdir database
   47  ls
   48  cd database/
   49  sqlite3 sqlite_db.db
   50  sqlite sqlite_db.db
   51  sudo apt install sqlite3
   52  sqlite3 sqlite_db.db
   53  touch sqlite_db.db
   54  sqlite3 sqlite_db.db
   55  clear
   56  cd ..
   57  git add .
   58  git init
   59  git add .
   60  git commit -m "first commit"
   61  airflow scheduler
   62  sudo apt install postgresql
   63  sudo service postgresql status
   64  sudo service postgresql start
   65  sudo service postgresql status
   66  cd airflow/
   67  ls
   68  history >> history3.txt
   69  sudo su - postgres
   70  history
kai@BreakThrough:~/airflow$ airflow scheduler
  ____________       _____________
 ____    |__( )_________  __/__  /________      __
____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
 _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
[2024-03-13T20:45:58.552+0530] {task_context_logger.py:63} INFO - Task context logging is enabled
[2024-03-13T20:45:58.552+0530] {executor_loader.py:115} INFO - Loaded executor: SequentialExecutor
[2024-03-13 20:45:58 +0530] [298037] [INFO] Starting gunicorn 21.2.0
[2024-03-13 20:45:58 +0530] [298037] [INFO] Listening at: http://[::]:8793 (298037)
[2024-03-13 20:45:58 +0530] [298037] [INFO] Using worker: sync
[2024-03-13 20:45:58 +0530] [298038] [INFO] Booting worker with pid: 298038
[2024-03-13T20:45:58.628+0530] {scheduler_job_runner.py:808} INFO - Starting the scheduler
[2024-03-13T20:45:58.629+0530] {scheduler_job_runner.py:815} INFO - Processing each file at most -1 times
[2024-03-13T20:45:58.636+0530] {manager.py:169} INFO - Launched DagFileProcessorManager with pid: 298039
[2024-03-13T20:45:58.639+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-13T20:45:58.642+0530] {settings.py:60} INFO - Configured default timezone UTC
[2024-03-13 20:45:58 +0530] [298040] [INFO] Booting worker with pid: 298040
[2024-03-13T20:45:58.714+0530] {manager.py:392} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
^C[2024-03-13 20:46:31 +0530] [298037] [INFO] Handling signal: int
[2024-03-13T20:46:31.347+0530] {scheduler_job_runner.py:258} INFO - Exiting gracefully upon receiving signal 2
[2024-03-13 20:46:31 +0530] [298038] [INFO] Worker exiting (pid: 298038)
[2024-03-13 20:46:31 +0530] [298040] [INFO] Worker exiting (pid: 298040)
^C[2024-03-13T20:46:31.534+0530] {scheduler_job_runner.py:258} INFO - Exiting gracefully upon receiving signal 2
[2024-03-13 20:46:31 +0530] [298037] [INFO] Shutting down: Master
^C[2024-03-13T20:46:31.705+0530] {scheduler_job_runner.py:258} INFO - Exiting gracefully upon receiving signal 2
^C[2024-03-13T20:46:31.853+0530] {scheduler_job_runner.py:258} INFO - Exiting gracefully upon receiving signal 2
[2024-03-13T20:46:32.861+0530] {process_utils.py:131} INFO - Sending Signals.SIGTERM to group 298039. PIDs of all processesin the group: [298039]
[2024-03-13T20:46:32.861+0530] {process_utils.py:86} INFO - Sending the signal Signals.SIGTERM to group 298039
[2024-03-13T20:46:32.955+0530] {process_utils.py:79} INFO - Process psutil.Process(pid=298039, status='terminated', exitcode=0, started='20:45:58') (298039) terminated with exit code 0
[2024-03-13T20:46:32.957+0530] {process_utils.py:131} INFO - Sending Signals.SIGTERM to group 298039. PIDs of all processesin the group: []
[2024-03-13T20:46:32.958+0530] {process_utils.py:86} INFO - Sending the signal Signals.SIGTERM to group 298039
[2024-03-13T20:46:32.958+0530] {process_utils.py:100} INFO - Sending the signal Signals.SIGTERM to process 298039 as process group is missing.
[2024-03-13T20:46:32.958+0530] {scheduler_job_runner.py:884} INFO - Exited execute loop
kai@BreakThrough:~/airflow$ pip install apache-airflow-providers-postgres
Defaulting to user installation because normal site-packages is not writeable
Collecting apache-airflow-providers-postgres
  Downloading apache_airflow_providers_postgres-5.10.2-py3-none-any.whl (20 kB)
Collecting psycopg2-binary>=2.8.0
  Downloading psycopg2_binary-2.9.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)
      3.0/3.0 MB 8.3 MB/s eta 0:00:00
Requirement already satisfied: apache-airflow>=2.6.0 in /home/kai/.local/lib/python3.10/site-packages (from apache-airflow-providers-postgres) (2.8.2)
Requirement already satisfied: apache-airflow-providers-common-sql>=1.3.1 in /home/kai/.local/lib/python3.10/site-packages (from apache-airflow-providers-postgres) (1.11.0)
Requirement already satisfied: connexion[flask]<3.0,>=2.10.0 in /home/kai/.local/lib/python3.10/site-packages (from apache-airflow>=2.6.0->apache-airflow-providers-postgres) (2.14.2)
Requirement already satisfied: markdown-it-py>=2.1.0 in /home/kai/.local/lib/python3.10/site-packages (from apache-airflow>=2.6.0->apache-airflow-providers-postgres) (3.0.0)
Requirement already satisfied: cryptography>=0.9.3 in /home/kai/.local/lib/python3.10/site-packages (from apache-airflow>=2.6.0->apache-airflow-providers-postgres) (41.0.7)
Requirement already satisfied: gunicorn>=20.1.0 in /home/kai/.local/lib/python3.10/site-packages (from apache-airflow>=2.6.0->apache-airflow-providers-postgres) (21.2.0)
Requirement already satisfied: requests<3,>=2.27.0 in /home/kai/.local/lib/python3.10/site-packages (from apache-airflow>=2.6.0->apache-airflow-providers-postgres) (2.31.0)
Requirement already satisfied: unicodecsv>=0.14.1 in /home/kai/.local/lib/python3.10/site-packages (from apache-airflow>=2.6.0->apache-airflow-providers-postgres) (0.14.1)
Requirement already satisfied: python-daemon>=3.0.0 in /home/kai/.local/lib/python3.10/site-packages (from apache-airflow>=2.6.0->apache-airflow-providers-postgres) (3.0.1)
Requirement already satisfied: werkzeug<3,>=2.0 in /home/kai/.local/lib/python3.10/site-packages (from apache-airflow>=2.6.0->apache-airflow-providers-postgres) (2.2.3)
Requirement already satisfied: apache-airflow-providers-ftp in /home/kai/.local/lib/python3.10/site-packages (from apache-airflow>=2.6.0->apache-airflow-providers-postgres) (3.7.0)
Requirement already satisfied: httpx in /home/kai/.local/lib/python3.10/site-packages (from apache-airflow>=2.6.0->apache-airflow-providers-postgres) (0.23.3)
Requirement already satisfied: setproctitle>=1.1.8 in /home/kai/.local/lib/python3.10/site-packages (from apache-airflow>=2.6.0->apache-airflow-providers-postgres) (1.3.3)
Requirement already satisfied: rich>=12.4.4 in /home/kai/.local/lib/python3.10/site-packages (from apache-airflow>=2.6.0->apache-airflow-providers-postgres) (13.7.0)
Requirement already satisfied: sqlalchemy-jsonfield>=1.0 in /home/kai/.local/lib/python3.10/site-packages (from apache-airflow>=2.6.0->apache-airflow-providers-postgres) (1.0.2)
Requirement already satisfied: configupdater>=3.1.1 in /home/kai/.local/lib/python3.10/site-packages (from apache-airflow>=2.6.0->apache-airflow-providers-postgres) (3.2)
Requirement already satisfied: apache-airflow-providers-common-io in /home/kai/.local/lib/python3.10/site-packages (from apache-airflow>=2.6.0->apache-airflow-providers-postgres) (1.3.0)
Requirement already satisfied: sqlalchemy<2.0,>=1.4.28 in /home/kai/.local/lib/python3.10/site-packages (from apache-airflow>=2.6.0->apache-airflow-providers-postgres) (1.4.51)
Requirement already satisfied: pygments>=2.0.1 in /home/kai/.local/lib/python3.10/site-packages (from apache-airflow>=2.6.0->apache-airflow-providers-postgres) (2.17.2)
Requirement already satisfied: flask-session<0.6,>=0.4.0 in /home/kai/.local/lib/python3.10/site-packages (from apache-airflow>=2.6.0->apache-airflow-providers-postgres) (0.5.0)
Requirement already satisfied: rich-argparse>=1.0.0 in /home/kai/.local/lib/python3.10/site-packages (from apache-airflow>=2.6.0->apache-airflow-providers-postgres) (1.4.0)
Requirement already satisfied: fsspec>=2023.10.0 in /home/kai/.local/lib/python3.10/site-packages (from apache-airflow>=2.6.0->apache-airflow-providers-postgres) (2024.2.0)
Requirement already satisfied: rfc3339-validator>=0.1.4 in /home/kai/.local/lib/python3.10/site-packages (from apache-airflow>=2.6.0->apache-airflow-providers-postgres) (0.1.4)
Requirement already satisfied: tenacity!=8.2.0,>=6.2.0 in /home/kai/.local/lib/python3.10/site-packages (from apache-airflow>=2.6.0->apache-airflow-providers-postgres) (8.2.3)
Requirement already satisfied: dill>=0.2.2 in /home/kai/.local/lib/python3.10/site-packages (from apache-airflow>=2.6.0->apache-airflow-providers-postgres) (0.3.1.1)
Requirement already satisfied: python-dateutil>=2.3 in /home/kai/.local/lib/python3.10/site-packages (from apache-airflow>=2.6.0->apache-airflow-providers-postgres) (2.8.2)
Requirement already satisfied: pathspec>=0.9.0 in /home/kai/.local/lib/python3.10/site-packages (from apache-airflow>=2.6.0->apache-airflow-providers-postgres) (0.12.1)
Requirement already satisfied: markupsafe>=1.1.1 in /home/kai/.local/lib/python3.10/site-packages (from apache-airflow>=2.6.0->apache-airflow-providers-postgres) (2.1.5)
Requirement already satisfied: flask<2.3,>=2.2 in /home/kai/.local/lib/python3.10/site-packages (from apache-airflow>=2.6.0->apache-airflow-providers-postgres) (2.2.5)
Requirement already satisfied: opentelemetry-api>=1.15.0 in /home/kai/.local/lib/python3.10/site-packages (from apache-airflow>=2.6.0->apache-airflow-providers-postgres) (1.23.0)
Requirement already satisfied: blinker in /home/kai/.local/lib/python3.10/site-packages (from apache-airflow>=2.6.0->apache-airflow-providers-postgres) (1.7.0)
Requirement already satisfied: itsdangerous>=2.0 in /home/kai/.local/lib/python3.10/site-packages (from apache-airflow>=2.6.0->apache-airflow-providers-postgres) (2.1.2)
Requirement already satisfied: jsonschema>=4.18.0 in /home/kai/.local/lib/python3.10/site-packages (from apache-airflow>=2.6.0->apache-airflow-providers-postgres) (4.21.1)
Requirement already satisfied: pendulum<4.0,>=2.1.2 in /home/kai/.local/lib/python3.10/site-packages (from apache-airflow>=2.6.0->apache-airflow-providers-postgres) (3.0.0)
Requirement already satisfied: colorlog<5.0,>=4.0.2 in /home/kai/.local/lib/python3.10/site-packages (from apache-airflow>=2.6.0->apache-airflow-providers-postgres) (4.8.0)
Requirement already satisfied: lazy-object-proxy in /home/kai/.local/lib/python3.10/site-packages (from apache-airflow>=2.6.0->apache-airflow-providers-postgres) (1.10.0)
Requirement already satisfied: flask-appbuilder==4.3.11 in /home/kai/.local/lib/python3.10/site-packages (from apache-airflow>=2.6.0->apache-airflow-providers-postgres) (4.3.11)
Requirement already satisfied: opentelemetry-exporter-otlp in /home/kai/.local/lib/python3.10/site-packages (from apache-airflow>=2.6.0->apache-airflow-providers-postgres) (1.23.0)
Requirement already satisfied: python-slugify>=5.0 in /home/kai/.local/lib/python3.10/site-packages (from apache-airflow>=2.6.0->apache-airflow-providers-postgres) (8.0.4)
Requirement already satisfied: jinja2>=3.0.0 in /home/kai/.local/lib/python3.10/site-packages (from apache-airflow>=2.6.0->apache-airflow-providers-postgres) (3.1.3)
Requirement already satisfied: apache-airflow-providers-imap in /home/kai/.local/lib/python3.10/site-packages (from apache-airflow>=2.6.0->apache-airflow-providers-postgres) (3.5.0)
Requirement already satisfied: apache-airflow-providers-sqlite in /home/kai/.local/lib/python3.10/site-packages (from apache-airflow>=2.6.0->apache-airflow-providers-postgres) (3.7.1)
Requirement already satisfied: mdit-py-plugins>=0.3.0 in /home/kai/.local/lib/python3.10/site-packages (from apache-airflow>=2.6.0->apache-airflow-providers-postgres) (0.4.0)
Requirement already satisfied: cron-descriptor>=1.2.24 in /home/kai/.local/lib/python3.10/site-packages (from apache-airflow>=2.6.0->apache-airflow-providers-postgres) (1.4.3)
Requirement already satisfied: python-nvd3>=0.15.0 in /home/kai/.local/lib/python3.10/site-packages (from apache-airflow>=2.6.0->apache-airflow-providers-postgres) (0.15.0)
Requirement already satisfied: alembic<2.0,>=1.13.1 in /home/kai/.local/lib/python3.10/site-packages (from apache-airflow>=2.6.0->apache-airflow-providers-postgres) (1.13.1)
Requirement already satisfied: marshmallow-oneofschema>=2.0.1 in /home/kai/.local/lib/python3.10/site-packages (from apache-airflow>=2.6.0->apache-airflow-providers-postgres) (3.1.1)
Requirement already satisfied: flask-caching>=1.5.0 in /home/kai/.local/lib/python3.10/site-packages (from apache-airflow>=2.6.0->apache-airflow-providers-postgres) (2.1.0)
Requirement already satisfied: universal-pathlib<0.2.0,>=0.1.4 in /home/kai/.local/lib/python3.10/site-packages (from apache-airflow>=2.6.0->apache-airflow-providers-postgres) (0.1.4)
Requirement already satisfied: termcolor>=1.1.0 in /home/kai/.local/lib/python3.10/site-packages (from apache-airflow>=2.6.0->apache-airflow-providers-postgres) (2.4.0)
Requirement already satisfied: packaging>=14.0 in /home/kai/.local/lib/python3.10/site-packages (from apache-airflow>=2.6.0->apache-airflow-providers-postgres) (23.2)
Requirement already satisfied: argcomplete>=1.10 in /home/kai/.local/lib/python3.10/site-packages (from apache-airflow>=2.6.0->apache-airflow-providers-postgres) (3.2.2)
Requirement already satisfied: asgiref in /home/kai/.local/lib/python3.10/site-packages (from apache-airflow>=2.6.0->apache-airflow-providers-postgres) (3.7.2)
Requirement already satisfied: croniter>=0.3.17 in /home/kai/.local/lib/python3.10/site-packages (from apache-airflow>=2.6.0->apache-airflow-providers-postgres) (2.0.1)
Requirement already satisfied: pyjwt>=2.0.0 in /home/kai/.local/lib/python3.10/site-packages (from apache-airflow>=2.6.0->apache-airflow-providers-postgres) (2.8.0)
Requirement already satisfied: pluggy>=1.0 in /home/kai/.local/lib/python3.10/site-packages (from apache-airflow>=2.6.0->apache-airflow-providers-postgres) (1.4.0)
Requirement already satisfied: attrs>=22.1.0 in /home/kai/.local/lib/python3.10/site-packages (from apache-airflow>=2.6.0->apache-airflow-providers-postgres) (23.2.0)
Requirement already satisfied: lockfile>=0.12.2 in /home/kai/.local/lib/python3.10/site-packages (from apache-airflow>=2.6.0->apache-airflow-providers-postgres) (0.12.2)
Requirement already satisfied: google-re2>=1.0 in /home/kai/.local/lib/python3.10/site-packages (from apache-airflow>=2.6.0->apache-airflow-providers-postgres) (1.1)
Requirement already satisfied: psutil>=4.2.0 in /home/kai/.local/lib/python3.10/site-packages (from apache-airflow>=2.6.0->apache-airflow-providers-postgres) (5.9.8)
Requirement already satisfied: linkify-it-py>=2.0.0 in /home/kai/.local/lib/python3.10/site-packages (from apache-airflow>=2.6.0->apache-airflow-providers-postgres) (2.0.3)
Requirement already satisfied: tabulate>=0.7.5 in /home/kai/.local/lib/python3.10/site-packages (from apache-airflow>=2.6.0->apache-airflow-providers-postgres) (0.9.0)
Requirement already satisfied: deprecated>=1.2.13 in /home/kai/.local/lib/python3.10/site-packages (from apache-airflow>=2.6.0->apache-airflow-providers-postgres) (1.2.14)
Requirement already satisfied: flask-login>=0.6.2 in /home/kai/.local/lib/python3.10/site-packages (from apache-airflow>=2.6.0->apache-airflow-providers-postgres) (0.6.3)
Requirement already satisfied: flask-wtf>=0.15 in /home/kai/.local/lib/python3.10/site-packages (from apache-airflow>=2.6.0->apache-airflow-providers-postgres) (1.2.1)
Requirement already satisfied: apache-airflow-providers-http in /home/kai/.local/lib/python3.10/site-packages (from apache-airflow>=2.6.0->apache-airflow-providers-postgres) (4.9.1)
Requirement already satisfied: apispec[yaml]<7,>=6.0.0 in /home/kai/.local/lib/python3.10/site-packages (from flask-appbuilder==4.3.11->apache-airflow>=2.6.0->apache-airflow-providers-postgres) (6.4.0)
Requirement already satisfied: WTForms<4 in /home/kai/.local/lib/python3.10/site-packages (from flask-appbuilder==4.3.11->apache-airflow>=2.6.0->apache-airflow-providers-postgres) (3.1.2)
Requirement already satisfied: marshmallow-sqlalchemy<0.27.0,>=0.22.0 in /home/kai/.local/lib/python3.10/site-packages (from flask-appbuilder==4.3.11->apache-airflow>=2.6.0->apache-airflow-providers-postgres) (0.26.1)
Requirement already satisfied: click<9,>=8 in /home/kai/.local/lib/python3.10/site-packages (from flask-appbuilder==4.3.11->apache-airflow>=2.6.0->apache-airflow-providers-postgres) (8.1.7)
Requirement already satisfied: Flask-SQLAlchemy<3,>=2.4 in /home/kai/.local/lib/python3.10/site-packages (from flask-appbuilder==4.3.11->apache-airflow>=2.6.0->apache-airflow-providers-postgres) (2.5.1)
Requirement already satisfied: Flask-JWT-Extended<5.0.0,>=4.0.0 in /home/kai/.local/lib/python3.10/site-packages (from flask-appbuilder==4.3.11->apache-airflow>=2.6.0->apache-airflow-providers-postgres) (4.6.0)
Requirement already satisfied: marshmallow<4,>=3.18.0 in /home/kai/.local/lib/python3.10/site-packages (from flask-appbuilder==4.3.11->apache-airflow>=2.6.0->apache-airflow-providers-postgres) (3.20.2)
Requirement already satisfied: sqlalchemy-utils<1,>=0.32.21 in /home/kai/.local/lib/python3.10/site-packages (from flask-appbuilder==4.3.11->apache-airflow>=2.6.0->apache-airflow-providers-postgres) (0.41.1)
Requirement already satisfied: prison<1.0.0,>=0.2.1 in /home/kai/.local/lib/python3.10/site-packages (from flask-appbuilder==4.3.11->apache-airflow>=2.6.0->apache-airflow-providers-postgres) (0.2.1)
Requirement already satisfied: Flask-Babel<3,>=1 in /home/kai/.local/lib/python3.10/site-packages (from flask-appbuilder==4.3.11->apache-airflow>=2.6.0->apache-airflow-providers-postgres) (2.0.0)
Requirement already satisfied: colorama<1,>=0.3.9 in /home/kai/.local/lib/python3.10/site-packages (from flask-appbuilder==4.3.11->apache-airflow>=2.6.0->apache-airflow-providers-postgres) (0.4.6)
Requirement already satisfied: Flask-Limiter<4,>3 in /home/kai/.local/lib/python3.10/site-packages (from flask-appbuilder==4.3.11->apache-airflow>=2.6.0->apache-airflow-providers-postgres) (3.5.1)
Requirement already satisfied: email-validator<2,>=1.0.5 in /home/kai/.local/lib/python3.10/site-packages (from flask-appbuilder==4.3.11->apache-airflow>=2.6.0->apache-airflow-providers-postgres) (1.3.1)
Requirement already satisfied: sqlparse>=0.4.2 in /home/kai/.local/lib/python3.10/site-packages (from apache-airflow-providers-common-sql>=1.3.1->apache-airflow-providers-postgres) (0.4.4)
Requirement already satisfied: more-itertools>=9.0.0 in /home/kai/.local/lib/python3.10/site-packages (from apache-airflow-providers-common-sql>=1.3.1->apache-airflow-providers-postgres) (10.2.0)
Requirement already satisfied: Mako in /home/kai/.local/lib/python3.10/site-packages (from alembic<2.0,>=1.13.1->apache-airflow>=2.6.0->apache-airflow-providers-postgres) (1.3.2)
Requirement already satisfied: typing-extensions>=4 in /home/kai/.local/lib/python3.10/site-packages (from alembic<2.0,>=1.13.1->apache-airflow>=2.6.0->apache-airflow-providers-postgres) (4.9.0)
Requirement already satisfied: PyYAML<7,>=5.1 in /home/kai/.local/lib/python3.10/site-packages (from connexion[flask]<3.0,>=2.10.0->apache-airflow>=2.6.0->apache-airflow-providers-postgres) (6.0.1)
Requirement already satisfied: inflection<0.6,>=0.3.1 in /home/kai/.local/lib/python3.10/site-packages (from connexion[flask]<3.0,>=2.10.0->apache-airflow>=2.6.0->apache-airflow-providers-postgres) (0.5.1)
Requirement already satisfied: clickclick<21,>=1.2 in /home/kai/.local/lib/python3.10/site-packages (from connexion[flask]<3.0,>=2.10.0->apache-airflow>=2.6.0->apache-airflow-providers-postgres) (20.10.2)
Requirement already satisfied: pytz>2021.1 in /home/kai/.local/lib/python3.10/site-packages (from croniter>=0.3.17->apache-airflow>=2.6.0->apache-airflow-providers-postgres) (2024.1)
Requirement already satisfied: cffi>=1.12 in /home/kai/.local/lib/python3.10/site-packages (from cryptography>=0.9.3->apache-airflow>=2.6.0->apache-airflow-providers-postgres) (1.16.0)
Requirement already satisfied: wrapt<2,>=1.10 in /home/kai/.local/lib/python3.10/site-packages (from deprecated>=1.2.13->apache-airflow>=2.6.0->apache-airflow-providers-postgres) (1.16.0)
Requirement already satisfied: cachelib<0.10.0,>=0.9.0 in /home/kai/.local/lib/python3.10/site-packages (from flask-caching>=1.5.0->apache-airflow>=2.6.0->apache-airflow-providers-postgres) (0.9.0)
Requirement already satisfied: referencing>=0.28.4 in /home/kai/.local/lib/python3.10/site-packages (from jsonschema>=4.18.0->apache-airflow>=2.6.0->apache-airflow-providers-postgres) (0.33.0)
Requirement already satisfied: rpds-py>=0.7.1 in /home/kai/.local/lib/python3.10/site-packages (from jsonschema>=4.18.0->apache-airflow>=2.6.0->apache-airflow-providers-postgres) (0.18.0)
Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/kai/.local/lib/python3.10/site-packages (from jsonschema>=4.18.0->apache-airflow>=2.6.0->apache-airflow-providers-postgres) (2023.12.1)
Requirement already satisfied: uc-micro-py in /home/kai/.local/lib/python3.10/site-packages (from linkify-it-py>=2.0.0->apache-airflow>=2.6.0->apache-airflow-providers-postgres) (1.0.3)
Requirement already satisfied: mdurl~=0.1 in /home/kai/.local/lib/python3.10/site-packages (from markdown-it-py>=2.1.0->apache-airflow>=2.6.0->apache-airflow-providers-postgres) (0.1.2)
Requirement already satisfied: importlib-metadata<7.0,>=6.0 in /home/kai/.local/lib/python3.10/site-packages (from opentelemetry-api>=1.15.0->apache-airflow>=2.6.0->apache-airflow-providers-postgres) (6.11.0)
Requirement already satisfied: time-machine>=2.6.0 in /home/kai/.local/lib/python3.10/site-packages (from pendulum<4.0,>=2.1.2->apache-airflow>=2.6.0->apache-airflow-providers-postgres) (2.13.0)
Requirement already satisfied: tzdata>=2020.1 in /home/kai/.local/lib/python3.10/site-packages (from pendulum<4.0,>=2.1.2->apache-airflow>=2.6.0->apache-airflow-providers-postgres) (2024.1)
Requirement already satisfied: setuptools>=62.4.0 in /home/kai/.local/lib/python3.10/site-packages (from python-daemon>=3.0.0->apache-airflow>=2.6.0->apache-airflow-providers-postgres) (69.1.1)
Requirement already satisfied: docutils in /home/kai/.local/lib/python3.10/site-packages (from python-daemon>=3.0.0->apache-airflow>=2.6.0->apache-airflow-providers-postgres) (0.20.1)
Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.3->apache-airflow>=2.6.0->apache-airflow-providers-postgres) (1.16.0)
Requirement already satisfied: text-unidecode>=1.3 in /home/kai/.local/lib/python3.10/site-packages (from python-slugify>=5.0->apache-airflow>=2.6.0->apache-airflow-providers-postgres) (1.3)
Requirement already satisfied: urllib3<3,>=1.21.1 in /home/kai/.local/lib/python3.10/site-packages (from requests<3,>=2.27.0->apache-airflow>=2.6.0->apache-airflow-providers-postgres) (2.0.7)
Requirement already satisfied: idna<4,>=2.5 in /home/kai/.local/lib/python3.10/site-packages (from requests<3,>=2.27.0->apache-airflow>=2.6.0->apache-airflow-providers-postgres) (3.6)
Requirement already satisfied: charset-normalizer<4,>=2 in /home/kai/.local/lib/python3.10/site-packages (from requests<3,>=2.27.0->apache-airflow>=2.6.0->apache-airflow-providers-postgres) (3.3.2)
Requirement already satisfied: certifi>=2017.4.17 in /home/kai/.local/lib/python3.10/site-packages (from requests<3,>=2.27.0->apache-airflow>=2.6.0->apache-airflow-providers-postgres) (2024.2.2)
Requirement already satisfied: greenlet!=0.4.17 in /home/kai/.local/lib/python3.10/site-packages (from sqlalchemy<2.0,>=1.4.28->apache-airflow>=2.6.0->apache-airflow-providers-postgres) (3.0.3)
Requirement already satisfied: requests_toolbelt in /home/kai/.local/lib/python3.10/site-packages (from apache-airflow-providers-http->apache-airflow>=2.6.0->apache-airflow-providers-postgres) (1.0.0)
Requirement already satisfied: aiohttp>=3.9.2 in /home/kai/.local/lib/python3.10/site-packages (from apache-airflow-providers-http->apache-airflow>=2.6.0->apache-airflow-providers-postgres) (3.9.3)
Requirement already satisfied: httpcore<0.17.0,>=0.15.0 in /home/kai/.local/lib/python3.10/site-packages (from httpx->apache-airflow>=2.6.0->apache-airflow-providers-postgres) (0.16.3)
Requirement already satisfied: sniffio in /home/kai/.local/lib/python3.10/site-packages (from httpx->apache-airflow>=2.6.0->apache-airflow-providers-postgres) (1.3.0)
Requirement already satisfied: rfc3986[idna2008]<2,>=1.3 in /home/kai/.local/lib/python3.10/site-packages (from httpx->apache-airflow>=2.6.0->apache-airflow-providers-postgres) (1.5.0)
Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc==1.23.0 in /home/kai/.local/lib/python3.10/site-packages (from opentelemetry-exporter-otlp->apache-airflow>=2.6.0->apache-airflow-providers-postgres) (1.23.0)
Requirement already satisfied: opentelemetry-exporter-otlp-proto-http==1.23.0 in /home/kai/.local/lib/python3.10/site-packages (from opentelemetry-exporter-otlp->apache-airflow>=2.6.0->apache-airflow-providers-postgres) (1.23.0)
Requirement already satisfied: opentelemetry-sdk~=1.23.0 in /home/kai/.local/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc==1.23.0->opentelemetry-exporter-otlp->apache-airflow>=2.6.0->apache-airflow-providers-postgres) (1.23.0)
Requirement already satisfied: grpcio<2.0.0,>=1.0.0 in /home/kai/.local/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc==1.23.0->opentelemetry-exporter-otlp->apache-airflow>=2.6.0->apache-airflow-providers-postgres) (1.62.0)
Requirement already satisfied: googleapis-common-protos~=1.52 in /home/kai/.local/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc==1.23.0->opentelemetry-exporter-otlp->apache-airflow>=2.6.0->apache-airflow-providers-postgres) (1.62.0)
Requirement already satisfied: opentelemetry-proto==1.23.0 in /home/kai/.local/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc==1.23.0->opentelemetry-exporter-otlp->apache-airflow>=2.6.0->apache-airflow-providers-postgres) (1.23.0)
Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.23.0 in /home/kai/.local/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc==1.23.0->opentelemetry-exporter-otlp->apache-airflow>=2.6.0->apache-airflow-providers-postgres) (1.23.0)
Requirement already satisfied: protobuf<5.0,>=3.19 in /home/kai/.local/lib/python3.10/site-packages (from opentelemetry-proto==1.23.0->opentelemetry-exporter-otlp-proto-grpc==1.23.0->opentelemetry-exporter-otlp->apache-airflow>=2.6.0->apache-airflow-providers-postgres) (4.25.3)
Requirement already satisfied: frozenlist>=1.1.1 in /home/kai/.local/lib/python3.10/site-packages (from aiohttp>=3.9.2->apache-airflow-providers-http->apache-airflow>=2.6.0->apache-airflow-providers-postgres) (1.4.1)
Requirement already satisfied: multidict<7.0,>=4.5 in /home/kai/.local/lib/python3.10/site-packages (from aiohttp>=3.9.2->apache-airflow-providers-http->apache-airflow>=2.6.0->apache-airflow-providers-postgres) (6.0.5)
Requirement already satisfied: aiosignal>=1.1.2 in /home/kai/.local/lib/python3.10/site-packages (from aiohttp>=3.9.2->apache-airflow-providers-http->apache-airflow>=2.6.0->apache-airflow-providers-postgres) (1.3.1)
Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/kai/.local/lib/python3.10/site-packages (from aiohttp>=3.9.2->apache-airflow-providers-http->apache-airflow>=2.6.0->apache-airflow-providers-postgres) (4.0.3)
Requirement already satisfied: yarl<2.0,>=1.0 in /home/kai/.local/lib/python3.10/site-packages (from aiohttp>=3.9.2->apache-airflow-providers-http->apache-airflow>=2.6.0->apache-airflow-providers-postgres) (1.9.4)
Requirement already satisfied: pycparser in /home/kai/.local/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=0.9.3->apache-airflow>=2.6.0->apache-airflow-providers-postgres) (2.21)
Requirement already satisfied: dnspython>=1.15.0 in /home/kai/.local/lib/python3.10/site-packages (from email-validator<2,>=1.0.5->flask-appbuilder==4.3.11->apache-airflow>=2.6.0->apache-airflow-providers-postgres) (2.6.1)
Requirement already satisfied: Babel>=2.3 in /home/kai/.local/lib/python3.10/site-packages (from Flask-Babel<3,>=1->flask-appbuilder==4.3.11->apache-airflow>=2.6.0->apache-airflow-providers-postgres) (2.14.0)
Requirement already satisfied: ordered-set<5,>4 in /home/kai/.local/lib/python3.10/site-packages (from Flask-Limiter<4,>3->flask-appbuilder==4.3.11->apache-airflow>=2.6.0->apache-airflow-providers-postgres) (4.1.0)
Requirement already satisfied: limits>=2.8 in /home/kai/.local/lib/python3.10/site-packages (from Flask-Limiter<4,>3->flask-appbuilder==4.3.11->apache-airflow>=2.6.0->apache-airflow-providers-postgres) (3.9.0)
Requirement already satisfied: anyio<5.0,>=3.0 in /home/kai/.local/lib/python3.10/site-packages (from httpcore<0.17.0,>=0.15.0->httpx->apache-airflow>=2.6.0->apache-airflow-providers-postgres) (4.3.0)
Requirement already satisfied: h11<0.15,>=0.13 in /home/kai/.local/lib/python3.10/site-packages (from httpcore<0.17.0,>=0.15.0->httpx->apache-airflow>=2.6.0->apache-airflow-providers-postgres) (0.14.0)
Requirement already satisfied: zipp>=0.5 in /home/kai/.local/lib/python3.10/site-packages (from importlib-metadata<7.0,>=6.0->opentelemetry-api>=1.15.0->apache-airflow>=2.6.0->apache-airflow-providers-postgres) (3.17.0)
Requirement already satisfied: exceptiongroup>=1.0.2 in /home/kai/.local/lib/python3.10/site-packages (from anyio<5.0,>=3.0->httpcore<0.17.0,>=0.15.0->httpx->apache-airflow>=2.6.0->apache-airflow-providers-postgres) (1.2.0)
Requirement already satisfied: importlib-resources>=1.3 in /home/kai/.local/lib/python3.10/site-packages (from limits>=2.8->Flask-Limiter<4,>3->flask-appbuilder==4.3.11->apache-airflow>=2.6.0->apache-airflow-providers-postgres) (6.1.1)
Requirement already satisfied: opentelemetry-semantic-conventions==0.44b0 in /home/kai/.local/lib/python3.10/site-packages (from opentelemetry-sdk~=1.23.0->opentelemetry-exporter-otlp-proto-grpc==1.23.0->opentelemetry-exporter-otlp->apache-airflow>=2.6.0->apache-airflow-providers-postgres) (0.44b0)
Installing collected packages: psycopg2-binary, apache-airflow-providers-postgres
Successfully installed apache-airflow-providers-postgres-5.10.2 psycopg2-binary-2.9.9
kai@BreakThrough:~/airflow$ psql
psql: error: connection to server on socket "/var/run/postgresql/.s.PGSQL.5432" failed: FATAL:  database "kai" does not exist
kai@BreakThrough:~/airflow$ sudo psql
psql: error: connection to server on socket "/var/run/postgresql/.s.PGSQL.5432" failed: FATAL:  role "root" does not exist
kai@BreakThrough:~/airflow$ sudo service postgresql status
 postgresql.service - PostgreSQL RDBMS
     Loaded: loaded (/lib/systemd/system/postgresql.service; enabled; vendor preset: enabled)
     Active: active (exited) since Wed 2024-03-13 20:38:57 IST; 8min ago
    Process: 297883 ExecStart=/bin/true (code=exited, status=0/SUCCESS)
   Main PID: 297883 (code=exited, status=0/SUCCESS)

Mar 13 20:38:57 BreakThrough systemd[1]: Starting PostgreSQL RDBMS...
Mar 13 20:38:57 BreakThrough systemd[1]: Finished PostgreSQL RDBMS.
kai@BreakThrough:~/airflow$ sudo su - postgres
postgres@BreakThrough:~$ ^C
postgres@BreakThrough:~$ ^C
postgres@BreakThrough:~$ ^C
postgres@BreakThrough:~$ exit
logout
kai@BreakThrough:~/airflow$ psql
psql: error: connection to server on socket "/var/run/postgresql/.s.PGSQL.5432" failed: FATAL:  database "kai" does not exist
kai@BreakThrough:~/airflow$ sudo su - postgres
postgres@BreakThrough:~$ psql
psql (14.11 (Ubuntu 14.11-0ubuntu0.22.04.1))
Type "help" for help.

postgres=# \l
                              List of databases
   Name    |  Owner   | Encoding | Collate |  Ctype  |   Access privileges
-----------+----------+----------+---------+---------+-----------------------
 postgres  | postgres | UTF8     | C.UTF-8 | C.UTF-8 |
 template0 | postgres | UTF8     | C.UTF-8 | C.UTF-8 | =c/postgres          +
           |          |          |         |         | postgres=CTc/postgres
 template1 | postgres | UTF8     | C.UTF-8 | C.UTF-8 | =c/postgres          +
           |          |          |         |         | postgres=CTc/postgres
(3 rows)

postgres=# create database customers_db;
CREATE DATABASE
postgres=# \c customers_db
You are now connected to database "customers_db" as user "postgres".
customers_db=# exit
postgres@BreakThrough:~$ exit
logout
kai@BreakThrough:~/airflow$ sudo su - kai
kai@BreakThrough:~$ exit
logout
kai@BreakThrough:~/airflow$ sudo su - postgres
postgres@BreakThrough:~$ \l
l: command not found
postgres@BreakThrough:~$ psql
psql (14.11 (Ubuntu 14.11-0ubuntu0.22.04.1))
Type "help" for help.

postgres=# \u
invalid command \u
Try \? for help.
postgres=# \l
                               List of databases
     Name     |  Owner   | Encoding | Collate |  Ctype  |   Access privileges
--------------+----------+----------+---------+---------+-----------------------
 customers_db | postgres | UTF8     | C.UTF-8 | C.UTF-8 |
 postgres     | postgres | UTF8     | C.UTF-8 | C.UTF-8 |
 template0    | postgres | UTF8     | C.UTF-8 | C.UTF-8 | =c/postgres          +
              |          |          |         |         | postgres=CTc/postgres
 template1    | postgres | UTF8     | C.UTF-8 | C.UTF-8 | =c/postgres          +
              |          |          |         |         | postgres=CTc/postgres
(4 rows)

postgres=# \c customers_db
You are now connected to database "customers_db" as user "postgres".
customers_db=# \dt
Did not find any relations.
customers_db=# select*from user;
   user
----------
 postgres
(1 row)

customers_db=# alter user postgres password 'kai';
ALTER ROLE
customers_db=# exit
postgres@BreakThrough:~$ exit
logout
kai@BreakThrough:~/airflow$ sudo su - postgres
postgres@BreakThrough:~$ exit
logout
kai@BreakThrough:~/airflow$ airflow scheduler
  ____________       _____________
 ____    |__( )_________  __/__  /________      __
____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
 _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
[2024-03-13T20:52:39.160+0530] {task_context_logger.py:63} INFO - Task context logging is enabled
[2024-03-13T20:52:39.161+0530] {executor_loader.py:115} INFO - Loaded executor: SequentialExecutor
[2024-03-13 20:52:39 +0530] [298227] [INFO] Starting gunicorn 21.2.0
[2024-03-13 20:52:39 +0530] [298227] [INFO] Listening at: http://[::]:8793 (298227)
[2024-03-13 20:52:39 +0530] [298227] [INFO] Using worker: sync
[2024-03-13 20:52:39 +0530] [298228] [INFO] Booting worker with pid: 298228
[2024-03-13T20:52:39.224+0530] {scheduler_job_runner.py:808} INFO - Starting the scheduler
[2024-03-13T20:52:39.225+0530] {scheduler_job_runner.py:815} INFO - Processing each file at most -1 times
[2024-03-13T20:52:39.231+0530] {manager.py:169} INFO - Launched DagFileProcessorManager with pid: 298230
[2024-03-13T20:52:39.234+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-13T20:52:39.238+0530] {settings.py:60} INFO - Configured default timezone UTC
[2024-03-13 20:52:39 +0530] [298229] [INFO] Booting worker with pid: 298229
[2024-03-13T20:52:39.259+0530] {manager.py:392} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
^C[2024-03-13T20:54:38.595+0530] {scheduler_job_runner.py:258} INFO - Exiting gracefully upon receiving signal 2
[2024-03-13 20:54:38 +0530] [298227] [INFO] Handling signal: int
[2024-03-13 20:54:38 +0530] [298228] [INFO] Worker exiting (pid: 298228)
[2024-03-13 20:54:38 +0530] [298229] [INFO] Worker exiting (pid: 298229)
[2024-03-13 20:54:38 +0530] [298227] [INFO] Shutting down: Master
^C[2024-03-13T20:54:38.809+0530] {scheduler_job_runner.py:258} INFO - Exiting gracefully upon receiving signal 2
^C[2024-03-13T20:54:39.197+0530] {scheduler_job_runner.py:258} INFO - Exiting gracefully upon receiving signal 2
^C[2024-03-13T20:54:39.433+0530] {scheduler_job_runner.py:258} INFO - Exiting gracefully upon receiving signal 2
[2024-03-13T20:54:40.443+0530] {process_utils.py:131} INFO - Sending Signals.SIGTERM to group 298230. PIDs of all processesin the group: [298230]
[2024-03-13T20:54:40.444+0530] {process_utils.py:86} INFO - Sending the signal Signals.SIGTERM to group 298230
[2024-03-13T20:54:40.538+0530] {process_utils.py:79} INFO - Process psutil.Process(pid=298230, status='terminated', exitcode=0, started='20:52:38') (298230) terminated with exit code 0
[2024-03-13T20:54:40.542+0530] {process_utils.py:131} INFO - Sending Signals.SIGTERM to group 298230. PIDs of all processesin the group: []
[2024-03-13T20:54:40.542+0530] {process_utils.py:86} INFO - Sending the signal Signals.SIGTERM to group 298230
[2024-03-13T20:54:40.542+0530] {process_utils.py:100} INFO - Sending the signal Signals.SIGTERM to process 298230 as process group is missing.
[2024-03-13T20:54:40.543+0530] {scheduler_job_runner.py:884} INFO - Exited execute loop
kai@BreakThrough:~/airflow$ airflow scheduler
  ____________       _____________
 ____    |__( )_________  __/__  /________      __
____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
 _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
[2024-03-13T20:54:54.481+0530] {task_context_logger.py:63} INFO - Task context logging is enabled
[2024-03-13T20:54:54.482+0530] {executor_loader.py:115} INFO - Loaded executor: SequentialExecutor
[2024-03-13 20:54:54 +0530] [298461] [INFO] Starting gunicorn 21.2.0
[2024-03-13 20:54:54 +0530] [298461] [INFO] Listening at: http://[::]:8793 (298461)
[2024-03-13 20:54:54 +0530] [298461] [INFO] Using worker: sync
[2024-03-13 20:54:54 +0530] [298462] [INFO] Booting worker with pid: 298462
[2024-03-13 20:54:54 +0530] [298463] [INFO] Booting worker with pid: 298463
[2024-03-13T20:54:54.562+0530] {scheduler_job_runner.py:808} INFO - Starting the scheduler
[2024-03-13T20:54:54.565+0530] {scheduler_job_runner.py:815} INFO - Processing each file at most -1 times
[2024-03-13T20:54:54.573+0530] {manager.py:169} INFO - Launched DagFileProcessorManager with pid: 298464
[2024-03-13T20:54:54.579+0530] {scheduler_job_runner.py:1608} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-03-13T20:54:54.585+0530] {settings.py:60} INFO - Configured default timezone UTC
[2024-03-13T20:54:54.636+0530] {manager.py:392} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
